---
output:
  pdf_document: default
  html_document: default
---
# Ordered Categorical Data, i.e., Likert-scales


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(patchwork)
library(rethinking)
library(tidyverse)
```

One of a very popular type of response in psychology and social sciences are so-called Likert-scale response. For example, you may be asked to respond on how attractive you find a person in a photo from 1 (very unattractive) to 7 (very attractive). Or to respond how satisfied you are with a service from 1 (very unsatisfied) to 4 (very satisfied). Or rate your confidence on a 5-point scale, etc. Likert-scale responses are extremely common and are quite often analyzed via linear models (i.e., a _t_-test, a repeated measures ANOVA, or linear-mixed models) assuming that response levels correspond directly to real numbers. The purpose of these notes is to document both conceptual and technical problems this approach entails.

## Conceptualization of responses: internal continuous variable discritized into external responses via a set of cut-points
First, let us think what behavioral responses correspond to as it will become very important once we discuss conceptual problems with a common "direct" approach of using linear models for Likert-scale data.

When we ask a participant to respond "On scale from 1 to 7, how attractive do you find the face in the photo?", we assume that there is a _continuous_ internal variable (for example, encoded via a neural ensemble) that represents attractiveness of a face (our satisfaction with service, our confidence, etc.). The strength of that representation varies in a continuous manner from its minimum (e.g., baseline firing rate, if we assume that strength is encoded by spiking rate) to maximum (maximum firing rate for that neural ensemble). When we impose a seven-point scale on participants, we force them to discretize (bin) this continuous variable, creating a _many-to-one_ mapping. In other words, a participant decides that values (intensities) within a particular range all get mapped on $1$, a different but adjacent range of higher values corresponds to $2$, etc. You can think about it as values within that range being "rounded"^[regressed?] towards the mean that defines the response. Or, equivalently, you can think in terms of cut points that define range for individual values. This is how the discretization is depicted in the figure below. If the signal is below the first cut point, our participant's response is "1". When it is between first and second cut points, the response is "2" and so on. When it is to the right of the last sixth cut point, it is "7". This conceptualization means that responses are an ordered categorical variable, as any underlying intensity for a response "1" is necessarily smaller than _any_ intensity for response "2" and both are smaller than, again, _any_ intensity for response "3", etc.

```{r echo=FALSE}
intensity <- c(0.5, 1.7, 6.4)
ggplot() +
  geom_hline(yintercept = 1:6) +
  geom_hline(yintercept = c(0, 7), color="black", size=1)+ 
  geom_bar(data = tibble(x=1:length(intensity), y=intensity), aes(x=x, y=y), stat="identity") +
  scale_y_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:7, labels = sprintf("\u2190%d", 1:7))) + 
  scale_x_continuous(name = "Intensity to Response", breaks = 1:length(intensity), labels = sprintf("%.1f\u2192%d", intensity, ceiling(intensity))) +
  coord_flip() +
  labs(subtitle="Many-to-one mapping of a continuous variable on categorical responses")
```

As per usual, even when we use the same stimulus and ask the same question, participant's internal continuous response varies from trial to trial due to noise. We model this by assuming that on a given trial a value is drawn from a normal distribution centered at the "true" intensity level^[Here, I will assume that cut points are fixed and it is parameters of the normal distribution that get adjusted. The actual implementation of ordered logit/probit models has it the other way around, so that intensity always comes from a normal distribution centered at $0$ and with a standard deviation of $1$ and it is cut-points that get adjusted. The two are mostly mathematically equivalent but I find the former to be more intuitive.]. When the noisy intensity is converted to discrete responses, their variability will depend on the location (mean) and the width (standard deviation) of this distribution. The broader this distribution and / or closer it is to a cut point, the more activity will "spill over" a cut point into adjacent regions and more variable discrete responses will be.

```{r echo=FALSE}
x <- seq(0, 7, length.out = 500)
gaussians <- bind_rows(
  tibble(x = x, label = "1") %>%
    mutate(y = dnorm(x, mean = 2.5, sd = 0.125)),
    tibble(x = x, label = "2") %>%
    mutate(y = dnorm(x, mean = 2.5, sd = 1)),
    tibble(x = x, label = "3") %>%
    mutate(y = dnorm(x, mean = 2, sd = 0.125))
  ) %>%
  mutate(Response = ifelse(x == 0, 1, ceiling(x)),
         Response = ifelse(Response > 7, 7, Response),
         Response = factor(Response, levels = 1:7))

ggplot() +
  geom_ribbon(data=gaussians, aes(x=x, ymax=y, fill=Response), ymin=0) +
  geom_vline(xintercept = 1:6) +
  geom_vline(xintercept = c(0, 7), color="black", size=1)+ 
  scale_x_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:7, labels = sprintf("\u2190%d", 1:7))) + 
  facet_wrap(label~., ncol=1) + 
  theme(strip.background = element_blank(),  strip.text.x = element_blank()) +
  scale_y_continuous(name = "", breaks = NULL)
```

Given this conceptualization, our goal is to recover cut points and model shifts of the mean for the _continuous_ internal variable (in response to our experimental manipulation) using only observed _discrete_ responses.


## Conceptual problem with linear models: we change our mind about what responses correspond to.
A very common approach is to fit Likert-scale data using a linear model (a _t_-test, a repeated-measures ANOVA, linear-mixed models, etc.) while assuming that responses correspond directly to real numbers. In other words, when participants responded "very unattractive", or "not confident at all", or "do not agree at all" they literally meant a real number $1.0$. When they used the middle (let's say the third on a five-point scale) option "neither agree, nor disagree" they literally meant $3.0$.

```{r echo=FALSE}
tibble(x = 1:5,
       label = c("Strongly\ndisagree", "Disagree", "Neither agree,\nnor disagree", "Agree", "Strongly\nagree")) %>%
ggplot(aes(x = x)) +
  geom_point(y = 0) +
  geom_text(aes(label = label), y = 1, vjust=0.5) +
  geom_segment(aes(xend = x), y = 0.9, yend=0.1, arrow = arrow()) +
  scale_x_continuous(name = "Real values", breaks = 1:5, labels = sprintf("%.1f", 1:5), limits = c(0.5, 5.5)) +
  scale_y_continuous(name = NULL, breaks = NULL, limits = c(0, 1.1)) +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())
```

This assumption appears to simplify our life dramatically but at the expense of changing the narrative. Recall that our original (and, for me, very intuitive) conceptualization was that responses reflect a many-to-one mapping between an underlying continuous variable and a discrete (ordered categorical) response. But by converting them directly to real numbers and using them as an outcome variable of a linear model we assume a _one-to-one_ mapping between the _continuous_ real-valued internal variable and _continuous_(!) real-valued observed responses. This means that from a linear model point of view, for a 7-point Likert scale _any_ real value is a valid and possible response and therefore a participant _could have_ responded with 6.5, 3.14, or 2.71828 but, for whatever reason (sheer luck?), we only observed a handful of (integer) values.

```{r echo=FALSE}
x <- seq(0, 7, length.out = 500)
gaussians <- 
  tibble(x = x, label = "2") %>%
  mutate(y = dnorm(x, mean = 2.5, sd = 1)) %>%
  mutate(Response = ifelse(x == 0, 1, ceiling(x)),
         Response = ifelse(Response > 7, 7, Response),
         Response = factor(Response, levels = 1:7),
         IsFour = Response == 4)

ggplot() +
  geom_ribbon(data=gaussians, aes(x=x, ymax=y, fill=IsFour), ymin=0, show.legend = FALSE) +
  geom_vline(xintercept = 1:6) +
  geom_vline(xintercept = c(0, 7), color="black", size=1)+ 
  scale_x_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:7, labels = sprintf("\u2190%d", 1:7))) + 
  theme(strip.background = element_blank(),  strip.text.x = element_blank()) +
  scale_y_continuous(name = "", breaks = NULL) +
  scale_fill_manual(values = c("gray", "red")) + 
  labs(title = "Many-to-one mapping", 
       subtitle = 'Likelihood of response "4" corresponds to the area under the curve.')

ggplot() +
  geom_ribbon(data=gaussians, aes(x=x, ymax=y), ymin=0, fill="gray") +
  geom_vline(xintercept = 1:6) +
  geom_vline(xintercept = c(0, 7), color="black", size=1)+
  geom_segment(data = NULL, aes(x = 4, y = 0, xend = 4, yend = dnorm(4, mean = 2.5, sd = 1)),color = "red") +
  geom_point(data = NULL, aes(x = 4, y = dnorm(4, mean = 2.5, sd = 1)), color = "red", size=3) +
  scale_x_continuous(name = "Signal intensity", breaks = c(0, 7), labels = c("Minimal signal", "Maximal signal"), limits = c(-0.5, 7.5),
                     sec.axis = dup_axis(name="Cut points", breaks = 1:7, labels = sprintf("%d", 1:7))) + 
  theme(strip.background = element_blank(),  strip.text.x = element_blank()) +
  scale_y_continuous(name = "", breaks = NULL) +
  labs(title = "One-to-one mapping", 
       subtitle = 'Likelihood of response "4" corresponds to probablity density for value 4.0 (red dot).')

```


Notice that this is _not_ how we thought participants behave. I think everyone^[Never say always!] would object to the idea that the limited repertoire of responses is due to endogenous processing rather than exogenous limitations imposed by an experimental design. Yet, this is how a _linear model_ "thinks" about it given the outcome variable you gave it and, if you are not careful, it is easy to miss this change in the narrative. It is, however, important as it means that estimates produced by such model are about that alternative one-to-one kind of continuous responses, not the many-to-one discrete ones that you had in mind! That alternative is not a bad story per se, it is just a _different_ story that should not be confused with the original one.

This change in the narrative of what responses correspond to is also a problem if you want to use a (fitted) linear model to generate predirection and simulate the data. It will happily spit out real valued responses like 6.5, 3.14, or 2.71828^[If you feel lucky enough to expect an integer response, you'd better use this luck on playing an actual lottery]. You have two options. You can bite the bullet and take them at their face value, sticking to "response is a real-valued variable" and one-to-one mapping between an internal variable and an observed response. That lets you keep the narrative but means that real and ideal observers play by different rules. Their responses are different and that means your conclusions based on an ideal observer behavior are of limited use. Alternatively, you can round real-valued responses off to a closest integer getting discrete categorical-like responses. Unfortunately, that means changing the narrative yet again. In this case, you fitted the model assuming a one-to-one mapping but you use its predictions assuming many-to-one. Not good. It is really hard to understand what is going on, if you keep changing your mind on what responses mean. A linear model will also generate out-of-range responses, like -1 or 8. Here, you have little choice but to clip them into the valid range, forcing the many-to-one mapping on at least some responses. Again, change of a narrative means that model fitting and model interpretation rely on different conceptualizations of what response is.

This may sound too conceptual but I suspect that few people who use linear models on Likert-scale data directly realize that their model is not doing what they think it is doing and, erroneously!, interpret one-to-one linear-model estimates as many-to-one. The difference may or may not be crucial but, unfortunately, one cannot know how important it is without comparing two kind of models directly. And that raises a question: Why employ a model that does something different to what you need to to being with? Remember, using an appropriate model and interpreting it correctly is _your_ job, not that of a mathematical model, nor is it a job of a software package.

## A technical problem: Data that bunches up near a range limit.
When you use a linear model, you assume that residuals are normally distributed. This is something that you may not be sure of _before_ you fit a specific model, as it is residuals not the data that must be normally distributed. However, in some cases you may be fairly certain that this will not be the case, such as when a variable has only a limited range of values and the mean (a model prediction) is close to one of these limits. Whenever you have observations that are close to that hard limit, they will "bunch up" against it because they cannot go lower or higher than that. See the figure below for an illustration of how it happens if a _continuous_ variable $x$ is restricted to 1 to 7 range^[Note that for skewed distributions their mode is different from the mean!].
```{r echo=FALSE}
x <- seq(0, 1, length.out = 100)
mu <- c(2, 4, 5)
curves_df <- purrr::map_dfr(mu, ~tibble(y = rethinking::dbeta2(x, (. - 1)/ 6, 10), x = x * 6 + 1, `μ`= as.character(.)))

ggplot(data = curves_df, aes(x = x, y = y, color = `μ`)) + 
  geom_line() +
  ylab("Probability density") +
  scale_x_continuous("Continuous variable with 1..7 range", breaks = 1:7)
```

The presence of a limit is not a deal breaker for using linear models per se. Most physical measures cannot be negative^[Try coming up with one and don't say "temperature"!] but as long your observations are sufficiently far away from zero, you are fine. You cannot have a negative height but you certainly can use linear models for adult height as, for example, an average female height in USA is 164±6.4 cm. In other words, the mean is more than 25 standards deviations away from the range limit of zero and the latter can be safely ignored.

Unfortunately, Likert-scale data combines an extremely limited range with a very coarse step. Even a 7-point Likert scale does not give you much of a wiggle room and routinely used 5-point scales are even narrower. This means that unless the mean is smack in the middle (e.g., at four for a 7-point scale), you are getting closer to one of the limits and your residuals become either positively (when approaching a lower limit) or negatively (for the upper one) skewed. In other words, the residuals are _systematically_ not normally distributed and their distributions depends on the mean. This clearly violates an assumption of normality of residuals and of their conditional i.i.d. (Independent and Identically Distributed). This is a deal breaker for parametric frequentist statistics (a _t_-test, a repeated-measures ANOVA, linear-mixed models), as their inferences are built on that assumptions and, therefore, become unreliable and should not to be trusted.

## Another technical problem: Can we assume that responses correspond to real numbers that we picked?
The skewed residuals described above are a fundamental problem for parametric frequentist methods but is not critical if you use Bayesian or non-parametric bootstrapping/permutation linear models. Does this mean it is safe to use them? Probably not. When you use responses directly, you assume a direct correspondence between a response label (e.g., "agree") and a real number $4.0$. If your responses do correspond to the real numbers you have picked, you can perform the usual arithmetic with them. E.g., you can assume that $(4.0 + 4.0) / 2$ is equal to $(3.0 + 5.0) / 2$ to $(2.0 + 6.0) / 2$ to $(1.0 + 7.0)/ 2$. However, what if this is _not_ the case, what if the responses do not correspond to the real numbers that you've picked? Then our basic arithmetic stops working the way you think! Take a look at the figure below where "real value" of responses is not an integer that we have picked for it.

```{r echo=FALSE}
cutpoints <- c(0.05, 0.1, 0.3, 0.6, 0.8, 0.9)
responses <- c(4, 2)

plot_cutpoints_sum <- function(cutpoints, responses, title){
  actual_responses <- cutpoints[responses]
  response_label <- c("average", responses)
  
  ggplot() +
    scale_x_continuous(name = "Intensity to Response", breaks = 1:length(response_label), labels = response_label) +
    scale_y_continuous(name = "Signal intensity", breaks = c(0, cutpoints, 1), labels = c("Minimal\nsignal", cutpoints, "Maximal\nsignal"), limits=c(0, 1),
                       sec.axis = dup_axis(name="Response", breaks = cutpoints, labels = 1:6)) +
    coord_flip() +
    geom_bar(data = tibble(x=1 + (1:length(actual_responses)), y=actual_responses), aes(x=x, y=y), stat="identity") +
    geom_bar(data = tibble(x=1, y=mean(cutpoints[responses])), aes(x=x, y=y), stat="identity", fill = "darkgreen") +
    
    geom_hline(yintercept = cutpoints, color = "white") +
    labs(title = title) + 
    theme(panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())
}


plot_cutpoints_sum(cutpoints, c(4, 2), '("2" + "4")/2 > "3"') / 
plot_cutpoints_sum(cutpoints, c(6, 2), '("6" + "2")/2 < "4"')
```

Unless you _know_ that response levels correspond to the selected real number and that the simple arithmetic holds, you are in danger of computing nonsense. This problem is more obvious when individual response levels are labelled, e.g., `"Strongly disagree"`, `"Disagree"`, `"Neither disagree, nor agree"`, `"Agree"`, `"Strongly agree"`. What is an average of `"Strongly disagree"` and `"Strongly agree"`? Is it the same as an average of `"Disagree"` and `"Agree"`? Is increase from `"Strongly disagree"` to `"Disagree"` identical to that from `"Neither disagree, nor agree"` to `"Agree"`? The answer is "who knows?!" but in my experience scales are rarely truly linear as people tend to avoid extremes and have their own idea about range of internal variable levels that correspond to a particular response.

As noted above, even when scale levels are explicitly named, it is very common to "convert" them to numbers because you cannot ask computer to compute an average of `"Disagree"` and `"Agree"` (it will flatly refuse to do this) but it will compute an average of $2$ and $4$. And there will be no error message! And it will return $3$! Problem solved, right? Not really. Yes, the computer will not complain but this is because it has no idea what $2$ and $4$ stand for. You give it real numbers, it will do the math. So, if you pretend that `"Disagree"` and `"Agree"` correspond directly to $2$ and $4$ it will certainly _look like_ normal math. And imagine that responses are `"Disagree"` and `"Strongly agree"`, so the numbers are $2$ and $5$ and the computer will return an average value of $3.5$. It will be even easier to convince yourself that your responses are real numbers (see, there is a _decimal point_ where!), just like linear models assume. Unfortunately, you are not fooling the computer (it seriously does not care), you are fooling yourself. Your math might check out, if responses do correspond to the real numbers you have picked, or it might not. And in both cases, there will be no warning or an error message, just some numbers that you will interpret at their face value and reach possibly erroneous conclusions. Again, the problem is that you wouldn't know whether the numbers you are looking at are valid or nonsense and the same dilemma (valid or nonsense?) will be applicable to any inferences and conclusions that you draw from them. In short, a direct correspondence between response levels and specific real numbers is a _very_ strong assumption that should be validated, not taken on pure faith.

## Solution: an ordered logit/probit model

So far I have summarized problems of using linear models when assuming that responses correspond to real numbers. How can we solve them? By using ordered [logistic](https://en.wikipedia.org/wiki/Ordered_logit)/[probit](https://en.wikipedia.org/wiki/Ordered_probit#:~:text=In%20statistics%2C%20ordered%20probit%20is,fair%2C%20good%2C%20excellent) models. They are built using the many-to-one mapping between a continuous variable that has a limited range (for simplicity it ranges from 0 to 1) and is discretized to match behavioral responses using a set of cut points. In principle, the latter can be fixed but in most cases they should be fitted as part of the model. Both logit and probit models assume that the sampling distribution of the underlying continuous variable is a standard normal and, therefore, both the continuous variable and cut points live on the infinite real number line that is transformed to 0..1 range via either logit or probit link function. Strictly speaking, the latter step is not necessary but makes things easier both for doing math and for understanding.

From a mathematical point of view, using logit and probit makes it easy to compute the area under the curve between two cut points. Logit or probit are cumulative functions, so for a standard normal distribution (centered at $0$ with standard deviation of $1$) they compute an area under the curve starting from $-\infty$ up to some point $k_i$.

```{r echo = FALSE}
cutpoints <- c(-1.4, 0, 1.098612, 2.7)

df <- 
  tibble(x = seq(-3, 3, length.out = 1000)) %>%
  mutate(y = dnorm(x, 0, 1),
         ylogit = rethinking::inv_logit(x))



ggplot(df, aes(x = x, y = y)) +
  geom_ribbon(data = df %>% filter(x < cutpoints[2]),
              aes(ymax = y, ymin = 0), fill = "red") +
  geom_vline(xintercept = cutpoints, color = "darkgray", size = 1) +
  geom_line() +
  geom_segment(data = NULL, x = -1, xend = -0.55, y = dnorm(-0.5), yend = dnorm(-0.5), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_label(data = NULL, x = -1.1, y = dnorm(-0.5), label = "inv_logit(0) = 0.5", hjust=1) +
  ylab("PDF") +
  scale_x_continuous(breaks = -3:3, sec.axis = dup_axis(name = "Cut points", breaks = cutpoints, labels = c("\u21901", "\u21902", "\u21903", "\u21904 5\u2192")))


ggplot(df, aes(x = x, y = y)) +
  geom_ribbon(data = df %>% filter(x < cutpoints[3]),
              aes(ymax = y, ymin = 0), fill = "red") +
  geom_vline(xintercept = cutpoints, color = "darkgray", size = 1) +
  geom_line() +
  geom_segment(data = NULL, x = 1, xend = 0.55, y = dnorm(0.5), yend = dnorm(0.5), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_label(data = NULL, x = 1.1, y = dnorm(-0.5), label = "inv_logit(1.1) = 0.75", hjust=0) +
  ylab("PDF") +
  scale_x_continuous(breaks = -3:3, sec.axis = dup_axis(name = "Cut points", breaks = cutpoints, labels = c("\u21901", "\u21902", "\u21903", "\u21904 5\u2192")))
```

Therefore, if we want to compute an area between two cut points $k_{i-1}$ and $k_i$, we can do it as $logit(k_{i})-logit(k_{i-1})$ (same goes for probit).

```{r echo=FALSE}
cutpoints <- c(-1.4, 0, 1.098612, 2.7)

df <- 
  tibble(x = seq(-3, 3, length.out = 1000)) %>%
  mutate(y = dnorm(x, 0, 1),
         ylogit = rethinking::inv_logit(x))
  
pdf_plot <-
  ggplot(df, aes(x = x, y = y)) +
  geom_ribbon(data = df %>% filter(x >= cutpoints[2] & x < cutpoints[3]),
              aes(ymax = y, ymin = 0), fill = "red") +
  geom_vline(xintercept = cutpoints, color = "darkgray", size = 1) +
  geom_line() +
  ylab("PDF") +
  scale_x_continuous(breaks = -3:3, sec.axis = dup_axis(name = "Cut points", breaks = cutpoints, labels = c("\u21901", "\u21902", "\u21903", "\u21904 5\u2192")))
  
cum_plot <-
    ggplot(df, aes(x = x, y = y)) +
    geom_line(aes(y = ylogit)) +
    geom_vline(xintercept = cutpoints, color = "darkgray", size = 1) +
    geom_segment(data = NULL, x = cutpoints[2], xend = 2, y = rethinking::inv_logit(cutpoints[2]), yend = rethinking::inv_logit(cutpoints[2])) +
    geom_segment(data = NULL, x = cutpoints[3], xend = 2, y = rethinking::inv_logit(cutpoints[3]), yend = rethinking::inv_logit(cutpoints[3])) +
    geom_segment(data = NULL, x = 1.9, xend = 1.9, y = rethinking::inv_logit(cutpoints[2]), yend = rethinking::inv_logit(cutpoints[3]), color = "red", arrow = arrow(length = unit(0.03, "npc"))) +
    geom_segment(data = NULL, x = 1.9, xend = 1.9, y = rethinking::inv_logit(cutpoints[3]), yend = rethinking::inv_logit(cutpoints[2]), color = "red", arrow = arrow(length = unit(0.03, "npc"))) +
      geom_point(data = NULL, aes(x = cutpoints[2], y = rethinking::inv_logit(cutpoints[2])), size = 3) +
    geom_point(data = NULL, aes(x = cutpoints[3], y = rethinking::inv_logit(cutpoints[3])),  size = 3) +

    ylab("inv_logit(x)") +
  scale_x_continuous(breaks = -3:3, sec.axis = dup_axis(name = "Cut points", breaks = cutpoints, labels = c("\u21901", "\u21902", "\u21903", "\u21904 5\u2192")))
  
pdf_plot / cum_plot

```

Because both logit and probit are non-linear transformations cut points evenly distributed on a 0..1 range will be not evenly distributed on the real  numbers line and vice versa. Transforming from real space to 0..1 range also makes it easier to understand relative positions of cut points and changes in continuous variable (that we translate into discrete responses via cut points).

```{r echo=FALSE}
z_breakpoints <- seq(0, 1, length.out = 8)[2:8] #c(0.2, 0.4, 0.6, 0.8, 1.0)
z_intensity <- c(0.25, 0.62)
response <- purrr::map_dbl(z_intensity, ~min(which(z_breakpoints>=.)))

breakpoints <- rethinking::logit(z_breakpoints[1:(length(z_breakpoints) - 1)])
intensity <- rethinking::logit(z_intensity)

ggplot() +
  geom_hline(yintercept = z_breakpoints) +
  geom_hline(yintercept = c(0, 1), color="black", size=1)+ 
  # geom_bar(data = tibble(x=1:length(z_intensity), y=z_intensity), aes(x=x, y=y), stat="identity") +
  geom_point(data = tibble(x=1:length(z_intensity), y=z_intensity), aes(x=x, y=y), size=5, color="red") +
  scale_y_continuous(name = "Signal intensity", 
                     sec.axis = dup_axis(name="Cut points", breaks = z_breakpoints, labels = sprintf("\u2190%d", 1:length(z_breakpoints)))) + 
  scale_x_continuous(name = "Intensity to Response", breaks = 1:length(z_intensity), labels = sprintf("%.2f\u2192%d", z_intensity, response), limits = c(0.5, 2.5)) +
  coord_flip() +
  labs(subtitle="Cut points and intensity on 0..1 range")
```

```{r echo=FALSE}
ggplot() +
  geom_hline(yintercept = breakpoints) +
#   geom_bar(data = tibble(x=1:length(intensity), y=intensity), aes(x=x, y=y), stat="identity") +
  geom_point(data = tibble(x=1:length(intensity), y=intensity), aes(x=x, y=y), size=5, color="red") +
  scale_y_continuous(name = "logit(Signal intensity)", limits = c(-2.5, 2.5),
                     sec.axis = dup_axis(name="Cut points", breaks = breakpoints, labels = sprintf("\u2190%d", 1:length(breakpoints)))) + 
  scale_x_continuous(name = "Intensity to Response", breaks = 1:length(intensity), labels = sprintf("%.2f\u2192%d", intensity, response)) +
  coord_flip() +
  labs(subtitle="Same cut points and intensity but after logit transformation.")
```


