# Intuition for how Cholesky decomposition makes possible to generate correlated random variables

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
```

In chapter 14 of the "Statistical Rethinking", a Cholesky decomposition of a partial correlations matrix is used to generate _correlated_ random variables with matching partial correlations. For uninitiated the whole procedure looks magic but "It would be magic, except that it is just algebra." The purpose of these notes is to provide an intuition of how this works. I will present two explanations: A purely algebraic one and a more visual one. Unfortunately, both of them require knowledge of essentials of linear algebra. I would recommend the entire [Essense of linear algebra](https://youtu.be/fNk_zzaMoSs) series by Grant Sanderson, a.k.a. [3Blue1Brown](https://www.3blue1brown.com/) but first three videos are a must at least for the visual proof. These are bite-sized 10-15 minute videos that deep you a deep intuition about a single topic, so you can always watch them when you have a break.

## Purely algebraic proof
Below is a very brief description, for further details please refer to an excellent [post](https://mlisi.xyz/post/simulating-correlated-variables-with-the-cholesky-factorization) by [Matteo Lisi](https://mlisi.xyz/). Given that we have a correlations matrix $\Sigma$, we can use [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition):
$$\Sigma = LL^T$$

Next, we can generate a matrix of _uncorrelated_ random numbers $Z$ (e.g., z-scores from a standard normal distribution). Assuming that samples are uncorrelated then the covariance matrix is $\mathbb{E}(ZZ^T) = I$ (an identity matrix with $1$ on the diagonal and $0$ everywhere else). For the proof you need to remember that:

1. order of factor is reversed if you apply transposition or inverse to the individual factors of a matrix product $(AB)^T = B^TA^T$,
2. $\mathbb{E}(cX) = c\mathbb{E}(X)$ an expected value of a scaled quantity is equal to an expected value of the unscaled quantity times the scaling factor (e.g., $mean(3\cdot x) = 3\cdot mean(x)$)
3. matrix multiplication by an identity matrix does not change anything $IM=M$ (this is just like multiplying a scalar by $1$: $1\cdot m = m$),

Now, given that $M=LZ$ (we matrix multiply our uncorrelated data with the Cholesky factor of the desired pairwise correlations matrix), we can compute partial correlations for these new values
$$\mathbb{E}(MM^T) = \\
\mathbb{E}((LZ)(LZ)^T) =$$
Applying rule #1 to open the brackets:
$$\mathbb{E}(L Z Z^T L^T) =$$
Applying rule #2 to move constant $L$ out of expected value operator:
$$L\mathbb{E}(Z Z^T) L^T =$$
Remembering that $\mathbb{E}(ZZ^T) = I$
$$L I L^T =$$
Applying rule #3 to multiply by an identity matrix
$$L L^T = \Sigma$$

The proof is very simple and you see that you must end up with a roughly the same^[Depending on how close $\mathbb{E}(ZZ^T)$ to $I$.] correlations. Unfortunately, although I can see _why_ this should work (because algebra), I do not get any intuition about _how_ it works. If you are like me, keep on reading for a more verbose and visual explanation.

## More verbose and visual proof
For me the critical part was to understand what exactly does the Cholesky decomposition perform, i.e., what information the matrix $L$ holds and what kind of linear transformation it represents. First, let us think what a correlations matrix is:
$$\Sigma = \begin{vmatrix} corr(x, x) & corr(x, y) \\ corr (y, x) & corr(y, y) \end{vmatrix} $$

Now we need to recall some linear algebra, namely that a dot product between to vectors is $x \cdot y = |x| |y| cos(\theta_{xy})$, where $|x|$ and $|y|$ are length of vectors $x$ and $y$, and $\theta_{xy}$ is an angle between them. Also note that $cos(\theta)$ corresponds to Pearson's correlation $\rho$. If $x$ and $v$ are z-scores with mean of zero and standard deviation of one then their length is $1$ then their dot product computes Pearson's correlation directly: 
$$x \cdot y = |x| |y| cos(\theta_{xy}) = 1 \times 1 \times cos(\theta_{xy}) = cos(\theta_{xy})$$

This means that we can rewrite our correlation matrix as a matrix of cosines of angles between individual vectors (variables):
$$\Sigma = \begin{vmatrix} cos(\theta_{xx}) & cos(\theta_{xy})) \\ cos(\theta_{yx}) & cos(\theta_{yy})) \end{vmatrix} $$

Now it is time to get to Cholesky decomposition: $\Sigma = L L^T$. Given that $\Sigma$ is a matrix of pairwise (cosines of) angles between vectors, Cholesky decomposition produces a set of vectors with exactly those angles in-between.

Let us visualize this for a 2D case. A correlation of $0.5$ corresponds to a $cos(60 ^{\circ})$

```{r}
Sigma <- matrix(c(1, 0.5, 0.5, 1), ncol = 2)
L <- chol(Sigma)
L
```

```{r echo=FALSE}
ggplot() + 
  geom_segment(aes(x = 0, y = 0, xend = L[1, ], yend = L[2, ]), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_curve(aes(x = L[1, 1] * 0.3, y = L[2, 1]  * 0.3, xend = L[1, 2]  * 0.3, yend = L[2, 2]  * 0.3), curvature = 0.5) +
  geom_text(aes(x = 0.3 * cos(30 * pi / 180), y = 0.35 * sin(30 * pi / 180), label = "cos(60°) = 0.5"), hjust=-1, vjust=-1) +
  coord_equal()
```

Now for a negative correlation
```{r}
Sigma <- matrix(c(1, -0.5, -0.5, 1), ncol = 2)
L <- chol(Sigma)
L
```

```{r echo=FALSE}
ggplot() + 
  geom_segment(aes(x = 0, y = 0, xend = L[1, ], yend = L[2, ]), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_curve(aes(x = L[1, 1] * 0.3, y = L[2, 1]  * 0.3, xend = L[1, 2]  * 0.3, yend = L[2, 2]  * 0.3), curvature = 0.5) +
  geom_text(aes(x = 0.3 * cos(60 * pi / 180), y = 0.35 * sin(60 * pi / 180), label = "cos(120°) = -0.5"), hjust=-1, vjust=-1) +
  coord_equal()
```

The same logic applies to cases beyond 2D but are trickier to visualize. 

Now that we know what matrix $L$ represents (set of vectors that are positioned at desired angles relative to each other), we can think about what $LZ$ achieves given that $Z$ is our $N\times 2$ matrix random numbers. Recall that a (square) matrix describes a linear transformation of space and can be understood by where it sends basis vectors (axes) $\hat{i}$ and $\hat{j}$ (watch [this video](https://youtu.be/kYB8IZa5AuE) to get a visual feel for it). Our matrix $L$ sends $\hat{i}$ to a new location described by its first column^[Spoilers: it is the same location as before.] and $\hat{j}$ to a location defined by its second column. This rotates and sheers the space. When applied to our random vectors, this means that although they started as independent (lying along orthogonal axes) they become correlated (as the axes they lie along stop being orthogonal). 

Again let us visualize for the case of $\rho=0.5$, i.e., $\theta_{xy} = 60 ^\circ$:
```{r}
set.seed(192405)
Sigma <- matrix(c(1, 0.5, 0.5, 1), ncol = 2)
L <- chol(Sigma)
L
Z <- matrix(rnorm(n = 200), ncol = 2)
cor(Z)
```
Below is the plot of the original data. Note two highlighted points, here, values of x (positive for a red dot, negative for a blue one) are uncorrelated with values of y (roughly zero, which is why I have picked these two points). 
```{r echo=FALSE}
ggplot() + 
  geom_point(aes(x=Z[, 1], y=Z[, 2])) +
  geom_segment(aes(x = 0, y = 0, xend = c(1, 0), yend = c(0, 1)), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_point(aes(x=Z[85, 1], y=Z[85, 2]), color="red", size=3) +
  geom_point(aes(x=Z[75, 1], y=Z[75, 2]), color="blue", size=3) +
  geom_label(aes(x = c(0, 1) * 1.2, y = c(1, 0) * 1.2, label = c("\U00ee", "\U0135")), color="blue") +
  coord_equal() +
  xlab("x") +
  ylab("y") +
  labs(subtitle = "Original uncorrelated data and orthogonal basis vectors (axes)")
```
Now the same plot but for $M = ZL$. Here, knowing $x$ coordinate allows you to predict $y$ as well. 
```{r echo=FALSE}
M <- Z %*% L
ggplot() + 
  geom_point(aes(x=M[, 1], y=M[, 2])) +
  geom_segment(aes(x = 0, y = 0, xend = L[1, ], yend = L[2, ]), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_point(aes(x=M[85, 1], y=M[85, 2]), color="red", size=3) +
  geom_point(aes(x=M[75, 1], y=M[75, 2]), color="blue", size=3) +
  geom_label(aes(x = L[, 2] * c(1.1, 1.25), y = L[, 1] * 1.1, label = c("\U00ee", "\U0135")), color="blue") +
  coord_equal() +
  xlab("x") +
  ylab("y") +
  labs(subtitle = "Correlated data due to oblique basis vectors (axes)")
```

Below is the plot for all points, showing their positions before and after the transformation. Note how the dots are only moved along the vertical axes. This is because Cholesky decomposition left the first basis vector $\hat{i}$ (x-axis) unchanged (still $(1, 0)$) but adjusted the $\hat{j}$ to achieve the desired angle of $60^\circ$. Also note that we _could have_ used a different vector for $\hat{i}$, after all there are infinite pairs of $\hat{i}$ and $\hat{j}$ with $60^\circ$ between them. However, Cholesky decomposition needs a starting point --- a first vector --- and keeping $\hat{i}$ at its original position is as good of a starting point as any.
```{r echo=FALSE}
df <-
  rbind(
  data.frame(x = Z[, 1], y = Z[, 2], matrix = "Z"),
  data.frame(x = M[, 1], y = M[, 2], matrix = "M=ZL"))
df$matrix <- factor(df$matrix, levels = c("Z", "M=ZL")) 
ggplot() +
  geom_segment(data=NULL, aes(x=Z[, 1], y=Z[, 2], xend=M[, 1], yend=M[, 2]), arrow = arrow(length = unit(0.03, "npc"))) +
  geom_point(data = df, aes(x = x, y = y, color = matrix)) +
  xlab("x") +
  ylab("y") +
  coord_equal()
```
