<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Bayesian vs. fequentist statisics | Notes on Statistics</title>
<meta name="author" content="Alexander Pastukhov">
<meta name="description" content="I suspect that many student who read “Statistical Rethinking” have a feeling that it is something completely different from what they have been learning in “traditional” statistics classes. That...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="Chapter 7 Bayesian vs. fequentist statisics | Notes on Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="I suspect that many student who read “Statistical Rethinking” have a feeling that it is something completely different from what they have been learning in “traditional” statistics classes. That...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 7 Bayesian vs. fequentist statisics | Notes on Statistics">
<meta name="twitter:description" content="I suspect that many student who read “Statistical Rethinking” have a feeling that it is something completely different from what they have been learning in “traditional” statistics classes. That...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css%20-%20style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Notes on Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Precis</a></li>
<li><a class="" href="loss-functions.html"><span class="header-section-number">2</span> Loss functions</a></li>
<li><a class="" href="directed-acyclic-graphs-and-causal-reasoning.html"><span class="header-section-number">3</span> Directed Acyclic Graphs and Causal Reasoning</a></li>
<li><a class="" href="collider-bias.html"><span class="header-section-number">4</span> Collider bias</a></li>
<li><a class="" href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></li>
<li><a class="" href="information-criteria.html"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="active" href="bayesian-vs.-fequentist-statisics.html"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></li>
<li><a class="" href="mixtures.html"><span class="header-section-number">8</span> Mixtures</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">9</span> Instrumental Variables</a></li>
<li><a class="" href="parameters-combining-information-from-an-individual-with-population.html"><span class="header-section-number">10</span> Parameters: combining information from an individual with population</a></li>
<li><a class="" href="incorporating-measurement-error-a-rubber-band-metaphor.html"><span class="header-section-number">11</span> Incorporating measurement error: a rubber band metaphor</a></li>
<li><a class="" href="generalized-additive-models-as-continuous-random-effects.html"><span class="header-section-number">12</span> Generalized Additive Models as continuous random effects</a></li>
<li><a class="" href="flat-priors-the-strings-attached.html"><span class="header-section-number">13</span> Flat priors: the strings attached</a></li>
<li><a class="" href="unbiased-mean-versus-biased-variance-in-plain-english.html"><span class="header-section-number">14</span> Unbiased mean versus biased variance in plain English</a></li>
<li><a class="" href="probability-mass-versus-probability-density.html"><span class="header-section-number">15</span> Probability mass versus probability density</a></li>
<li><a class="" href="effective-degrees-of-freedom-number-of-parameters.html"><span class="header-section-number">16</span> Effective degrees of freedom / number of parameters</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alexander-pastukhov/notes-on-statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayesian-vs.-fequentist-statisics" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Bayesian vs. fequentist statisics<a class="anchor" aria-label="anchor" href="#bayesian-vs.-fequentist-statisics"><i class="fas fa-link"></i></a>
</h1>
<p>I suspect that many student who read “Statistical Rethinking” have a feeling that it is something completely different from what they have been learning in “traditional” statistics classes. That Bayesian approach is more “hands-on” and complicated, whereas “normal” statistics is simpler and easy to work with even it is “less powerful.”<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Not really.&lt;/p&gt;"><sup>17</sup></a> Thus, the purpose of this note is to walk you through a typical statistical analysis and focus on practical differences and, more importantly, similarity of the two approaches.</p>
<div id="choice-of-likelihood-both" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Choice of likelihood (both)<a class="anchor" aria-label="anchor" href="#choice-of-likelihood-both"><i class="fas fa-link"></i></a>
</h2>
<p>The very first we do is to look at the data and decide which distribution we will use to model the data / residuals be it normal, binomial, Poisson, beta, etc. That is the very first line of our models that goes like this
<span class="math display">\[
\color{red}{y_i \sim Normal(\mu_i, \sigma)} \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots \\
\sigma \sim Exponential(1)
\]</span></p>
<p>This decision is neither Bayesian, nor frequentist. This is a decision about the model that best describes the data, so it is independent of the inference method you will use. This is a decision that you are making even if you are using “prepackaged” statistical tests like the t-test or ANOVA that assume normally distributed residuals<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Admittedly, in this case people often start with the statistical test and see whether data is suitable rather than the other way around.&lt;/p&gt;"><sup>18</sup></a>.</p>
</div>
<div id="linear-model-both" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Linear model (both)<a class="anchor" aria-label="anchor" href="#linear-model-both"><i class="fas fa-link"></i></a>
</h2>
<p>Next, you decide on the deterministic part of the model that expresses how a parameter of the distribution you chose on the previous step is computed from various predictors. E.g., for the linear regression with normally distributed residuals, you decide which predictors do you use to compute the mean. The model line would look something like this
<span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\color{red}{\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso} \\
\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots
\]</span>
Again, this is neither Bayesian, nor frequentist decision, it is a linear model decision. Chapters 4-6 and 8 concentrate on how to make this decision using directed-acyclic graphs (DAGs) and introduce concepts of multicollinearity, colliders and bias they can produce, backdoor paths and how to identify them, etc. They explain how you can make educated decision on which predictors to use based on your knowledge of the field or of the problem. At this stage you also decide on whether to normalize data, as it could make interpreting the model easier.</p>
<p>You always have to make this decision. For example, if you use the (repeated measures) ANOVA, you do need to decide which factors to use, whether to include interactions, should you transform the data to make coefficients directly interpretable, do you use a link function, etc.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;However, you do see cases when one simply throws all factors and interactions into the pot with little regard for an underlying causal model or interpretability of the coefficients.&lt;/p&gt;"><sup>19</sup></a></p>
</div>
<div id="priors-optional-for-bayesian" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Priors (optional for Bayesian)<a class="anchor" aria-label="anchor" href="#priors-optional-for-bayesian"><i class="fas fa-link"></i></a>
</h2>
<p>Priors are a Bayesian way to regularize the model, so this is something you do need to think about when doing Bayesian statistics<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Modern packages like &lt;em&gt;brms&lt;/em&gt; make it easy for you by deducing a set of reasonable priors for you. However, it is always a good idea to double-check them.&lt;/p&gt;"><sup>20</sup></a>. In a model this part would look something like this
<span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\color{red}{\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots}
\]</span></p>
<p>This is probably a decision that students worry about the most as it feels more subjective and arbitrary than other decisions, such as choice of the likelihood or predictors. Chapter 4 through 7 gave you multiple examples that there is nothing particularly arbitrary about these choices and that you can come up with a set of justifiable priors based on what you know about the topic or based on how your pre-processed the data<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;In my experience, people tend to worry about priors for data unseen. Some kind of data of which you know absolutely nothing, hence, have trouble deducing priors. In practice, you always know something about the topic and the data. If not, you should read on it instead of using flat priors!&lt;/p&gt;"><sup>21</sup></a>.</p>
<p>Still, I think for a lot of people “normal” statistics with its flat priors feels simpler and also more objective and, therefore, more trustworthy (“we did not favor any specific range of values!”). If that is the case then use flat priors (but see the side note below) making Bayesian and frequentists models identical! For me, though, writing it down explicitly makes one realize that range <span class="math inline">\(-\infty, +\infty\)</span> is remarkably large to the point of being an obvious overkill
<span class="math display">\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\color{red}{\alpha \sim Uniform(-\infty, +\infty) \\
\beta_{x1} \sim Uniform(-\infty, +\infty) \\
\cdots}
\]</span></p>
<p>In short, Bayesian inference gives you an <em>option</em> to specify priors. You do not need to take on the option and can use flat frequentist’s priors.</p>
<p><em>Side note.</em> In reality, flat priors are never good priors. If there is sufficient data then, in most cases, the priors (flat or not) do not have much influence. However, if the data is limited then flat priors almost inevitably lead to overfitting as there is no additional information to counteract the effect of noise. This overfitting may feel more “objective” and “data-driven” than a more conservative underfitting of data via strongly regularizing priors but the latter is more likely to lead to better out-of-sample predictions and, therefore, are more likely to be replicated.</p>
</div>
<div id="maximum-likelihood-maximum-a-posteriori-estimate-both" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Maximum-likelihood / Maximum A Posteriori estimate (both)<a class="anchor" aria-label="anchor" href="#maximum-likelihood-maximum-a-posteriori-estimate-both"><i class="fas fa-link"></i></a>
</h2>
<p>Once you fitted the model, you get estimates for each parameter you specified. If you opted for flat priors, these estimates will be the same but for very minor differences due to sampling in Bayesian statistics. If you did specify regularizing priors then MAP estimates will be different from MLE, although the magnitude of that difference will depend the amount of data: the more data you have, the smaller the influence of the priors, we closer the estimates will be (see also the side note above). Importantly, both types of inferences will produce (very similar) estimates and you interpret their values the same way.</p>
</div>
<div id="uncertainty-about-estimates-different-but-comparable" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Uncertainty about estimates (different but comparable)<a class="anchor" aria-label="anchor" href="#uncertainty-about-estimates-different-but-comparable"><i class="fas fa-link"></i></a>
</h2>
<p>This is the point where the two approach fundamentally diverge. In case of frenquetists statistics you obtain confidence intervals and p-values based on appropriate statistics and degrees of freedom, whereas in case of Bayesian inference you obtain credible/compatibility intervals and can use posterior distribution for individual parameters to compute probability that they are strictly positive, negative, concentrated within a certain region around zero, etc.</p>
<p>These measures are conceptually different but tend to be interpreted similarly and mostly from Bayesian perspective. I think it is a good idea to compute and report all of them. If they are close, it would make you more certain about the results. More importantly, whenever they diverge it serves as a warning to investigate the case and what can cause this difference.</p>
</div>
<div id="model-comparison-via-information-criteria-both" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Model comparison via information criteria (both)<a class="anchor" aria-label="anchor" href="#model-comparison-via-information-criteria-both"><i class="fas fa-link"></i></a>
</h2>
<p>Both approaches use information criteria to compare models with Akaike and Bayesian/Schwarz information criteria being developed specifically for the case of flat priors of frenquetist models. Here, Bayesian approach holds an advantage as the full posterior allows for more elaborate information criteria such as DIC, WAIC, or LOO. Still the core idea and the interpretation of the comparison results are the same.</p>
</div>
<div id="generating-predictions-both" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Generating predictions (both)<a class="anchor" aria-label="anchor" href="#generating-predictions-both"><i class="fas fa-link"></i></a>
</h2>
<p>You generate predictions using the model definition which is the same for both approach. Hence, you are going to get similar predictions, at least for the mean (depending on your priors, see MLE vs. MAP above). As the two approach differ in how they characterize uncertainty, the uncertainty of predictions will be different but, typically, comparable.</p>
</div>
<div id="conclusions" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> Conclusions<a class="anchor" aria-label="anchor" href="#conclusions"><i class="fas fa-link"></i></a>
</h2>
<p>As you can see, from <em>practical</em> point of view, apart from optional Bayesian priors and different ways to quantify the uncertainty of estimates, the two approaches are the same. They require you making same decisions and the results are interpreted the same way. This lack of difference becomes even more apparent when you use software packages for running your statistical models. E.g., the way you specify your model in <code>lme4</code> (frenquetist) and <code>brms</code> (Bayesian) is very much the same to the point that in most cases you only need to change the function name (<code>lmer</code> to <code>brm</code> or vice versa) and leave the rest the same.</p>
<p>Thus, the point is that you should not be choosing between studying or doing frenquetists or Bayesian statistic. I feel more comfortable with Bayesian, mostly because it makes it easier interpret statistical significance. However, my typical approach is to start with frenquetist statistics (fast, good for shooting from a hip), once I am certain about my decisions (likelihood, model) I re-impliement the same model in Bayesian using informative priors and see whether results match (with reason). Then, I report both sets of inferences. This costs me remarkably little time precisely because there is so little difference between the two approaches from practical point of view!</p>
</div>
<div id="take-home-message-1" class="section level2" number="7.9">
<h2>
<span class="header-section-number">7.9</span> Take home message<a class="anchor" aria-label="anchor" href="#take-home-message-1"><i class="fas fa-link"></i></a>
</h2>
<p>We are not studying something completely different! We are merely approaching it from an unusual angle that leads to deeper understanding and interesting insights in the long run.</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="information-criteria.html"><span class="header-section-number">6</span> Information Criteria</a></div>
<div class="next"><a href="mixtures.html"><span class="header-section-number">8</span> Mixtures</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayesian-vs.-fequentist-statisics"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></li>
<li><a class="nav-link" href="#choice-of-likelihood-both"><span class="header-section-number">7.1</span> Choice of likelihood (both)</a></li>
<li><a class="nav-link" href="#linear-model-both"><span class="header-section-number">7.2</span> Linear model (both)</a></li>
<li><a class="nav-link" href="#priors-optional-for-bayesian"><span class="header-section-number">7.3</span> Priors (optional for Bayesian)</a></li>
<li><a class="nav-link" href="#maximum-likelihood-maximum-a-posteriori-estimate-both"><span class="header-section-number">7.4</span> Maximum-likelihood / Maximum A Posteriori estimate (both)</a></li>
<li><a class="nav-link" href="#uncertainty-about-estimates-different-but-comparable"><span class="header-section-number">7.5</span> Uncertainty about estimates (different but comparable)</a></li>
<li><a class="nav-link" href="#model-comparison-via-information-criteria-both"><span class="header-section-number">7.6</span> Model comparison via information criteria (both)</a></li>
<li><a class="nav-link" href="#generating-predictions-both"><span class="header-section-number">7.7</span> Generating predictions (both)</a></li>
<li><a class="nav-link" href="#conclusions"><span class="header-section-number">7.8</span> Conclusions</a></li>
<li><a class="nav-link" href="#take-home-message-1"><span class="header-section-number">7.9</span> Take home message</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alexander-pastukhov/notes-on-statistics/blob/master/06-bayesian-frequentist.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alexander-pastukhov/notes-on-statistics/edit/master/06-bayesian-frequentist.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Notes on Statistics</strong>" was written by Alexander Pastukhov. It was last built on 2021-11-10.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
