% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Notes on Statistics},
  pdfauthor={Alexander Pastukhov},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Notes on Statistics}
\author{Alexander Pastukhov}
\date{2022-04-28}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{precis}{%
\chapter{Precis}\label{precis}}

This is a collection of notes on statistics that attempt to provide further details and intuition for some topics, such as loss functions, information theory, information criteria, MCMC algorithms, etc. The primary source for these notes is ``Statistical Rethinking'' book by Richard McElreath, so if I mention a chapter, I mean a chapter from that book.

These notes are aimed primarily at future-me but I hope that might be useful others, which is why this material is \textbf{free to use} and is licensed under the \href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{Creative Commons Attribution-NonCommercial-NoDerivatives V4.0 International License}.

\hypertarget{loss-functions}{%
\chapter{Loss functions}\label{loss-functions}}

The purpose of this comment is to give you an intuition about loss functions, mentioned in chapter 3. In particular, I want you to understand why different loss functions (L0, L1, and L2) correspond to different point-estimates (mode, median, and mean). Plus, I want you to understand that you can view a choice of a likelihood function, as in picking Gaussian in chapter 4, as being analogous to picking a loss function.

I am afraid that the easiest way to explain why an \emph{L2} loss results in \emph{mean} is via a derivative. So, if you are not confident in your basic calculus skill, it might be useful for you to first watch a few episodes of \href{https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr}{Essense of Calculus} series by Grant Sanderson, a.k.a. \href{https://www.3blue1brown.com/}{3Blue1Brown}. I would suggest watching at least the first three episodes (actually, I would recommend to watch the whole series) but if you are short on time watch only episode 2\footnote{Although, if you skip episode 1, you won't know why it is \emph{obvious} that area of a circle is \(\pi\cdot r^2\)}.

\hypertarget{loss-function-the-concept}{%
\section{Loss function, the concept}\label{loss-function-the-concept}}

Imagine that you are selling ice-cream on a beach, so we can assume it is a narrow strip of sand and, therefore, a one-dimensional problem. It is hot, so \emph{everyone} wants ice-cream (obviously) and you want to maximize the number of ice-creams you sell (obviously). People are distributed in some random (not necessarily uniform or symmetric) way along the beach, so the question is: Where do you put your \emph{single} ice-cream stand to maximize your profits? The answer depends on your choice of the \emph{loss function} that describes how distance between a particular person and your stand influences whether person will buy your ice-cream. In other words, it describes the \emph{cost} of getting to your stand, i.e.~walking all-the-way through the sand in that heat. This \emph{cost} clearly depends on the distance and in the simplest case, it is linearly proportional to the distance: If you need to walk twice the distance, your costs for getting an ice-cream are twice as high. However, the relationship between the distance and cost does not have to be so simple and linear and this is why we have many different \emph{loss} / \emph{cost} functions.

We can write a loss/cost function more formally as \(L(stand, person_i)\) where \texttt{stand} is the location of your stand and \texttt{person\_i} is a location of a particular ith person. The cost can be either zero or positive, i.e., we assume there is no benefit in walking all the way, only no or some cost. So, where should you put your ice-cream stand?

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-where-to} \end{center}

\hypertarget{l0-mode}{%
\section{L0 (mode)}\label{l0-mode}}

The simplest loss function is\\
\[L0(stand, person_i) =  \begin{cases}
0, stand == person_i \\
\infty, stand \neq person_i
\end{cases}\]

This function assumes that everybody hates walking so much, that \emph{any} walk is unbearable and should be avoided. Thus, there is no cost for getting your ice-cream only for people who are positioned right next to your stand. For everybody else, even one meter away, the costs of walking are infinite, so they won't bother and, therefore, won't buy your ice-cream. Still, we are in the business of selling one, so where do we put our stand given how lazy our customers are? Well, we just find the biggest group of people and put our stand next to them. No one else will come but at least you got the biggest group of customers you could. If you look at the \emph{distribution} of your customers along the beach this is the highest peak (that you peak) and it is called the \emph{mode} of the distribution.

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-L0} \end{center}

\hypertarget{l1-median}{%
\section{L1 (median)}\label{l1-median}}

The next loss function, that I already mentioned, assumes a simple linear relationship between the distance and the cost
\[L1(stand, person_i) = |person_i - stand|\]
In other words, the cost is equal to distance (we need \texttt{\textbar{}\ \textbar{}} to get an absolute value, because the person could be ``to the left of'' of stand, in which case \texttt{person\ -\ stand} distance will be negative). So, where should we put our stand? Let us start at a fairly random location so that 3 of our customers are on the left and 7 are on the right.

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-L1-off-median} \end{center}

We can, in principle, compute the actual cost but it is simpler to ask the question of whether we can \emph{improve} on that cost by moving somewhere else? Imagine that we move to the left where \emph{minority} of our customers are. Now we have 1 on the left and 8 on the right (plus 2 more at our location).

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-L1-left} \end{center}

The problem is, we moved \emph{away} from the majority of the people so our total cost is \emph{original cost - 3 (improvement due to moving close to minority) + 8 (increase in loss due to moving away from majority)}, so \(\Delta L1 = +5\). Oops, we made it worse! How about moving to the \emph{right}?

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-L1-right} \end{center}

Now that we move \emph{towards} the majority of customers, we have four on the left and six on the right (plus one at our location). The change in cost is \emph{original cost + 4 (loss due to moving away from minority) - 6 (improvement due to moving towards majority)}, so \(\Delta L1 = -2\). Which gives us an idea: we should try to get even closer to that majority by keeping walking to the right! Eventually, you will get to point of the 50/50. Should you keep moving to the right? Should you move to the left? Should you move at all?

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-L1-median} \end{center}

There is no point in moving to the left. You just came from where because moving to the right made things better. However, if you keep moving to the right, you will keep passing people, so that majority now will be on the left and you would be walking \emph{away} from the majority, raising the costs (and your losses). So, once you get to point where half of your customers are on the left and half are on the right, you cannot do any better. Any movement that gets you from 50/50 means there are more customers on one side (say left, if you moved to the right) and, as we already figured out, your best strategy is to move towards the majority, which gets you back where you started at 50/50 point. That 50/50 points split, when half of customers / probability mass is on one side and half is on the other, is called \emph{median}.

\hypertarget{l2-mean}{%
\section{L2 (mean)}\label{l2-mean}}

The classic loss function is Euclidean distance \[L2(stand, person_i) = (person - stand)^2\]
Here, every next step becomes progressively harder for our customers. The cost of walking 1 meter is 1 (unit of effort). But walking 2 is \(2^2 = 4\) and is \(3^2=9\) for 3 meters. Thus, the penalty (cost/loss) for being further away from your stand increases as a power law. Still, one needs to sell ice-cream, so one needs to find the best spot where total cost is minimal

\[L2(stand, person) = \sum_{i=1}^{N}{(person_i - stand)^2}\]

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-L2-mean} \end{center}

Or, we can compute the minimal \emph{average} cost by dividing the sum by the total number of customers \texttt{N}:
\[<L2(stand, person)> = \frac{1}{N}\sum_{i=1}^{N}{(person_i - stand)^2}\]

Conceptually, you find that minimum by walking along the beach in the direction that reduces the cost until you hit the point where it start going up again. This strategy is called \emph{gradient descent} and, generally speaking, this is how computer finds minima computationally: They make steps in different directions to see which way is down and keep going until things start going up. However, in one-dimensional well-behaving case we have here things are even simpler as you can use calculus to figure out the solution analytically. If you watched the videos I advertised above, you'll know that the \emph{derivative} of the function is zero at the extrema (minima or maxima), so we just need to differentiate our average \emph{L2} over position of the stand and find where it is zero\footnote{I've nicked the derivations from {[}\url{https://stats.stackexchange.com/a/312997}{]}}.

\[\frac{\partial L2}{\partial stand} = -\frac{2}{N}\sum_{i=1}^{N}{(person_i - stand)}\]
As we want
\[\frac{\partial L2}{\partial stand} = 0\]
we state
\[\frac{2}{N}\sum_{i=1}^{N}{(person_i - stand)} = 0.\]

Opening up brackets and rearranging we get

\[- \frac{2}{N}\sum_{i=1}^{N}{person_i} + \frac{2 \cdot N}{N} \cdot stand = 0 \\
2 \cdot stand = \frac{2}{N}\sum_{i=1}^{N}{person_i} \\
stand = \frac{1}{N}\sum_{i=1}^{N}{person_i}\]

So, the optimal location of your stand is the \emph{mean}: an average location of all people on the beach.

\hypertarget{l1-median-vs.-l2-mean}{%
\section{L1 (median) vs.~L2 (mean)}\label{l1-median-vs.-l2-mean}}

One problem about the \emph{mean} is that it is sensitive to outliers. Because the costs grow as a power law, this approach favors a lot of medium-sized distances over lots of smalls ones plus one really large one. Thus, a single person at a far side of the beach would have a big influence on your stand's location (you already saw the difference in the example above). In data analysis, this means that those outliers will pull your estimates away from the majority of responses. Which is why it might be a good idea to consider using \texttt{median} rather than \texttt{mean}. If you distribution is symmetric, the difference will be negligible but in presence of outliers \texttt{median}, as a point-estimate, is more robust.

\hypertarget{choosing-a-likelihood}{%
\section{Choosing a likelihood}\label{choosing-a-likelihood}}

So far we talked about selling ice-cream on the beach but same question of choosing your loss function applies when you are trying to fit a distribution or a regression line, as in chapter 4. Here, you also have a point-estimate (regression line at each point) and you try to put it in such a way as to minimize the costs of having data points off that line (the distance from the point-estimate of the line and each data point is called a \emph{residual}). The classic way is to use \emph{L2} distance and the approach is called \emph{ordinary least squares}, as you try to minimize squared residuals.

Alternatively, you can express same costs-of-being-off-the-line using a distribution, such as Gaussian. You put its peak (mean) at the (candidate) location of your point estimate (that point has highest probability, so lowest cost) and the loss is computed as a probability of the residual (distance-to-the-point). You can think about it in terms of the probability that a person will go and buy ice-cream from your stand.

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-Gaussian} \end{center}

The Gaussian is special because it uses L2 distance, see \((x - \mu)^2\) inside the exponential:
\[f(x) = \frac{1}{\sigma \sqrt(2 \pi)}e^{\left(-\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2}\right)}\]

so using it is equivalent to fitting via ordinary least squares. However, as McElreath hinted, you can choose different likelihoods that are different not only in the distance-to-loss formula (like \emph{L1} is different from \emph{L2}) but also in symmetry. Both \emph{L1} and \emph{L2} (and Gaussian) ignore the sign of the distance. It does not matter whether customers are on the left or on the right. Other distributions, such as Beta, Gamma, or Log Normal are not symmetric, so the same distance will cost differently depending on the side the customer is at.

\begin{center}\includegraphics[width=1\linewidth]{images/loss-functions-Gamma} \end{center}

This allows you to think about the choice of your likelihood distribution in terms of choosing a loss function. Both describe how tolerant you are for points to be off the point estimate (regression line). For example, a t-distribution has heavier tails than a Gaussian (if you want to sound like a real mathematician, you say ``leptokurtic''), so its losses for outliers (penalty for larger residuals) are lower. Using it instead of a Gaussian would be similar to changing the loss function from L2 to be more like L1 (e.g.~\(|person_i - stand|^{1.5}\)). Conversely, you can pick a symmetric distribution that is narrower than a Gaussian to make residuals penalty even higher (e.g.~using \((person_i - stand)^{4}\)). You can also consider other properties: Should it be symmetric? Should it operate only within certain range (1..7 for a Likert scale, 0..1 for proportions, positive values for Gamma)? Should it weight all points equally? As you saw in the examples above, picking a different function moves your cart (regression line), so you should keep in mind that using a different likelihood will move the regression line and produce different estimates and predictions.

How do you pick a likelihood/loss function? It depends on the kind of data you have, on your knowledge about the process that generated the data, robustness of inferences in the presence of outliers, etc. However, most real-life cases you are likely to encounter will be covered by the distributions described in the book (Gaussian, exponential, binomial, Poisson, Gamma, etc.). After finishing the book, you will have a basic understanding of which are most appropiate in typical cases. The atypical cases you'll have to research yourself!

\hypertarget{gaussian-in-frenquentist-versus-bayesian-statistics}{%
\section{Gaussian in frenquentist versus Bayesian statistics}\label{gaussian-in-frenquentist-versus-bayesian-statistics}}

Later on in the book McElreath will note that erroneously assuming normal distribution for residuals ruins your inferences in frequentist statistics but not in Bayesian. This is because picking a distribution means different things in frequentist and Bayesian. As I wrote above, in the Bayesian case, likelihood is merely a loss function that translate distance from a data point to a regression line (residual) into a penalty (again, it determines just how tolerant you are for points off the line). Thus, you are using penalties for \emph{observed residuals} and having a bad loss function will make your posterior distribution suboptimal but you still can make inferences because it still is based on your actual residuals.

In contrast, in frequentist statistics, when you are stating that your observed residuals are a sample from a particular distribution, your actual residuals are used to determine parameters of this distribution. Then, however, you make your inferences using \emph{that distribution} not the residuals themselves. This is a very strong conjecture and probably the biggest leap of faith in frequentist statistics saying ``I know the true distribution''. Problem is, if you got your likelihood function / distribution wrong, your inferences are based on a model that describes \emph{something else} not your data. For example, you have a proportion data but you assume Gaussian distribution for residuals and build a model as if your residuals are always symmetrically distributed (not squashed on one side by floor or celing). That model will not be about your data, it will be about normally distributed \emph{something else}. The numbers for that \emph{something else} may look good (or bad) but they are not the numbers you are interested in. This is a mistake that is remarkably easy to do because computers won't stop you from making it. Think back to Chapter 1: Golems don't care! You can abuse any statistical model/test and they will simply spit out the numbers, even if tests are completely unsuitable for your data. Making sure that distribution is correct and that you are doing the right thing is on you, not the Golem!

\hypertarget{directed-acyclic-graphs-and-causal-reasoning}{%
\chapter{Directed Acyclic Graphs and Causal Reasoning}\label{directed-acyclic-graphs-and-causal-reasoning}}

\hypertarget{peering-into-a-black-box}{%
\section{Peering into a black box}\label{peering-into-a-black-box}}

To better understand the relationship between data and statistical analysis on the one hand and DAGs (directed acyclic graphs) on the other hand, I came up with an electric engineering metaphor\footnote{Dear electric engineers, yes, I know that's not quite how it works but it is still a good metaphor!}. Imagine that you have an electric device that you cannot take apart. However, there are certain points where you can connect your multimeter and check whether current flows between these two points. You also have a schematics for the device but you have no idea whether it is accurate. The names of the connector nodes match but \emph{the connectivity} between them is anybody's guess. So, what do you do? You look at the schematics and identify two nodes where current should \emph{definitely flow} and you measure whether that is the case. Do you have signal? Good, that means that \emph{at least with respect to the connectivity between these two nodes} you schematics is not completely wrong. No signal? Sorry, your schematics is no good, find a different one or try making one yourself. Conversely, you can identify two nodes, so that \emph{no current} should flow between them and measure it \emph{empirically}. No current? Good! Current? Bad, your schematics is wrong.

The relationship between the device and the schematics is that of the large (device) and small (schematics) world. Your job is to iteratively adjust the latter, so that it matches the former. You need to keep prodding the black box until predictions from your schematics start matching the actual readings. Only then you can say that you understand how device works and you can make predictions about what it will do under different conditions. Although testing based on schematics can be automated, generating such schematics is a creating process. It depends on our knowledge about devices of that kind and about individual exposed nodes.

Hopefully, you can see how it maps on our observational or experimental data (large world, readings from the device) and DAGs (small world, presumed schematics). As a scientist, you \emph{guess}\footnote{If you feel that science is not about guessing, you are wrong and I have Richard Feynman on my side! You always start by guessing a particular law or rule and then use empirical data to check whether your guess was correct. If you made an \emph{educated} guess, your chances of it being correct are higher, so your job is to study the field and prior work to make your guess as educated as possible. But, at the end, it is still a guess and it can be wrong. Good news, at least in my experience, is that guesses that turn out to be spectacularly wrong are the most informative ones, as they reveal something unexpected and, thus, hitherto unknown about the process.} the causal relationships between individual variables and you draw a schematics for that (a DAG). Using this DAG, you identify pairs of variables that must be dependent or independent \emph{assuming your DAG is correct} and check the data on whether this is indeed the case. It is? Good, your DAG is not (entirely) wrong! They are not? Bad news, their causal relationship is different from what you (or others in prior work) came up with. You need to modify DAG or draw a completely different one, just like a engineer must modify the schematics.

Note that this process is the opposite to that in a typical multivariate analysis approach using, say, ANOVA. In the latter case, you throw \emph{everything} together, including some/all interactions between the terms, and try figuring out causal relationship between individual independent and the dependent variable based on magnitude and significance of individual terms. So, \emph{first} you run the statistical analysis and \emph{then} you make your inferences about causal relationships. In causal reasoning using DAG, you \emph{first} formulate causal relationships and \emph{then} you use statistical analysis to check whether these relationship are correct. The second approach is much more powerful, because you can run ANOVA only once but you can formulate many DAGs that describe your data and test them iteratively, refining your understanding step-by-step. Moreover, ANOVA (or any other regression model like that) is about identifying relationships between invididual independent and the signle dependent variable (individual coefficients tell you how independent variables can be used to predict the dependent). DAG-based analysis allows you to predict and test relationship between pairs of \emph{dependent} variables as well, decomposing your complex model into \emph{testable} and \emph{verifyable} chunks. It is a more involved and time-consuming approach but it gives you much deeper understanding of the process you are styding compared to ``throw everything into ANOVA and hope it will magically figure it out''.

Moreover, causal calculus via DAGs has another trick up its sleeve. You can use \emph{conditional probabilities} (see below) to flip the relationship between variables. If two variables are independent, they may be dependent when conditioned on some third variable. Or vice versa, depedent variables can become \emph{conditionally independent}. This means that your predictions about connectivity between pairs of variables can be both more specific (e.g., they are related via a third variable or they both independently cause that third variable) and more testable, as you now have two \emph{opposite} predictions for the same pair of variables! You can check that current flows when it should (unconditional probability) and \emph{does not flow} once you condition it on the third variable. Both tests match? DAG is not that bad! One or none match? Back to the drawing board!

\hypertarget{turning-unconditional-dependence-into-conditional-independence}{%
\section{Turning unconditional dependence into conditional independence}\label{turning-unconditional-dependence-into-conditional-independence}}

Below, you will see how multiple regression can show conditional independence of two variable in case of the fork DAG in divorce data. However, there is another more general way to understand this concept it terms of conditional probabilities \(Pr(M|A)\) and \(Pr(D|A)\). For this, it helps to understand condition probabilities as \emph{filtering} operation. When we compute conditional probability for a \emph{specific} value of \(A\), this means that we slice the data, keeping only whose values of \(M\) and \(D\) that correspond to that chosen level of \(A\). It is easier to understand, if we visualize that conditional-probability-as-filtering in synthetic data. For illustration purposes, I will synthesize divorce data, keeping the relationships but I will space marriage age linearly and generate the data so that there are 20 data points for each age (makes it easier to see and understand).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{84321169}\NormalTok{)}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{180}
\NormalTok{sigma\_noise }\OtherTok{\textless{}{-}} \FloatTok{0.5}
\CommentTok{\# we repeat each value of age 10 times to make filtering operation easier to see}
\NormalTok{sim\_waffles }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{MedianAgeMarriage =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length.out=}\DecValTok{9}\NormalTok{), }\DecValTok{20}\NormalTok{), }
                      \AttributeTok{Divorce =} \FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, sigma\_noise),}
                      \AttributeTok{Marriage =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, sigma\_noise))}

\NormalTok{MD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Marriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}
  
\NormalTok{AD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}
  
\NormalTok{AM\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Marriage)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{)}

\NormalTok{MD\_plot }\SpecialCharTok{|}\NormalTok{ AD\_plot }\SpecialCharTok{|}\NormalTok{ AM\_plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-11-1} \end{center}

As you can see, all variables being dependent on each other, as in case of the original data. However, let us pick an arbitrary value, say \(A=1\)\footnote{The data is ``standardized'', therefore, age of 1 is one standard deviation away from the mean marriage rate.} and see which dots on the left plot will be selected via \emph{filtering} on that value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Marriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{alpha=}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{==} \DecValTok{1}\NormalTok{), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{!=} \DecValTok{1}\NormalTok{), }\AttributeTok{alpha=}\FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{) }
\NormalTok{AD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{==} \DecValTok{1}\NormalTok{), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{!=} \DecValTok{1}\NormalTok{), }\AttributeTok{alpha=}\FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}
  
\NormalTok{AM\_plot }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Marriage, }\AttributeTok{y=}\NormalTok{MedianAgeMarriage)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{==} \DecValTok{1}\NormalTok{), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{!=} \DecValTok{1}\NormalTok{), }\AttributeTok{alpha=}\FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{) }

\NormalTok{MD\_A1\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(MedianAgeMarriage }\SpecialCharTok{==} \DecValTok{1}\NormalTok{), }
         \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Marriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{color=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =} \StringTok{"Pr( |Mariage Age = 1)"}\NormalTok{)}

\NormalTok{(AD\_plot }\SpecialCharTok{|}\NormalTok{ MD\_plot) }\SpecialCharTok{/}
\NormalTok{(MD\_A1\_plot }\SpecialCharTok{|}\NormalTok{AM\_plot)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-12-1} \end{center}

First, take at top-left and bottom-right plots that plot, correspondingly, divorce and marriage rate versus marriage age. Note that I've flipped axes on the bottom-right plot, so that marriage rate is always mapped on x-axis. The \emph{red} dots in each plot correspond to divorce and marriage rates \emph{given that} (filtered on) marriage age is 1. The same dots are marked in red in the top-right plot and are plotted separately at the bottom-right. As you can see, the two variables \emph{conditional on A=1} are uncorrelated. Why? Because both were fully determined by marriage age and any variation (the spread of red dots vertically or horizontally in the top-left and bottom-right plots) was due to noise. Therefore, the plot on the bottom-left effectively plots noise in marriage rate versus noise in divorce rate and, by our synthetic data design, the noise in two variable was independent, hence, lack of correlation.

This opportunity to turn dependence into independence by conditioning two variables on the third is at the heart of causal reasoning\footnote{As you will learn later, the opposite is also true, so you can turn independence into conditional dependence.}. You draw a DAG and if it has a fork in it, you know that \emph{given your educated guess about causal relationship is correct} (small world!), your data (large world!) should show dependence \emph{and} conditional independence of the two variables (divorce and marriage rates). What if it does not? E.g., the two variables are always dependent or always independent, conditional or not? As we already discussed above, that just means that your educated guess was wrong and that relationship of the variables is different from how you thought it is. Thus, you need to go back to the drawing board, come up with another DAG, repeat the analysis and see if that new DAG is supported by data. If you have more variables, your DAGs will be more complex but the beauty of such causal reasoning is that you can concentrate on \emph{parts} of it, picking three variables and seeing whether your guess about these three variables was correct. This way, you can tinker with your causal model part-by-part, instead of hoping that you got \emph{everything} right on the very first attempt.

\hypertarget{spurious-association}{%
\chapter{Multiple regression - Spurious association}\label{spurious-association}}

These notes are on Chapter 5 ``The Many Variables \& The Spurious Waffles'', specifically, on section 5.1 ``Spurious association''. If you are to remember one thing from that chapter, it should be ``doing multiple regression is easy, understanding and interpreting it is hard''. As you will see below, fully understanding relationship between variables is complicate even when we are working with a minimal multiple regression with just two predictors.

When reading section 5.1 ``Spurious association'', I found relationships between the \emph{marriage age}, \emph{marriage rate}, and \emph{divorce rate} to be both clear and mysterious. On the one hand, everything is correlated with everything.

\begin{center}\includegraphics[width=1\linewidth]{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-14-1} \end{center}

On the other hand, once we fit linear model to predict divorce rate based on both median age marriage and marriage rate, the latter is \emph{clearly} irrelevant (output of code 5.11 shows that its coefficient is effectively zero, meaning that it is ignored) and, therefore, it has no causal influence on divorce rate.

If you are like me\footnote{Don't be like me, be better!}, you said ``Huh! But how does the model know that?''. And, at least for me, explanations in the chapter did not help much. The key figure is 5.4, that shows that (omitting intercept and slope symbols) \texttt{median\ age\ marriage\ =\ marriage\ rate\ +\ *extra\ information*\ +\ noise} but \texttt{marriage\ rate\ =\ median\ age\ marriage\ +\ noise}. In a nutshell, both variables code the same information but marriage rate is a noisier version of it, so it is ignored. Unfortunately, the answer ``but how?'' still stands. The figure 5.4, which shows fits on residuals is illustrative, but we do not fit residuals, we fit both variables at the same time \emph{without} fitting them on each other! Nowhere in the model 5.1.4 do we find \[\mu^{M}_{i} = \alpha_{AM} + \beta_{AM} * A_i\]

So, what's going on? \emph{How does it know?} To understand this, let us start with an issue of \emph{multicollinearity}.

\hypertarget{multicollinearity}{%
\section{Multicollinearity}\label{multicollinearity}}

To make things easier to understand, let us use simulated data. Imagine that both marriage and divorce rate are both \emph{caused by} marriage age and are almost perfectly linearly dependent it, so that \(D_i = \beta_A^{true} \cdot A_i\) (for the sake of simplicity \(\beta_A^{true} = -1\)) and \(M_i = -A_i\). The \emph{causal} relationship that we are modeling is called a fork:
\[Marriage~rate~\leftarrow~Age~of~marriage~\rightarrow~Divorce~rate\]
We pretend our variables are already standardized, so the plots would look something like this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"WaffleDivorce"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1212}\NormalTok{)}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(WaffleDivorce)}
\NormalTok{sim\_waffles }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{MedianAgeMarriage =} \FunctionTok{rnorm}\NormalTok{(N),}
                      \AttributeTok{Divorce =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, }\FloatTok{0.1}\NormalTok{),}
                      \AttributeTok{Marriage =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, }\FloatTok{0.01}\NormalTok{))}

\NormalTok{MD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Marriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}

\NormalTok{AD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}

\NormalTok{AM\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Marriage)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{)}

\NormalTok{MD\_plot }\SpecialCharTok{|}\NormalTok{ AD\_plot }\SpecialCharTok{|}\NormalTok{ AM\_plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-15-1} \end{center}

The relationship is the same as in the plots above but, as we assumed an almost perfect correlation, there is not much spread around the regression line. Still, by definition of \emph{how we constructed the data}, both marriage and divorce rate are \emph{caused} (computed from) median age and, importantly, marriage rate is \emph{never} used to compute the divorce rate. What happens if we analyze this simulated data using the same model 5.1.3, will it be able to figure ``marriage rate does not matter'' again?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_waffles }\OtherTok{\textless{}{-}}
\NormalTok{  sim\_waffles }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{A =}\NormalTok{ MedianAgeMarriage,}
         \AttributeTok{M =}\NormalTok{ Marriage,}
         \AttributeTok{D =}\NormalTok{ Divorce)}

\NormalTok{sim\_waffles\_fit }\OtherTok{\textless{}{-}} \FunctionTok{quap}\NormalTok{(}
  \FunctionTok{alist}\NormalTok{(}
\NormalTok{    D }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(mu , sigma) ,}
\NormalTok{    mu }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ bM}\SpecialCharTok{*}\NormalTok{M }\SpecialCharTok{+}\NormalTok{ bA}\SpecialCharTok{*}\NormalTok{A,}
\NormalTok{    a }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
\NormalTok{    bA }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{    bM }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{    sigma }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  ), }
  \AttributeTok{data =}\NormalTok{ sim\_waffles,}
\NormalTok{)}

\FunctionTok{precis}\NormalTok{(sim\_waffles\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              mean          sd        5.5%      94.5%
## a     0.002034578 0.012914402 -0.01860513 0.02267429
## bA    0.267403927 1.188973843 -1.63280591 2.16761377
## bM    1.245792079 1.187211226 -0.65160076 3.14318492
## sigma 0.090129176 0.008996557  0.07575094 0.10450741
\end{verbatim}

Oh no, we broke it! \(\beta_M\) is now about \texttt{1.25} rather than zero and \(\beta_A\) is around \texttt{0.27} rather than \texttt{-1}, as it should. Also note the uncertainty associated with both values, as they both overlap heavily with zero\footnote{I've made priors for both betas broad, so that they are not pushed towards zero too aggressively and uncertainty about them is more evident}. So, the data generation process is the same (\texttt{Divorce\ rate\ ←\ Age\ →\ Marriage\ rate}) and the model is the same (changes to priors have no particular impact in this case) but the ``magic'' of inferring the lack an ``causal arrow'' \texttt{Divorce\ rate\ \ →\ Marriage\ rate} is gone! The \emph{only} difference between the two data sets is extra variance (noise) in marriage rate variable, so let us see how the absence of that extra noise in simulated data breaks the magic.

When two variables, marriage age and rate in our case, are (almost) perfectly correlated (\(M = -A\)), that means that you can substitute one for another. Thus, we can rewrite\footnote{I've dropped likelihood and variance only to compress formulas and shed unimportant details. Adding them does not change the essence.}
\[D = \beta_A \cdot A + \beta_M \cdot M \\
D = \beta_A \cdot A + \beta_M \cdot (-A) \\
D = (\beta_A - \beta_M) \cdot A \\
D = \beta_A^{true} \cdot A\]
where
\[ \beta_A^{true} = (\beta_A - \beta_M)\]

That last bit is the curse of multicollinearity, because if two variable have \emph{the same} information, you are, effectively, fitting their \emph{sum}! This is equivalent to fitting the sum\footnote{in our case, the difference, because we defined that \texttt{M\ =\ -A}.} of coefficients times one of the variables and does not matter which one, since they are identical. We used \texttt{A} because we know that it causes \texttt{M}. If you look at the precis output above, you will see that we did fit the \(\beta_A^{true}\)! Since \texttt{bA\ =\ 0.27} and \texttt{bM\ =\ 1.25}, so plugging them in gives us
\[\beta_A^{true} = \beta_A - \beta_M = 0.27 - 1.25 = -0.98\]

Hey, that is the slope that we used to construct divorce rate, so fitting does work! Moreover, we can see that there is very little uncertainty about \(\beta_A^{true}\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{posterior\_samples }\OtherTok{\textless{}{-}} 
\NormalTok{  rethinking}\SpecialCharTok{::}\FunctionTok{extract.samples}\NormalTok{(sim\_waffles\_fit) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{bA\_true =}\NormalTok{ bA }\SpecialCharTok{{-}}\NormalTok{ bM)}

\FunctionTok{ggplot}\NormalTok{(posterior\_samples, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{bA\_true)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins=}\DecValTok{100}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"bA\_true = bA {-} bM"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-17-1.pdf}

But what about uncertainty for \emph{individual} slopes? It stems directly from the fact that \(\beta_A^{true} = \beta_A - \beta_M = -1\) (it is \texttt{-1} in our case, of course). There are infinite number of pairs of numbers whose difference would give -1: \(1-2\), \(2-3\), \((-200)-(-199)\), \(999.41-1000.41\), etc. All of them add up (subtract to) -1, so the fitting procedure cannot settle on any specific region for \emph{each} parameter and any specific pair of values. Any number will do, as long as the \emph{other one} differs by one.

\hypertarget{back-to-spurious-association}{%
\section{Back to spurious association}\label{back-to-spurious-association}}

Above, you have learned that if two variable have the same information, you can only fit \emph{both} of them but cannot get individual slopes. But wasn't that the case for real data we started with? Marriage age and rate \emph{are} correlated, so why fitting used one (age) and not their sum? The answer is \emph{extra noise} in marriage rate. In the real data marriage rate is age \emph{plus some noise}: \(M = -A + \epsilon\), where \(\epsilon\) is traditionally used to denote ``some noise''. How does that extra noise change our linear model for divorce rate?
\[D = \beta_A \cdot A + \beta_M \cdot M \\
D = \beta_A \cdot A + \beta_M (- A + \epsilon) \\
D = (\beta_A  - \beta_M ) \cdot A + \beta_M \cdot \epsilon\]

By definition, \(\epsilon\) is \emph{pure noise} and has zero predictive value with respect to divorce rate. Thus, if we would fit it \emph{alone}, we would expect to get a slope near zero, that is ``no significant relationship''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1231455}\NormalTok{)}
\NormalTok{sim\_waffles }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{MedianAgeMarriage =} \FunctionTok{rnorm}\NormalTok{(N),}
                      \AttributeTok{Divorce =} \FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, }\FloatTok{0.1}\NormalTok{),}
                      \AttributeTok{Marriage =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, }\FloatTok{0.01}\NormalTok{),}
                      \AttributeTok{epsilon =} \FunctionTok{rnorm}\NormalTok{(N))}

\FunctionTok{ggplot}\NormalTok{(sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{epsilon, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\FunctionTok{expression}\NormalTok{(epsilon)) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-18-1.pdf}

But we are not fitting it alone, as the coefficient \(\beta_M\) appears \emph{twice}:
\[D = (\beta_A  - \beta_M) \cdot A + \beta_M \cdot \epsilon\]
The latter part, \(\beta_M \cdot \epsilon\), pushes \(\beta_M\) towards zero slope, which is the best solution for pure noise, as you saw in the plot above. But the former part, \(\beta_A - \beta_M\) only needs to add up to \(\beta_A^{true}\), so however we fix \(\beta_M\), \(\beta_A\) can accommodate. Thus the closer \(\beta_M\) to zero, the closer is \(\beta_A\) to \(\beta_A^{true}\). And that's how the magic works! If one variable is other variable plus noise, that \emph{plus noise} induces extra penalty (extra residuals) and the only way to reduce residuals is to ignore the \emph{uncorrelated} noise by setting the slope to zero. Therefore, you ignore the variable as well, because it is merely a noisy twin of a better variable. You can see how added noise ``disambiguates'' the causal relationship\footnote{I've used ordinary least squares just to make simulations faster. You will get the same result using Bayesian fittings procedures.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{simulate\_waffles }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(sigma\_noise)\{}
  \CommentTok{\# generate same data but for noise in Marraige from Age relationship}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{169084}\NormalTok{)}
\NormalTok{  sim\_df }\OtherTok{\textless{}{-}}\NormalTok{ sim\_waffles }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{MedianAgeMarriage =} \FunctionTok{rnorm}\NormalTok{(N),}
                                  \AttributeTok{Divorce =} \FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, }\FloatTok{0.1}\NormalTok{),}
                                  \AttributeTok{Marriage =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, sigma\_noise))}
  
  \CommentTok{\# fit data using OLS and pulling out two slope coefficients}
  \FunctionTok{lm}\NormalTok{(Divorce }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Marriage }\SpecialCharTok{+}\NormalTok{ MedianAgeMarriage, }\AttributeTok{data=}\NormalTok{sim\_df) }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{summary}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    .}\SpecialCharTok{$}\NormalTok{coefficients }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{rownames\_to\_column}\NormalTok{(}\StringTok{"Variable"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{slice}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{LowerCI =}\NormalTok{ Estimate }\SpecialCharTok{{-}}\NormalTok{ Std..Error,}
           \AttributeTok{UpperCI =}\NormalTok{ Estimate }\SpecialCharTok{+}\NormalTok{ Std..Error) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(Variable, Estimate, LowerCI, UpperCI)}
\NormalTok{\}}

\NormalTok{simulated\_noise }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{epsilon =}\FunctionTok{exp}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\FloatTok{0.001}\NormalTok{), }\FunctionTok{log}\NormalTok{(}\FloatTok{0.3}\NormalTok{), }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(epsilon) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{do}\NormalTok{(}\FunctionTok{simulate\_waffles}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{epsilon))}

\FunctionTok{ggplot}\NormalTok{(simulated\_noise, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{epsilon, }\AttributeTok{y=}\NormalTok{Estimate)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{LowerCI, }\AttributeTok{ymax=}\NormalTok{UpperCI, }\AttributeTok{fill=}\NormalTok{Variable), }\AttributeTok{alpha=} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{Variable)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{name=}\FunctionTok{expression}\NormalTok{(epsilon)) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Slope estimate  ± standard error"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Marriage = MedianAgeMarriage + Normal(0, "}\NormalTok{, epsilon, }\StringTok{")"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-19-1.pdf}

The stripes show uncertainty (estimate ± standard error) and you can appreciate how quickly it is reduced as marriage rate becomes noisier and just how little noise is required for ``magic'' to start working and converge on the true causal relationship\footnote{It is true only in a sense that it matches the processes of creating the data. It is \emph{not} necessarily truly true for real data!}.

\hypertarget{chain-dag}{%
\section{Chain DAG}\label{chain-dag}}

So, a bit of noise will fix everything and we can \emph{know} causal relationships between the three variables? Not necessarily! Consider another possible causal diagram:
\[Marriage~rate~\rightarrow~Age~of~marriage~\rightarrow~Divorce~rate\]
Now marriage rate causes age of marriage that, in turn, causes divorce rate. Again, let us use synthetic data, so that we can be sure what causes what. However, we will add a fair amount of noise to make it more like real data and avoid multicoliniarity.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"WaffleDivorce"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{8973791}\NormalTok{)}
\NormalTok{N }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(WaffleDivorce)}
\NormalTok{sim\_waffles }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Marriage =} \FunctionTok{rnorm}\NormalTok{(N),}
                      \AttributeTok{MedianAgeMarriage =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, Marriage, }\FloatTok{0.2}\NormalTok{),}
                      \AttributeTok{Divorce =} \SpecialCharTok{{-}}\FunctionTok{rnorm}\NormalTok{(N, MedianAgeMarriage, }\FloatTok{0.2}\NormalTok{))}

\NormalTok{MD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Marriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}

\NormalTok{AD\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Divorce)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Divorce rate"}\NormalTok{)}

\NormalTok{AM\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{sim\_waffles, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{MedianAgeMarriage, }\AttributeTok{y=}\NormalTok{Marriage)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Median age marriage"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Marriage rate"}\NormalTok{)}

\NormalTok{MD\_plot }\SpecialCharTok{|}\NormalTok{ AD\_plot }\SpecialCharTok{|}\NormalTok{ AM\_plot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=1\linewidth]{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-20-1} \end{center}

The plots look very similar to those that we had before, so let us run the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim\_waffles }\OtherTok{\textless{}{-}}
\NormalTok{  sim\_waffles }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{A =}\NormalTok{ MedianAgeMarriage,}
         \AttributeTok{M =}\NormalTok{ Marriage,}
         \AttributeTok{D =}\NormalTok{ Divorce)}

\NormalTok{sim\_waffles\_fit }\OtherTok{\textless{}{-}} \FunctionTok{quap}\NormalTok{(}
  \FunctionTok{alist}\NormalTok{(}
\NormalTok{    D }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(mu , sigma) ,}
\NormalTok{    mu }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ bM}\SpecialCharTok{*}\NormalTok{M }\SpecialCharTok{+}\NormalTok{ bA}\SpecialCharTok{*}\NormalTok{A,}
\NormalTok{    a }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.2}\NormalTok{),}
\NormalTok{    bA }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{    bM }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{),}
\NormalTok{    sigma }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dexp}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{  ), }
  \AttributeTok{data =}\NormalTok{ sim\_waffles,}
\NormalTok{)}

\FunctionTok{precis}\NormalTok{(sim\_waffles\_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              mean         sd        5.5%       94.5%
## a      0.01141493 0.02370058 -0.02646318  0.04929304
## bA    -1.04399582 0.11019913 -1.22011532 -0.86787631
## bM    -0.05011269 0.12299879 -0.24668851  0.14646313
## sigma  0.16656407 0.01661272  0.14001374  0.19311440
\end{verbatim}

The model fit is virtually identical to that for the real data and, effectively, for a noisy simulated fork DAG. Yet, remember, that the underlying causal relationship between marriage rate and marriage age is now \emph{opposite}. In the fork DAG, age was causing marriage rate. Here, in the chain DAG, marriage rate for causing age. Moreover, the way we generated synthetic data, marriage rate causes divorce rate, although its effect is \emph{mediated} via marriage age. Yet, looking at the parameter values for \(\beta_M\) we might be tempted to conclude that marriage rate has no effect on divorce rate. To understand why, think about the relationship between marriage rate and age again. We designed it, so that \(A = -M + \epsilon\) and \(D = -A\) (we ignore the noise in the latter part for clarity). Substituting, we get \(D = M - \epsilon\) or, since we designed noise to be symmetric, we can also write \(D = M + \epsilon\). To put it differently, divorce rate are based on \emph{actual} values of age, which include noise. So, somewhat paradoxically, the cleaner version of the original variable is less correlated. If it still unclear, let me try with a metaphor. Imagine your friend sings you a song she heard. \(original \rightarrow friend \rightarrow you\). She remembered it wrong, so her version is somewhat different from the original. But because you are used to \emph{her} version of the song, the original, once you finally hear it, sounds wrong, as it does not have the distortions introduced by your friend.

\hypertarget{take-home-messages}{%
\section{Take-home messages}\label{take-home-messages}}

So, two DAGs, two \emph{differently} generated data sets, yet, one statistical model. Both DAGs agree that there is a direct causal path between marriage age and divorce but have opposite assumptions about causal relationship between marriage age and rate. What should we conclude from this? That it might be impossible to understand causal relationships between all variables even for the simplest case of just two predictors. The only thing to do is to embrace this ambiguity and be aware of it whenever you interpret regression models. So, you should use DAGs exactly for this purpose of understanding in how many ways can you generate the observed data. I understand that a simple unambiguous story is far more appealing\footnote{``Just tell me how things are!''} but as Jochen Braun of Magdeburg says: ``Do not fall for your own propaganda!''. Selecting just one story / DAG does not make them true. Considering all possible DAGs will likely lead to insights based on predictions they make. Even if your current data cannot decide between them, their different predictions for future experiments will help to solve the puzzle eventually.

Another take home message: Regression models cannot do magic. They can quantify correctional relationship and their signal-to-noise ratios but they do not know causality and they won't tell you which model is the ``true'' model. Imagine that we never measured marriage age, we would model divorce rate from marriage rate and would be quite pleased with the results. And, given how noisy the real data is, we probably did not consider some other \emph{very important} variables, those presence might render age as irrelevant as the marriage rate is now. Again, assuming the chain DAG
\[marriage~rate \rightarrow age \rightarrow variable~we~missed \rightarrow divorce~rate\]
Statistical models only work in a small world you created for them. Models are golems, they cannot and do not know about the large world. You do, so it is on you to understand both their limitations and power. Models can help you understand the process you are investigating but they won't understand it for you!

\hypertarget{the-haunted-dag}{%
\chapter{The haunted DAG}\label{the-haunted-dag}}

These are notes on section \textbf{6.3.2 The haunted DAG} that demonstrates how collider bias can arise, if one of the variables is unobserved and we do not realize that the have a collider in our DAG. The DAG itself is below, I've only changed \texttt{Unobserved} into \texttt{Neighborhood}.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-23-1.pdf}

The example in the book shows that if we include \texttt{Grandparents} while ``controlling for'' influence of \texttt{Parents} but ignore influence of the \texttt{Neighborhood}, the influence of \texttt{Grandparents} is \emph{negative} rather than \emph{zero}. It is zero in the book but it can originally be positive or negative, point is it is different from what is inferred by the model. And, importantly, an estimated effect of \texttt{Parents} is also different from its true value. The book shows it visually but I found that in this case algebra is helpful to understand it at the level of ``how does a regression model know''.

Let us start with a \emph{full} model. For the sake of simplicity, we will ignore coefficients for computing \texttt{Parents} variable from \texttt{Grandparents} and \texttt{Neighborhood} but this does not change the general picture.

\[ P = G + N\]
Thus, the linear model for child education is
\[C = \beta_P P + \beta_G G + \beta_N N\]

Substituting \$ P = G + N\$ gives us
\[C = \beta_P (G + N) + \beta_G G + \beta_N N\]

Rearranging terms a little
\[C = (\beta_P + \beta_G) G + (\beta_P  + \beta_N) N\]

Note that this means that we do not fit individual coefficients, in both cases we fit a \emph{sum} of two. And, as with multicollinearity, individual coefficients can be in a wrong place and unreliable as long as they add up to the ``true'' coefficient value. Thus, ignoring the noise and concealing the effect of \texttt{Parents}, we might as well fit
\[C = \beta_G^\prime G + \beta_N^\prime N\]
where, \(\beta_G^\prime = \beta_P + \beta_G\) and \(\beta_N^\prime = \beta_P + \beta_N\) are the total effect of grandparents and neighborhood of child education.

What if we ignore \texttt{Neighborhood}? This means that we explicitly set \(\beta_N = 0\) and that is the point when the sum of coefficients starts causing us problems. Recall that the model fits \(\beta_P + \beta_N\) and not each term separately. Thus, setting one of them to 0 does not upset the model, as it can always compensate with the \emph{other} coefficient. Here, that \emph{other} coefficient is \(\beta_P\), so its value now is that of the ``true'' sum: \(\beta_P = \beta_P^{true} + \beta_N^{true}\).

Unfortunately for us, \(\beta_P\) appears at \emph{two} places, as it is also used to quantify effect of grandparents:
\[(\beta_P + \beta_G) G\]

Originally, it reflected only the influence of parents, so it was not a problem. But now it is artificially inflated \footnote{In general, changed depending on the effect signs of individual effects.} as it stands for influence of both parents \emph{and} neighborhood. Problem? Not for a model that fits a \emph{sum}. How do you make sure that \emph{the sum} still adds up? You change \emph{other} coefficients! Here, we can still wiggle \(\beta_G\) so that everything adds up. Given that
\[\beta_P = \beta_P^{true} + \beta_N^{true}\]
model just needs to subtract that same \(\beta_N^{true}\) and get our sum back. So
\[\beta_P + \beta_G = (\beta_P^{true} + \beta_N^{true}) + (\beta_G^{true} - \beta_N^{true})\]

Thus, if we do not explicitly model the effect of neighborhood, it sneaks in nonetheless, hiding inside \emph{both} parent and grandparent coefficients. What makes it really problematic and confusing is that the effect is \emph{opposite} for the two terms: if we \emph{add} \(\beta_N^{true}\) at one place, we must \emph{subtract} it at the other.

So, what are you supposed to do with this new knowledge? You do not take fitted model coefficients at their face value. You always have a fine-print ``Only assuming that my small world is correct and it probably is not'' at the back of your head. You think of several models and think of ways to tease out the true relationship. DAGs and statistical models can help you, but they cannot do magic and tell the ``real truth'' by themselves.

\hypertarget{information-criteria}{%
\chapter{Information Criteria}\label{information-criteria}}

These are notes on information criteria. Their purpose is to provide some intuition about the information criteria, supplementing information presented in chapter 7 of the \emph{Statistical Rethinking} book by Richard McElreath. I deliberately oversimplified and overgeneralized certain aspects to paint a ``bigger picture''.

\hypertarget{deviance}{%
\section{Deviance}\label{deviance}}

In the chapter, deviance is introduced as an estimate for KL-divergence, which in turn is a relative entropy, i.e., the difference between cross-entropy and actual entropy of events. Keep that in mind but you could look at deviance itself as a straightforward goodness-of-fit measure, similar to squared residuals (RMSE, Root Mean Square Error) and coefficient of determination \(R^2\). In both cases, you have difference between model prediction (the regression line) and an actual data point. In ordinary least squares (OLS) approach, you quantify this imperfection of prediction by squaring residuals. You sum up all residuals to get the sum of squared residuals (\(SS_{res}\)) and then you can compute \(R^2\) by comparing it to the total sum of residuals in the data (\(SS_{total}\)):
\[R^2 = 1 - \frac{SS_{res}}{SS_{total}} = \frac{SS_{total} - SS_{res}}{SS_{total}}\]
As the total sum of residuals \(SS_{res}\) gets close to zero, the fraction gets close to 1. The disadvantage of squared residuals and \(R^2\) is that tricky to use with non-metric data, such as binomial, ordered categorical data, etc., or when deviations from the prediction might be asymmetric (binomial data, response times, etc.). Instead, you can use likelihood to compute the probability that a data point comes from a distribution defined by a model. Then, you compute the (total) joint probability by multiplying all probabilities or, better still, by computing the sum of their log-transform (log likelihood)\footnote{Mathematically equivalent but the sum of logs is far more numerically stable.}. Then, you can compare the observed log likelihood to the highest theoretically possible log likelihood for a \emph{saturated model} (\(\Theta_s\)), which has as many parameters as there are data points, so that it predicts each point perfectly. This is the original definition of (total) \emph{deviance}:
\[D = -2 \cdot (log(p(y|\Theta)) - log(p(y|\Theta_s)))\]

Recall that \(log(\frac{a}{b}) = log(a) - log(b)\), so we can rearrange it and see that it is a log-ratio of likelihoods:
\[D = -2 \cdot  log \left(\frac{p(y|\Theta)}{p(y|\Theta_s)}\right)\]
As \(p(y|\Theta)\) increases, the fraction inside get closer to 1. The \(log()\) bit flips and non-linearly scales it. The minus sign flips it back and we end up with smaller numbers meaning better fit.
\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-25-1.pdf}

The \(2\) is there to facilitate significance testing for \emph{nested} models. Models \(\Theta1\) and \(\Theta2\) are nested, if \(\Theta2\) has all predictors of \(\Theta1\) plus \emph{k} predictors. E.g., model \(Divorce = Marriage~Rate\) is nested inside \(Divorce = Marriage~Rate + Marriage~Age\) with later model having 1 more parameter. Your actual model \(\Theta\) is nested inside the saturated model \(\Theta_s\) that has \(k = n - k_{model}\) more parameters, where \(n\) is sample size and \(k_{model}\) is number of parameters in your model. It turns out that in this case, you can determine whether the difference in goodness-of-fit between two models is significant using \(\chi^2\) distribution with \emph{k} degrees of freedom. The only catch is that log-ratio is half the magnitude, so you need that \(2\) to match things up\footnote{I did not follow the derivation of that correspondence yet, so I cannot comment on how and why.}.

At this point, you might remember that the definition of deviance for a single model in the book was:
\[D = -2 \cdot log(p(y|\Theta))\]

Unfortunately, same term can be used both ways, to refer to a log likelihood of a single model or, technically more correctly, to the log-ratio you saw above. In reality, you will mostly see the deviance defined as in the book because it is used to compare nested models via \(\chi^2\) distribution as I've described above, with only difference that you compare any two nested models, not just to a saturated one. This is how nested models are frequently compared, for example, see \href{https://stat.ethz.ch/R-manual/R-patched/library/stats/html/anova.html}{anova()} function in R.

Note that a deviance for a single model still expresses the same idea of goodness-of-fit but is merely not normalized by the deviance of the saturated model. Thus deviance as in the book \(D = -2 \cdot log(p(y|\Theta))\) corresponds to sum of residuals (absolute values mean nothing but you can use them to compare two models on the same data), whereas \emph{total} deviance corresponds to the \(R^2\) (values are directly interpretable).

\hypertarget{general-idea-information-criteria-as-miles-per-gallon}{%
\section{General idea: information criteria as miles-per-gallon}\label{general-idea-information-criteria-as-miles-per-gallon}}

The general formula for all information criteria discussed below is
\[-2\cdot log \left( \frac{goodness~of~fit}{model~complexity} \right)\]

The goodness-of-fit in the numerator is the likelihood, the joint probability of observing the model given each data point \(\prod_i p(\Theta|y_i)\). The denominator expresses model complexity, i.e., its flexibility in fitting the sample and, therefore, its tendency to overfit. Thus, the fraction itself is \emph{goodness-of-fit per unit of model complexity}. This is like miles-per-gallon for car efficiency, so better models are more efficient models, churning out more goodness per complexity.

The numerator is the same\footnote{Almost, the minor difference is whether you use the entire posterior averaging over samples (DIC, WAIC, PSIS-LOO) or only a single maximum a posteriori sample (AIC, BIC).} for all information criteria discussed below and they differ only in how they compute the model complexity.

\hypertarget{akaike-information-criterion-aic}{%
\section{Akaike Information Criterion (AIC)}\label{akaike-information-criterion-aic}}

The formula most commonly used\footnote{And the one you should use to actually compute it, as it offers a better computation stability.} is
\[ AIC = -2\cdot log(p(\Theta|y)) + 2k \]
where \(k\) is the number of parameters of the model. If you are not a mathematician who is used to translate back-and-forth using logarithms, you may fail spot the ratio I was talking about earlier. For this, you need to keep in mind that \(log(\frac{a}{b}) = log(a) - log(b)\) and that \(a = log(exp(a))\). Let us re-arrange a bit to get the log-ratio back:
\[ AIC = -2\cdot log(p(\Theta|y)) + 2k \]
\[ AIC = -2 (log(p(\Theta|y)) - k)\]
\[ AIC = -2 (log(p(\Theta|y)) - log(exp(k))\]
\[ AIC = -2 \cdot log \left(\frac{p(\Theta|y)}{exp(k)} \right)\]
And here it is, the log-ratio I've promised! As you can see, AIC assumes that model complexity grows exponentially with the number of parameters.

If you are to use AIC, the current recommendation is to \emph{correct it} with an extra penalty for the size of the sample
\[AICc = AIC + \frac{2k^2 + 2k}{n - k - 1}\]
where \(n\) is the sample size. I won't do it here but you should be able to work out how it is added to the exponent in the denominator.

\hypertarget{bayesian-information-criterion-bic}{%
\section{Bayesian information criterion (BIC)}\label{bayesian-information-criterion-bic}}

A.k.a. Schwarz information criterion (SIC, SBC, SBIC)\footnote{McElreath writes that ``BIC\ldots it's not actually an ``information criterion.''``. So far, I was not able to figure out why.}. The motivation is the same as with AIC but the penalty (complexity term), in addition to the number of parameters, also reflects the sample size \(n\).

\[BIC = -2\cdot log(p(\Theta|y)) + log(n) k \]

Let us do re-arranging again
\[BIC = -2\cdot log(p(\Theta|y)) + log(n) k \]
\[BIC = -2 \left( log(p(\Theta|y)) + log(n) \frac{k}{2} \right) \]
\[BIC = -2 \left( log(p(\Theta|y)) - log \left(exp(log(n) \cdot \frac{k}{2} \right) \right) \]

For the complexity term, we need to keep in mind that \(exp(a \cdot b) = exp(a)^b\). Thus,
\[exp \left(log(n) \cdot \frac{k}{2} \right)= exp(log(n)) ^ \frac{k}{2} = n^\frac{k}{2}\]
Putting the complexity term back, we get
\[BIC = -2 \left( log(p(\Theta|y)) - log \left(n^\frac{k}{2} \right) \right)\]
\[BIC = -2 \cdot log \left(\frac{p(\Theta|y)}{n^\frac{k}{2}} \right)\]
Thus, we end up with very similar power law complexity term which uses sample size instead of Euler's number as the base.

\hypertarget{problem-of-aic-and-bic-one-size-may-not-fit-all}{%
\section{Problem of AIC and BIC: one size may not fit all}\label{problem-of-aic-and-bic-one-size-may-not-fit-all}}

Both AIC and BIC assume that model complexity and flexibility, that leads to overfitting, is reflected in the number of parameters \emph{k}. However, this is a fairly indirect measure of model flexibility, based on how models \emph{in general} tend to overfit data \emph{in general}. But you probably want to know how \emph{your specific} model uses its parameters to fit \emph{your specific} sample and how much overfitting you should expect in that specific case. Because even if a parameter is present in the model, it may not be able to fully use it in case of regularization or multilevel (adaptive regularization) models.

Regularization, in form of strong priors, lasso/ridge regression, etc., restricts the range of values that a given parameter can take. Thus, a model cannot exploit it as much as \emph{other} parameters and will be less able to use it to improve fit to the sample. Similarly, in hierarchical multilevel modeling, you may have dozens or hundreds of parameters that describe intercepts and/or slopes for individual participants (random factors, in general), but most of them could be trivially zero (same as or very similar to the group average) and contribute little to the actual fit. In these cases, a simple raw count, which treats all parameters as equals, will overestimate model complexity.

The desire to go beyond one-size-fits-all approach and be as model- and data-specific led to development of deviance information criterion (DIC) and widely-applicable information criterion (WAIC). Both use the entire posterior distribution of \emph{in-sample} deviance and base their penalty on how \emph{variable} this posterior distribution is. Higher variance, meaning that a model produces very different fits ranging from excellent to terrible, hints that model is too flexible for its own good and leads to higher complexity estimate (penalty for overfitting). Conversely, very similar posterior deviance (low variance) means that model is too restricted to fine-tune itself to the sample and its complexity is low.

If you understand why variance of the posterior distribution of divergence is related to models' flexibility and, therefore, to the number of effective parameters, just skip the next section. If not, I came up with a musical instruments metaphor that I and at least some people I've tested it upon found useful.

\hypertarget{musical-instruments-metaphor}{%
\section{Musical instruments metaphor}\label{musical-instruments-metaphor}}

Imagine that you are trying to play a song that you have just heard. But the only instrument you have is a triangle. This is not a particularly flexible instruments pitch-wise, so your rendition of that song will not be very good (your model underfits the data). The good news is that even if you do your worst and do not really try, no one will notice because your worst performance will sound very much like your best one. Simply because it is very hard to make songs sound different using a triangle. Thus, if you play that song many times, trying different versions of it with us judging how close you are to the original, the score we will give you will probably be not particularly high but very similar (low variance of the deviance for fitting to sample).

What if I give you an instrument that can vary the pitch at least a bit, like a xylophone for children. Now you have more freedom and your version of the music will sound much more like the original. But, it also gives you an opportunity to make a mess of it, so your rendition might sound nothing like the music you've just heard. In other words, a more flexbile instrument increases the difference between the best and the worst possible performance, so the variance of your performances (on how close they are to the original) also increases (higher variance of the deviance). A more flexible instrument will make the difference even bigger. Think violin or trombone which are not restricted to the scale, so you can play any sound in-between and you can match the music you just heard exactly. Imagine that the music your just heard has odd off-the-scale sounds. Was it a defect of the turntable, which cannot go at constant speed, so overall pitch wobbles overtime (noise)? Or is it an experimental music piece that was deliberately designed to sound odd (signal)? If you do not know for sure, you will try to play as close to the original music your heard as possible, matching those off-scale sounds. And, because you can play any sound, your range of possible performance is even larger from one-to-one to ``please, have mercy and stop!'' (even larger variance of deviance).

In short, variance of your performance (posterior divergence) reflects how flexible your instrument is. But why is it indicative of the \emph{effective} number of parameters? Here are regularizing priors in the world of music instrument metaphor. Imagine that in addition to the triangle, I also give you a rubber bell\footnote{It was used during a super-secret meeting in Stanislaw Lem's Eleventh Voyage of Ijon Tichy, so that no one would hear when they ring that bell!}. Now, technically you have two instruments (your number of parameters is 2) but that bell does not affect your performance (we put very strong regularizing priors so that coefficients are zero or something very-very close to zero). Thus, your ability to play the song did not change and your variance of performance stays the same. Two actual instruments, but only one ``effective'' one. Or, I give you a piano but allow you to use only one octave and only white keys. Yes, you have a piano but with this regularization it is as complex as as kids' xylophone. The \emph{potential} number of notes you can play is great (AIC and BIC would be very impressed and slap a heavy penalty on you) but the actual ``effective'' range is small. Or, you regularize yourself to play scale-only notes using violin (something you learn to do). In all of these cases, you deliberately restrict yourself. But why? Why not just play as you heard it? I.e., why not fit as well as you can? Because if the song you heard is short (sample is small), regularization based on your knowledge about real life helps you to ignore the noise that is always present. E.g., you know that song is for kids' xylophone, so even if you heard notes outside of a single octave that was probably a problem with recording. Or, you never heard that piece for violin but you do know other works of this composer and they always use scale-only notes, so you should not use violin to play off-scale sounds \emph{in that case}.

Multilevel models also limit the actual use of parameters. Imagine you heard a recording of a symphonic orchestra. Lots of violins but you figured out that most of them actually play the same melody. So you can get away with using one violin score (sample group average) and assume that most violins play like that (most participants are very close to group average). Any deviations from that group melody are probably mistakes by individual musicians, not the actual melody. Same goes if you hear a choir. Again, many people sing (lots of parameters!) but, mostly, in unison, so you do not need to create an individual score sheet (parameter) for each singer, just one per group of singers.

Wrapping up the metaphor, the more flexible your instrument is, the more variable your performance can be, the easier it is for you to mimic noise and imperfections of the recording that have nothing to do with the piece itself. But when you play it next time, matching the recording with all its noise and distortions perfectly, people who know the piece will be puzzled or may not even recognize it (poor out-of-sample predictions). Adopting the melody for a more limited instrument may make it easier for others to recognize the song! Thus, higher variance in performance accuracy (higher variance of deviance) indicates that you can overfit easily with that instrument (model) and you should be extra careful (impose higher penalty for complexity).

\hypertarget{deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic}{%
\section{Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)}\label{deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic}}

The two are very similar, as both compute the model complexity based on posterior distribution of log likelihood. The key difference is that DIC sums the log likelihood for each model (sample) first and then computes the variance over samples. WAIC computes variance of log likelihood per point and then sums those variances up. In the musical instrument metaphor, for DIC you perform the piece many times (generate many posterior samples), compute accuracy for each performance (deviance for a single sample), and then compute how variable they are. For WAIC, you go note by note (observation by observation). For each note you compute variance over all samples to see how consistent you are in playing it. Then, you sum this up.

\[DIC = -2 \cdot \left( log(p(y|\Theta)) - var(\sum log(p(y|\Theta_i))) \right)\]
\[WAIC = -2 \cdot \left( log(p(y|\Theta)) - \sum_i var(log(p(y_i|\Theta))) \right)\]

The penalty replaces \(k\) in AIC and, therefore, will go into the exponent inside the ratio. Again, same idea, that increase in variance of deviance (either per sample in DIC or per point in WAIC) leads to exponentially increasing estimate of complexity.

WAIC is more stable mathematically and is mode widely applicable (that's what statisticians tell us). Moreover, its advantage is that it explicitly recognizes that not all data points in your sample are equal. Some (outliers) are much harder to predict than others. And it is variance of log likelihood for these points that determines how much your model can overfit. An inflexible model will make a poor but consistent job (triangles don't care about pitch!), whereas a complex model can do anything from spot-on to terrible (violins can do anything). In short, you should use WAIC yourself but recognize DIC when you see it and think of it as somewhat less reliable WAIC, which is still better than AIC or BIC when you use regularizing priors and/or hierarchical models.

\hypertarget{importance-sampling}{%
\section{Importance sampling}\label{importance-sampling}}

Importance sampling is mentioned in the chapter but is never explained, so here is a brief description. The core idea is to pretend that you sample from a distribution you need (but have no access to or sampling from it directly is very inefficient) by sampling from another distribution (the one you have access to and that you can sample efficiently) and ``translating'' the probabilities via \emph{importance ratios}. What does this mean?

Imagine that you want to know an average total score for a given die after you throw it ten times. The procedure is as simple as it gets: you toss the die ten times, record the number you get on each throw, sum them up at the end. Repeat the same toss-ten-times-and-sum-it-up as many times as you want and compute your average. But what if you do not have access to that die because it is \emph{the die} and is kept under lock in International Bureau of Weights and Measures? Well, you have \emph{a die} which you can toss and you have a list of \emph{importance ratios} for each number. These \emph{important ratios} tell you how much more likely is the number for \emph{the die} (the one you are after) compared to \emph{a die} you have in your hand. Let's say the importance ratio for \emph{1} (so, number 1 comes up on top) is \texttt{3.0}. This means that whenever \emph{your die} gives you \emph{1}, you assume that \emph{the die} came up \emph{1} on \textbf{three} throws. If the importance ratio for \emph{2} is 0.5, whenever you see \emph{2} on your die, you record only half the throw (\emph{2} comes up twice as rarely for real die than for your die, so two throws that give you \emph{2} amount to a single throw). This way you can toss your die and every toss equates to different number of throws that generated the same number for \emph{the die}. So, you sample your die but record outcomes for the other die. Funny thing is that you don't even need to know how fair your die is and what is the probability of individual sides. As long as you know the importance ratios, keep tossing it and translating the probabilities, you will get the samples for \emph{the die} you are interested in.

Note that if you toss your die ten times, the translated number of tosses for \emph{the die} does not need to add up to ten. Imagine that, just by chance, you got \emph{1} four times. Given the importance ratio of \texttt{3.0} that alone translates into twelve tosses. Solution? You \emph{normalize} your result by \emph{sum of importance ratios} and get back you ten tosses.

The very obvious catch is, \emph{how do we know the importance ratios}? Well, that is situation specific. Sometimes, we can compute them because we know both distributions, it just that one is easier to sample than the target one, so, we optimize the use of computing power\footnote{A canonical example is using importance ratios to sample tails of a distribution, which you have access to but the data points from tails comes up by chance so rarely that sampling them directly is very inefficient.}. Sometimes, as in case of PSIS/LOO below, we can use an approximation.

\hypertarget{pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo}{%
\section{Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)}\label{pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo}}

The importance sampling I've described above is the key to the PSIS/LOO. The idea is the same, we want to sample from the posterior of the model that was fitted \emph{without} a specific data point \(y_i\) (we write it as \(p(\Theta_{-i,s}|y_i)\)). But we do not really want to refit the model. Instead, we want to use what we already have, samples from the model that was fit on all the data, \emph{including} point \(y_i\). So, wise minds figured out how to use the importance sampling trick, sampling from \(p(\Theta_s|y_i)\) and translating it to \(p(\Theta_{-i,s}|y_i)\). The only thing we need are importance factors
\[r_i = \frac{p(\Theta_{-i,s}|y_i)}{p(\Theta_s|y_i)} \propto \frac{1}{p(y_i|\Theta_s)}\]

The importance ratio tells you the worse you are at predicting a point \(y_i\) \emph{in-sample} (the smaller the \(p(y_i|\Theta_s)\) is), the more important it is when you consider model's performance \emph{out-of-sample} (the larger is \(\frac{1}{p(y_i|\Theta_s)}\)). Here is an intuition behind this. \emph{Any} observation will be harder to predict, if it was not included into the data the model was trained on. This is because, in its absence the model will use its parameters to fit the data that is present, including fitting noise, if it has spare parameters. So, you expect that out-of-sample deviance (\(p(y_i|\Theta_{-i}\)) should be always worse than in-sample deviance for the same observation (\(p(y_i|\Theta\)). How much worse depends on how ``typical'' the observation is. If it is typical and ``easy'' for a model to predict, in its absence the model will still see many similar ``typical'' observations and will be well prepared to predict it. However, if the observations is atypical, an outlier, the model won't see too many observations that are alike and will concentrate more on typical points.

More specifically, the penalty for a particular point based on the importance ratios across all samples reflects how \emph{variable} \(p(y_i|\Theta_s)\) is across the samples. Recall the definition in the book
\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}r(\theta_s)p(y_i|\theta_s)}{\sum^S_{s=1}r(\theta_s)}\]
Substituting the \(r_i = \frac{1}{p(y_i|\Theta_s)}\) we get
\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}p(y_i|\theta_s)}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]
\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}\frac{p(y_i|\theta_s)}{p(y_i|\Theta_s)}}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]
\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}1}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]

\[lppd_{IS} = \sum^N_{i=1} log \frac{S}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]
\[lppd_{IS} = \sum^N_{i=1} log \frac{1}{\frac{1}{S}\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]

This looks like some bizarre re-arrangement of the terms but it ends up producing a point-wise variance-based penalty very similar to WAIC. You can build your intuition for this by playing with easy to compute numbers. Let us take a vector where all values are the same, i.e., variance is zero and compute \(lppd\) and \(lppd_{IS}\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.2}\NormalTok{) }\CommentTok{\# mean = 0.2, variance = 0}
\NormalTok{logp }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(p)}
\NormalTok{lppd }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{(p)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(p))}
\NormalTok{lppd\_IS }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ ( (}\DecValTok{1} \SpecialCharTok{/} \FunctionTok{length}\NormalTok{(p)) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{( }\DecValTok{1}\SpecialCharTok{/}\NormalTok{ p)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## mean = 0.2, variance of log(p) = 0
\end{verbatim}

\begin{verbatim}
## lppd    = -1.609438
\end{verbatim}

\begin{verbatim}
## lppd_IS = -1.609438
\end{verbatim}

\begin{verbatim}
## lppd - lppd_IS =  0
\end{verbatim}

Now let us keep \emph{mean} the same but very slightly increase \emph{variance}. As you can see that tiny increase in sample induces a small \emph{decrease} in \(lppd_{IS}\), i.e., we assume that since the model is variable about this point, it probably has too much power leading to \emph{in-sample} overfitting and, therefore, poor \emph{out-of-sample} performance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.19}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.21}\NormalTok{)}
\NormalTok{logp }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(p)}
\NormalTok{lppd }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{(p)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(p))}
\NormalTok{lppd\_IS }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ ( (}\DecValTok{1} \SpecialCharTok{/} \FunctionTok{length}\NormalTok{(p)) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{( }\DecValTok{1}\SpecialCharTok{/}\NormalTok{ p)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## mean = 0.2, variance of log(p) = 0.001669798
\end{verbatim}

\begin{verbatim}
## lppd    = -1.609438
\end{verbatim}

\begin{verbatim}
## lppd_IS = -1.611107
\end{verbatim}

\begin{verbatim}
## lppd - lppd_IS =  0.001669449
\end{verbatim}

To understand where does the penalty comes from, it was helpful for me to derive the solution for the case of just two values \([p_1, p_2]\). In this case you can define them as difference to the their mean, just as we did above: \(p_1 = \mu - \epsilon\) and \(p_2 = \mu + \epsilon\). The \(lppd\) should be trivially equal to \(\mu\):
\[lppd = \frac{p_1 + p_2}{2}\]
\[lppd =\frac{(\mu - \epsilon) + (\mu + \epsilon)}{2}\]
\[lppd =\frac{2 \cdot \mu}{2}\]
\[lppd = \mu\]

What about \(lppd_{IS}\)?
\[lppd_{IS} = \frac{2}{\frac{1}{p_1} + \frac{1}{p_2}}\]

\[lppd_{IS} = \frac{2}{\frac{1}{\mu - \epsilon} + \frac{1}{\mu + \epsilon}}\]

Bringing the two fractions to a common denominator we get
\[lppd_{IS} = \frac{2}{\frac{(\mu + \epsilon) + (\mu - \epsilon)}{(\mu + \epsilon)(\mu - \epsilon)}}\]
Opening the brackets in the numerator
\[lppd_{IS} = \frac{2}{\frac{2 \cdot \mu }{(\mu + \epsilon)(\mu - \epsilon)}}\]
Now we can get flip the bottom fraction
\[lppd_{IS} = \frac{2 \cdot \frac{(\mu + \epsilon)(\mu - \epsilon)}{2 \cdot \mu}}{\frac{2 \cdot \mu }{(\mu + \epsilon)(\mu - \epsilon)} \cdot \frac{(\mu + \epsilon)(\mu - \epsilon)}{2 \cdot \mu}} \]

\[lppd_{IS} = 2 \cdot \frac{(\mu + \epsilon)(\mu - \epsilon)}{2 \cdot \mu}\]
The two goes away
\[lppd_{IS} = \frac{(\mu + \epsilon)(\mu - \epsilon)}{\mu}\]
Opening the brackets in the numerator
\[lppd_{IS} = \frac{\mu^2 + \mu\epsilon - \mu\epsilon - \epsilon^2}{\mu}\]
Simplifying
\[lppd_{IS} = \frac{\mu^2 - \epsilon^2}{\mu}\]
\[lppd_{IS} = \mu - \frac{\epsilon^2}{\mu}\]

Thus when variance (\(\epsilon\), deviation from the mean) increases, the \(lppd_{IS}\) decreases

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delta }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.0}\NormalTok{, }\FloatTok{0.19}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}

\NormalTok{create\_p }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dp) \{}\FloatTok{0.2} \SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{dp, }\DecValTok{0}\NormalTok{, dp)\}}

\NormalTok{lppd\_df }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Variance =}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{map\_dbl}\NormalTok{(delta, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{var2}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\FunctionTok{create\_p}\NormalTok{(.)))),}
         \AttributeTok{LPPD =}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{map\_dbl}\NormalTok{(delta, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(}\FunctionTok{sum}\NormalTok{(}\FunctionTok{create\_p}\NormalTok{(.))}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(}\FunctionTok{create\_p}\NormalTok{(.)))),}
         \AttributeTok{Kind =} \StringTok{"lppd"}\NormalTok{),}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Variance =}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{map\_dbl}\NormalTok{(delta, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{var2}\NormalTok{(}\FunctionTok{log}\NormalTok{(}\FunctionTok{create\_p}\NormalTok{(.)))),}
       \AttributeTok{LPPD =}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{map\_dbl}\NormalTok{(delta, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{log}\NormalTok{(}\DecValTok{1} \SpecialCharTok{/}\NormalTok{ ( (}\DecValTok{1} \SpecialCharTok{/} \FunctionTok{length}\NormalTok{(}\FunctionTok{create\_p}\NormalTok{(.))) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{( }\DecValTok{1}\SpecialCharTok{/} \FunctionTok{create\_p}\NormalTok{(.))))),}
       \AttributeTok{Kind =} \StringTok{"lppd\_IS"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{SD =} \FunctionTok{sqrt}\NormalTok{(Variance))}

\NormalTok{together\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(lppd\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Variance, }\AttributeTok{y=}\NormalTok{LPPD, }\AttributeTok{color=}\NormalTok{Kind)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"var log p (WAIC penalty)"}\NormalTok{)}

\NormalTok{difference\_plot }\OtherTok{\textless{}{-}}
\NormalTok{  lppd\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Kind, }\AttributeTok{values\_from =}\NormalTok{ LPPD) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{lppd\_IS {-} lppd}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ lppd\_IS }\SpecialCharTok{{-}}\NormalTok{ lppd) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Variance, }\AttributeTok{y=}\StringTok{\textasciigrave{}}\AttributeTok{lppd\_IS {-} lppd}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"var log p (WAIC penalty)"}\NormalTok{)}

\NormalTok{together\_plot }\SpecialCharTok{+}\NormalTok{ difference\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-30-1.pdf}

Compare the \(var log p(y_i|\Theta_s)\), which is a WAIC penalty term, with the decrease due to variance-based importance ratios. As you can see the two are very close and, therefore, will produce very similar estimates of out-of-sample performance. A \emph{Pareto smoothed} importance sampling has an advantage over WAIC is that it less keen on reducing the performance based on a few samples, which are smoothed away. Still, both methods should identify the same problematic (high variance) data points that you should pay closer attention to.

\hypertarget{bayes-factor}{%
\section{Bayes Factor}\label{bayes-factor}}

Not an information criterion. However, it is a popular way to compare Bayesian models. Compared to information criteria, the logic is reversed. In case of the information criteria, we are asking which \textbf{model fits data} the best given the penalty we impose for its complexity. In case of Bayes Factor, we already have two models (could be different models with different number of parameters or just with different parameter values) and we are interested how well the \textbf{data matches models} we already have.

Let's start with the Bayes theorem:
\[Pr(M|D)=\frac {\Pr(D|M)\Pr(M)}{\Pr(D)}\]
where, \emph{D} is data and \emph{M} is the model (hypothesis). The tricky part is the marginal probability (prior) of data \(Pr(D)\). We hardly ever know it for sure, making computing the ``correct'' value for \(Pr(M|D)\) problematic. When using posterior sampling, we side-step the issue by ignoring it and normalizing the posterior by the sum of the posterior distribution. Alternatively, when comparing two models, you can compute their ratio:
\[{\frac {\Pr(D|M_{1})}{\Pr(D|M_{2})}}={\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}} \cdot {\frac {\Pr(M_{1})}{\Pr(M_{2})}}\]
here \(\frac{\Pr(D|M_{1})}{\Pr(D|M_{2})}\) are \textbf{posterior odds}, \(\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}\) is \textbf{Bayes Factor}, and \(\frac {\Pr(M_{2})}{\Pr(M_{1})}\) are \textbf{prior odds}. The common \(Pr(D)\) nicely cancels out!

If you assume that both hypotheses/models are equally likely (you have flat priors), the prior odds are 1:1 and your posterior odds are equal to Bayes Factor or, vice versa, Bayes Factor is equal to posterior odds. This means you can just pick their likelihoods from the posterior sampled distribution and compute the ratio.

I am not a big fan of Bayes Factor for conceptual reasons. Although it can compare any two models (as long as the sample is the same), it looks a lot like a Bayesian version of a p-value and, therefore, lends itself naturally to the null-hypothesis testing. And, as far as my reading of literature in my field is concerned, this is how people most frequently use it, as a cooler Bayesian way of null-hypothesis testing. You have no worries about multiple comparisons (it is Bayesian, so no need for error correction!) and it can prove null hypothesis (it is the ratio, so flip it and see how much stronger \emph{H0} is)! There is nothing wrong with this per se but the advantage of Bayesian statistics and information criteria is that you do not \emph{need} to think in terms of null hypothesis testing and nested models. Adopting Bayes Factor may prevent you from seeing this and will allow you to continue doing same analysis just in a differently colored wrapper. Again, there is nothing wrong with exploratory analysis using null hypothesis testing until you can formulate a better model. But it should not be the \emph{only} way you approach modeling.

\hypertarget{bayesian-vs.-fequentist-statisics}{%
\chapter{Bayesian vs.~fequentist statisics}\label{bayesian-vs.-fequentist-statisics}}

I suspect that many student who read ``Statistical Rethinking'' have a feeling that it is something completely different from what they have been learning in ``traditional'' statistics classes. That Bayesian approach is more ``hands-on'' and complicated, whereas ``normal'' statistics is simpler and easy to work with even it is ``less powerful.''\footnote{Not really.} Thus, the purpose of this note is to walk you through a typical statistical analysis and focus on practical differences and, more importantly, similarity of the two approaches.

\hypertarget{choice-of-likelihood-both}{%
\section{Choice of likelihood (both)}\label{choice-of-likelihood-both}}

The very first we do is to look at the data and decide which distribution we will use to model the data / residuals be it normal, binomial, Poisson, beta, etc. That is the very first line of our models that goes like this
\[
\color{red}{y_i \sim Normal(\mu_i, \sigma)} \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots \\
\sigma \sim Exponential(1)
\]

This decision is neither Bayesian, nor frequentist. This is a decision about the model that best describes the data, so it is independent of the inference method you will use. This is a decision that you are making even if you are using ``prepackaged'' statistical tests like the t-test or ANOVA that assume normally distributed residuals\footnote{Admittedly, in this case people often start with the statistical test and see whether data is suitable rather than the other way around.}.

\hypertarget{linear-model-both}{%
\section{Linear model (both)}\label{linear-model-both}}

Next, you decide on the deterministic part of the model that expresses how a parameter of the distribution you chose on the previous step is computed from various predictors. E.g., for the linear regression with normally distributed residuals, you decide which predictors do you use to compute the mean. The model line would look something like this
\[
y_i \sim Normal(\mu_i, \sigma) \\
\color{red}{\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso} \\
\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots
\]
Again, this is neither Bayesian, nor frequentist decision, it is a linear model decision. Chapters 4-6 and 8 concentrate on how to make this decision using directed-acyclic graphs (DAGs) and introduce concepts of multicollinearity, colliders and bias they can produce, backdoor paths and how to identify them, etc. They explain how you can make educated decision on which predictors to use based on your knowledge of the field or of the problem. At this stage you also decide on whether to normalize data, as it could make interpreting the model easier.

You always have to make this decision. For example, if you use the (repeated measures) ANOVA, you do need to decide which factors to use, whether to include interactions, should you transform the data to make coefficients directly interpretable, do you use a link function, etc.\footnote{However, you do see cases when one simply throws all factors and interactions into the pot with little regard for an underlying causal model or interpretability of the coefficients.}

\hypertarget{priors-optional-for-bayesian}{%
\section{Priors (optional for Bayesian)}\label{priors-optional-for-bayesian}}

Priors are a Bayesian way to regularize the model, so this is something you do need to think about when doing Bayesian statistics\footnote{Modern packages like \emph{brms} make it easy for you by deducing a set of reasonable priors for you. However, it is always a good idea to double-check them.}. In a model this part would look something like this
\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\color{red}{\alpha \sim Normal(0, 1) \\
\beta_{x1} \sim Exponential(1) \\
\cdots}
\]

This is probably a decision that students worry about the most as it feels more subjective and arbitrary than other decisions, such as choice of the likelihood or predictors. Chapter 4 through 7 gave you multiple examples that there is nothing particularly arbitrary about these choices and that you can come up with a set of justifiable priors based on what you know about the topic or based on how your pre-processed the data\footnote{In my experience, people tend to worry about priors for data unseen. Some kind of data of which you know absolutely nothing, hence, have trouble deducing priors. In practice, you always know something about the topic and the data. If not, you should read on it instead of using flat priors!}.

Still, I think for a lot of people ``normal'' statistics with its flat priors feels simpler and also more objective and, therefore, more trustworthy (``we did not favor any specific range of values!''). If that is the case then use flat priors (but see the side note below) making Bayesian and frequentists models identical! For me, though, writing it down explicitly makes one realize that range \(-\infty, +\infty\) is remarkably large to the point of being an obvious overkill
\[
y_i \sim Normal(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{x1} \cdot X1 + \beta_{x2} \cdot X2 + \beta_{x1\cdot x2} \cdot X1 \cdot X2 \dotso \\
\color{red}{\alpha \sim Uniform(-\infty, +\infty) \\
\beta_{x1} \sim Uniform(-\infty, +\infty) \\
\cdots}
\]

In short, Bayesian inference gives you an \emph{option} to specify priors. You do not need to take on the option and can use flat frequentist's priors.

\emph{Side note.} In reality, flat priors are never good priors. If there is sufficient data then, in most cases, the priors (flat or not) do not have much influence. However, if the data is limited then flat priors almost inevitably lead to overfitting as there is no additional information to counteract the effect of noise. This overfitting may feel more ``objective'' and ``data-driven'' than a more conservative underfitting of data via strongly regularizing priors but the latter is more likely to lead to better out-of-sample predictions and, therefore, are more likely to be replicated.

\hypertarget{maximum-likelihood-maximum-a-posteriori-estimate-both}{%
\section{Maximum-likelihood / Maximum A Posteriori estimate (both)}\label{maximum-likelihood-maximum-a-posteriori-estimate-both}}

Once you fitted the model, you get estimates for each parameter you specified. If you opted for flat priors, these estimates will be the same but for very minor differences due to sampling in Bayesian statistics. If you did specify regularizing priors then MAP estimates will be different from MLE, although the magnitude of that difference will depend the amount of data: the more data you have, the smaller the influence of the priors, we closer the estimates will be (see also the side note above). Importantly, both types of inferences will produce (very similar) estimates and you interpret their values the same way.

\hypertarget{uncertainty-about-estimates-different-but-comparable}{%
\section{Uncertainty about estimates (different but comparable)}\label{uncertainty-about-estimates-different-but-comparable}}

This is the point where the two approach fundamentally diverge. In case of frenquetists statistics you obtain confidence intervals and p-values based on appropriate statistics and degrees of freedom, whereas in case of Bayesian inference you obtain credible/compatibility intervals and can use posterior distribution for individual parameters to compute probability that they are strictly positive, negative, concentrated within a certain region around zero, etc.

These measures are conceptually different but tend to be interpreted similarly and mostly from Bayesian perspective. I think it is a good idea to compute and report all of them. If they are close, it would make you more certain about the results. More importantly, whenever they diverge it serves as a warning to investigate the case and what can cause this difference.

\hypertarget{model-comparison-via-information-criteria-both}{%
\section{Model comparison via information criteria (both)}\label{model-comparison-via-information-criteria-both}}

Both approaches use information criteria to compare models with Akaike and Bayesian/Schwarz information criteria being developed specifically for the case of flat priors of frenquetist models. Here, Bayesian approach holds an advantage as the full posterior allows for more elaborate information criteria such as DIC, WAIC, or LOO. Still the core idea and the interpretation of the comparison results are the same.

\hypertarget{generating-predictions-both}{%
\section{Generating predictions (both)}\label{generating-predictions-both}}

You generate predictions using the model definition which is the same for both approach. Hence, you are going to get similar predictions, at least for the mean (depending on your priors, see MLE vs.~MAP above). As the two approach differ in how they characterize uncertainty, the uncertainty of predictions will be different but, typically, comparable.

\hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

As you can see, from \emph{practical} point of view, apart from optional Bayesian priors and different ways to quantify the uncertainty of estimates, the two approaches are the same. They require you making same decisions and the results are interpreted the same way. This lack of difference becomes even more apparent when you use software packages for running your statistical models. E.g., the way you specify your model in \texttt{lme4} (frenquetist) and \texttt{brms} (Bayesian) is very much the same to the point that in most cases you only need to change the function name (\texttt{lmer} to \texttt{brm} or vice versa) and leave the rest the same.

Thus, the point is that you should not be choosing between studying or doing frenquetists or Bayesian statistic. I feel more comfortable with Bayesian, mostly because it makes it easier interpret statistical significance. However, my typical approach is to start with frenquetist statistics (fast, good for shooting from a hip), once I am certain about my decisions (likelihood, model) I re-impliement the same model in Bayesian using informative priors and see whether results match (with reason). Then, I report both sets of inferences. This costs me remarkably little time precisely because there is so little difference between the two approaches from practical point of view!

\hypertarget{take-home-message}{%
\section{Take home message}\label{take-home-message}}

We are not studying something completely different! We are merely approaching it from an unusual angle that leads to deeper understanding and interesting insights in the long run.

\hypertarget{mixtures}{%
\chapter{Mixtures}\label{mixtures}}

\hypertarget{beta-binomial}{%
\section{Beta Binomial}\label{beta-binomial}}

Beta binomial is defined as a product of binomial and beta distributions.
\[BetaBinomial(k|N, p, \theta) = Binomial(k|N,p) \cdot Beta(p|\beta_1, \beta_2),\]
where \(k\) is number of successes (e.g., ``heads'' in a coin toss), \(N\) is total number of trials/draws, and \(p\) is the probability of success), and \(\beta_1\) and \(\beta_2\) determine the shape of the beta distribution. The book uses reparametrized version of the beta distribution, sometimes called \emph{beta proportion}:
\[BetaBinomial(k|N, p, \theta) = Binomial(k|N,p) \cdot Beta(p|p, \theta),\]
where \(\theta\) is precision parameter. \(p\) and \(\theta\) can be computed from \(\beta_1\) and \(\beta_2\) as
\[
p = \frac{\beta_1}{\beta_1 + \beta_2}\\
\theta = \beta_1 + \beta_2
\]
The latter form makes it more intuitive but if you look at the code of \texttt{dbetabinom()}, you will see that you can use \texttt{shape1} and \texttt{shape2} parameters instead of \texttt{probe} and \texttt{theta}.

Recall the (unnormalized) Bayes rule (\(p\) is probability, \(y\) is an outcome, \(...\) parameters of the prior distribution):
\[
Pr(p | y) = Pr(y | p) \cdot Pr(p | ...)
\]
Examine the formula again and you can see that you can think about beta binomial as a posterior distribution for a binomial likelihood with the beta distribution as prior for parameter \(p'\) of the binomial distribution:

\[BetaBinomial(N, p, \theta | k) = Binomial(k|N,p) \cdot Beta(p| p_{mode}, \theta)\]

Thus, beta binomial is a combination of \emph{all} binomial distributions weighted by a beta distribution that has its mode at \(p_{mode}\) and its width is determined by \(\theta\). In other words, when we use binomial distribution alone, we state that we can \emph{compute} the probability directly as \(p = \text{some linear model}\). Here, we state that our knowledge is incomplete and, \emph{at best}, we can predict mode of the beta distribution from which this probability comes from and we let data determine variance/precision (\(\theta\)) of that distribution. Thus, our posterior will reflect \emph{two} uncertainties based on two loss functions: one about the number of observed events (many counts are compatible with a given \(p\) but with different probabilities), as for the binomial, plus another one about \(p\) itself (many values of \(p\) are compatible with given \(p_{mode}\) and \(\theta\)). This allows model to compute a trade-off by considering values of \(p\) that are less likely from prior point of view (they are away from \(p_{mode}\)) but that result in higher probability of \(k\) given that chosen \(p\). We will see the same trick again later in the book, when we will use it to incorporate uncertainty about measured value (i.e., at best, we can say that the actual observed value comes from a distribution with that mean and standard deviation).

In practical terms, this means that parameter \(\theta\) controls the width of the distribution (see plots below). As \(\theta\) approaches positive infinity, our prior uncertainty about \(p\) is reduced to zero, which means that we now consider only one binomial distribution, where \(p' = p\), which is equivalent to the simple binomial distribution. Thus, beta binomial \emph{at most} is as narrow as the binomial distribution.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/beta-binomial-1.pdf}

\hypertarget{negative-binomial-a.k.a.-gamma-poisson}{%
\section{Negative binomial, a.k.a. Gamma Poisson}\label{negative-binomial-a.k.a.-gamma-poisson}}

The idea is the same: We do not have enough information to compute the rate of events, so instead, we compute the mean of the Gamma distribution rates come from and let data determine its variance (scale). Again, in practical terms this means that for the smallest scale our uncertainty about the rate is minimal and distribution matches the Poisson processes with a fixed rate. Any increase in uncertainty (larger values for scale parameter), mean broader distribution that is capable to account for more extreme values.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-32-1.pdf}

\hypertarget{ordered-categorical}{%
\section{Ordered categorical}\label{ordered-categorical}}

From log odds to logit link.
\[
log(\frac{Pr(y_i \leq k)}{1 - Pr(y_i \leq k)}) = \alpha_k \\
\frac{Pr(y_i \leq k)}{1 - Pr(y_i \leq k)} = e^{\alpha_k} \\
Pr(y_i \leq k) = e^{\alpha_k} \cdot ( 1 - Pr(y_i \leq k)) \\
Pr(y_i \leq k) \cdot ( 1 + e^{\alpha_k}) = e^{\alpha_k} \\
Pr(y_i \leq k) = \frac{e^{\alpha_k}}{1 + e^{\alpha_k}}
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_p }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{alpha =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{length.out=}\DecValTok{100}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p =} \FunctionTok{exp}\NormalTok{(alpha) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(alpha)))}

\FunctionTok{ggplot}\NormalTok{(df\_p, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{alpha, }\AttributeTok{y=}\NormalTok{p)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-33-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{equal\_probability }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(((}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{) }\SpecialCharTok{/} \DecValTok{7}\NormalTok{) }\SpecialCharTok{/}\NormalTok{  (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{) }\SpecialCharTok{/} \DecValTok{7}\NormalTok{))}
  
\NormalTok{df\_ord\_cat }\OtherTok{\textless{}{-}} 
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Response =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }
         \AttributeTok{p =}\NormalTok{ rethinking}\SpecialCharTok{::}\FunctionTok{dordlogit}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{, equal\_probability),}
         \AttributeTok{Label =} \StringTok{"Equal probability"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{Response =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }
         \AttributeTok{p =}\NormalTok{ rethinking}\SpecialCharTok{::}\FunctionTok{dordlogit}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{, equal\_probability }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{),}
         \AttributeTok{Label =} \StringTok{"Beta = 1"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{Response =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }
         \AttributeTok{p =}\NormalTok{ rethinking}\SpecialCharTok{::}\FunctionTok{dordlogit}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\DecValTok{0}\NormalTok{, equal\_probability }\SpecialCharTok{+} \DecValTok{2}\NormalTok{),}
         \AttributeTok{Label =} \StringTok{"Beta = {-}2"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Label =} \FunctionTok{factor}\NormalTok{(Label, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Beta = 1"}\NormalTok{, }\StringTok{"Equal probability"}\NormalTok{, }\StringTok{"Beta = {-}2"}\NormalTok{)))}

\NormalTok{df\_cuts }\OtherTok{\textless{}{-}}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Response =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }
         \AttributeTok{K =}\NormalTok{ equal\_probability,}
         \AttributeTok{Label =} \StringTok{"Equal probability"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{Response =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }
         \AttributeTok{K =}\NormalTok{ equal\_probability }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{,}
         \AttributeTok{Label =} \StringTok{"Beta = 1"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{tibble}\NormalTok{(}\AttributeTok{Response =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{, }
         \AttributeTok{K =}\NormalTok{ equal\_probability }\SpecialCharTok{+} \DecValTok{2}\NormalTok{,}
         \AttributeTok{Label =} \StringTok{"Beta = {-}2"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Label =} \FunctionTok{factor}\NormalTok{(Label, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Beta = 1"}\NormalTok{, }\StringTok{"Equal probability"}\NormalTok{, }\StringTok{"Beta = {-}2"}\NormalTok{)))}
  

\NormalTok{cuts\_plot }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{df\_cuts,) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ K, }\AttributeTok{color=}\NormalTok{Label), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{(Label}\SpecialCharTok{\textasciitilde{}}\NormalTok{.) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\StringTok{"Odds ratio"}\NormalTok{)}

\NormalTok{prob\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{df\_ord\_cat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Response, }\AttributeTok{y=}\NormalTok{p, }\AttributeTok{color=}\NormalTok{Label)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{()}

\NormalTok{cuts\_plot }\SpecialCharTok{+}\NormalTok{ prob\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-34-1.pdf}

\hypertarget{instrumental-variables}{%
\chapter{Instrumental Variables}\label{instrumental-variables}}

\hypertarget{disclaimer}{%
\section*{Disclaimer}\label{disclaimer}}
\addcontentsline{toc}{section}{Disclaimer}

My understanding is that instrumental variables are unicorns. They are immensely powerful and can magically turn an observational study into, effectively, a randomize experiment enabling you to infer causality. But they are rare, perhaps, even rarer than unicorns themselves, which is why most example you find are based on the same (or very similar) data.They are also often described as a source of ``natural randomization'', yet, the best examples I have found (effect of military experience on career/wages, effect of studying in a charter school on math skills) involved \emph{deliberate} randomization in a form of lottery that you can conveniently use.

\hypertarget{can-we-estimate-an-effect-of-military-experience-on-wages}{%
\section*{Can we estimate an effect of military experience on wages}\label{can-we-estimate-an-effect-of-military-experience-on-wages}}
\addcontentsline{toc}{section}{Can we estimate an effect of military experience on wages}

One of the examples, which you can find online, is an effect of military experience on wages. Conceptually, everything is simple, you just want to know a direct effect of military experience on wages.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-37-1} \end{center}

However, in practice both a decision to join military \emph{and} choices of career can be cause by the same set of unobserved factors. If you come from a family with military background, this will affect both your decision join military and the kind of career you want to pursue. If you are patriotic and want to serve the country, you might opt for both military and lower-earning career in public sector. Conversely, if you are interested in becoming a successful lawyer, going into military might be more of a hindrance than of help. In short, we have a backdoor path through unobserved variables and we have no easy way to close it.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-38-1} \end{center}

\hypertarget{draft-as-an-instrumental-variable}{%
\section*{Draft as an instrumental variable}\label{draft-as-an-instrumental-variable}}
\addcontentsline{toc}{section}{Draft as an instrumental variable}

If we would be able to perform a \emph{randomized} experiment, things would be easy: Take similar people and randomly send / not send them to military, observe the effect of this on their career and wages. Now, \emph{we} cannot do it but, perhaps, we can find a situation it was actually done and use it for our advantage. This makes the whole situation a sort of a hybrid: we \emph{observe} an outcome of \emph{randomization} that someone else did for their own purposes

In this case, during the Vietnam war people were drafted based on their assigned draft numbers. The latter were determined by your date of birth but the relationship between the birth day within a year and the draft number was \href{https://en.wikipedia.org/wiki/Draft_lottery_(1969)}{randomized through the lottery}. In addition, the order \emph{within} a day was randomized through lottery of initials. The purpose was to create a perfect randomization where any background (apart from you being male and being eligible for military service) had no effect on whether or not your were drafted. And, although the lottery was created to address inequalities, it created an almost perfect randomized experiment for us to use. Almost, because some people with early numbers avoided being drafted through various means, whereas some people with later draft numbers volunteered and were enlisted.

The DAG it creates is the following

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-39-1} \end{center}

As you can see, we are still not able to close the backdoor path between \emph{military experience} and \emph{wages} because the randomization is not perfect. You will see how we can still overcome this problem using two-stage least squares (the default approach you find almost everywhere, referred to as ``two-stage worst squares'' by McElreath) and using covariance of residuals (as in the book).

\hypertarget{two-stage-least-squares}{%
\section*{Two-stage least squares}\label{two-stage-least-squares}}
\addcontentsline{toc}{section}{Two-stage least squares}

The main idea is two split our DAG into two halves and estimated the effects one by one, hence, ``two-stage''. First, use draft number to \emph{predict} the military experience.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-40-1} \end{center}

The linear model is
\[ME \sim Normal(\widehat{ME}, \sigma_{ME})\\
\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN\]

where \(\alpha_M\) is an arbitrary intercept term, \(\beta_{DN}\) encode the main effect of draft number, and \(\sigma\) is the standard deviation of residuals. You can also write \emph{the same} model in a different format, which you are likely to encounter in other tutorials and lectures:
\[\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN + \epsilon_{ME}\]
Here, the difference between the \emph{predicted} military experience (note the hat above \(\widehat{ME}\)) and the actual one is described by \(\epsilon_{ME}\): residuals that come from a normal distribution that is centered at 0 with standard deviation of \(sigma_{ME}\), which is implicit in this format. Note that \emph{both} versions express same variables and encode the same dependence but differ in whether the residuals (\(epsilon\)) or the standard deviation of the distribution they come from (\(sigma\)) are implicit or explicit.

As draft number is independent from unobserved variables due to the lottery, the \emph{predicted} military experience is \emph{also} independent from them. All the dependence gets transferred to the \emph{residuals} instead, because residuals are variance that our \emph{signal} cannot explain (that point will become important later on).

Now, for the second stage, we use \emph{predicted} military experience, as it is independent of the unobserved variables and, therefore, we do not need to close the backdoor path!

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-41-1} \end{center}

The model is, again, simple but do note that in two-stage least squares we are using \emph{predicted} military experience \(\widehat{ME}\)!
\[CW \sim Normal(\widehat{CW}, \sigma_{CW})\\
\widehat{CW} = \alpha_{CW} + \beta_{ME} \cdot \widehat{CW}\\
or\\
\widehat{CW} = \alpha_{CW} + \beta_{ME} \cdot \widehat{ME} + \epsilon_{CW}
\]

Again, all the dependence between unobserved variables and career/wages gets offloaded into the residuals \(\epsilon_{CW}\) and you get you nice and clean \emph{direct} effect of \emph{predicted} military experience. Importantly, for you interpretation you \emph{equate} the effect of the predicted military experience with an effect of an actual one. This is, of course, one big elephant in the room because you need to be certain that \(\widehat{ME}\) is indeed accurate. Unfortunately, if it is not, your inferences are incorrect and you are back to square one.

\hypertarget{covarying-residuals}{%
\section*{Covarying residuals}\label{covarying-residuals}}
\addcontentsline{toc}{section}{Covarying residuals}

This is the approach presented in the book that allows you to use \emph{actual} military experience as a predictor and still solve the problem of the backdoor path. The initial idea is the same: Let us use our instrumental variable to compute a ``pure'' military experience, uncontaminated by unobserved variables.

\[\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN + \epsilon_{ME}\]

Again, all the variance due to unobserved variables is accumulated by residuals (\(\epsilon_{ME}\)), which are \emph{correlated with them}. The latter part is trivially true because residuals are \emph{always} correlated with ``noise'', precisely because we defined it as influence of unobserved variables we did not measure and, therefore, cannot explicitly account for. If we could measure and include them, we would observe \emph{same correlation} explicitly. Alas, we did not measure them and, therefore, in our typical model the fact that residuals are correlated with unobserved variables is of no immediate practical value (beyond checking for their exchangeability).

Now, imagine that we \emph{could} compute not just a prediction from our first stage but a \emph{pure randomized} military experience that is not correlated with unobserved variables. Obviously, we cannot, otherwise we would not worry about the backdoor path, but what do we know about a linear model that includes that magic \emph{pure randomized} military experience as a predictor? That its \emph{true} direct effect on career/wages will be \emph{independent} from unobserved variables and, therefore, all their influence will be offloaded into residuals (\(\epsilon_{\widehat{CW}}\), note the hat that differentiates them the residuals that you get if you use observed military experience), which will be correlated with those unobserved variables. Just like the residuals of our \emph{first stage model} (\(\epsilon_{ME}\))! But if both sets of residuals are correlated with unobserved variables (\(\epsilon_{ME} \propto UV\) and \(\epsilon_{\widehat{CW}} \propto UV\)), the two sets of residuals are \emph{also} correlated: \(\epsilon_{ME} \propto \epsilon_{\widehat{CW}}\).

These correlated residuals sound wonderfully entertaining but why should we care about them given that \(\epsilon_{\widehat{CW}}\) are \emph{hypothetical} residuals given the \emph{hypothetical} randomized military experience that we do not have access to? Because we can make them real by using their covariance with \(\epsilon_{ME}\) that we \emph{can} compute! Here is the idea: let us use two simple linear models to predict military experience from draft number and career/wages from \emph{actual} military experience but allow the residuals of two models to be correlated. Again, given that \(\epsilon_{ME} \propto UV\), if we make \(\epsilon_{ME} \propto \epsilon_{CW}\), therefore, \(\epsilon_{CW} \propto UV\). In other words, \(\epsilon_{CW} \approx \epsilon_{\widehat{CW}}\). But this means that since \(\epsilon_{CW}\) accounts for the variance due to unobserved variables, it must not be accounted for by \emph{other} terms of the linear model and our effect of military experience (\(\beta^{true}_{ME}\)) expresses just the direct path:
\[\widehat{CW} = \alpha_{CW} + \beta^{true}_{ME} \cdot ME + \epsilon_{\widehat{CW}}\]

This is a very elegant trick: We do not know what the correlation between residuals and unobserved variables is but we do know that it must be the same (very similar) and we account for it by enforcing the correlation between the residuals that we deduced must exist. On the one hand, there is an obvious elephant in the room in that the whole thing works well only if the assumption that residuals are correlated is true. On the other hand, because we are fitting the covariance matrix as part of the model, we can check whether our assumption about that correlation was supported by data. If it was not, our instrumental variable was probably not as good as hoped for.

\hypertarget{parameters-combining-information-from-an-individual-with-population}{%
\chapter{Parameters: combining information from an individual with population}\label{parameters-combining-information-from-an-individual-with-population}}

When we infer values of parameters, we must decide on how to combine information available for an individual ``random effect'' entry (e.g., participant) and information about the distribution of values for this parameter in the sample in general or in some group within that sample. The former --- data for an individual --- describes an individual itself but is, necessarily, noisy. The latter --- data for the entire sample --- has better signal-to-noise ratio, as it pools information across all individuals, but is informative about averages not individuals.

Below I list various strategies that were used throughout the book. The main purpose is to show that they differ primarily in the relative contribution of the two sources and how individuals are used to compute averages. I will talk primarily about intercepts, as this is the most often varied parameter, but the same logic is applicable to \emph{any} free parameter in the model.

\hypertarget{everyone-is-the-same-single-parameter}{%
\section{Everyone is the same (single parameter)}\label{everyone-is-the-same-single-parameter}}

The very first strategy that we used was to employ just a single intercept parameter in the model.

\[height_i \sim \alpha\\
\alpha \sim Normal(178, 9.5)\]

This is an extreme case of when we ignore the fact that people (monkeys, countries, etc.) are different and, effectively, model everyone as a single typical average meta-individual. The information about variance within the sample is discarded.

In the plot below, measurement for each individual (distinguished by their position on y-axis) is plotted on x-axis (black circles). But using a single intercept (vertical line) in the model, means all of them get an average height (blue open circles).

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-44-1} \end{center}

Another way to view the same data is by plotting raw data (y-axis) vs.~used estimates (x-axis). The vertical line denotes the population mean, whereas the diagonal line implies that estimates are equal to the data.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-45-1} \end{center}

\hypertarget{everyone-is-unique-independent-parameters}{%
\section{Everyone is unique (independent parameters)}\label{everyone-is-unique-independent-parameters}}

The other extreme is to assume that everyone is unique and should be judged (estimated) using only their own data. Here, the information about the population is discarded.

\[height_i \sim \alpha_i\\
\alpha_i \sim Normal(178, 9.5)\]

This is the approach taken by paired t-test and repeated measures ANOVA. Note that it is likely to overfit the data, as we allow limited and noisy data to fully determine intercepts. Use of weak or flat priors (as in frequentist approach) is likely to make out-of-sample performance even worse.

In the plot below, measurement for each individual (distinguished by their position on y-axis) is plotted on x-axis (black circles). You cannot see black circles because they are covered by open blue circles --- the estimates used by the model.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-46-1} \end{center}

Here is another representation of the same data with all dots lying on the diagonal (estimate is equal to data). The vertical blue line still denotes the population mean this information is not used for estimates.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-47-1} \end{center}

\hypertarget{people-are-different-but-belong-to-a-population-pooled-parameters}{%
\section{People are different but belong to a population (pooled parameters)}\label{people-are-different-but-belong-to-a-population-pooled-parameters}}

A more balanced approach is to combine data from an individual and population. This is a two level (multilevel) approach, where an individual parameter value comes from a population (group) level distribution.

\[height_i \sim \alpha_i\\
\alpha_i \sim Normal(\alpha^{pop}, \sigma^{\alpha})\\
\alpha^{pop} \sim Normal(178, 9.5)\\
\sigma^{\alpha} \sim Exponential(1)\]

The basic idea is the ``regression to the mean'', so that unusual-looking individuals are probably more average than they appear. In other words, they are so unusual because of the noise during that particular measurement. During a next measurement the noise will be different, probably not so extreme, and the individual will appear to be more normal. The pull of extreme observations toward the mean is the same as one from a static prior. The main difference is that we use \emph{adaptive} priors and determine how typical/unusual an observation is based on all observations themselves, \emph{including} the extreme ones. As with ``normal'' priors, the influence of adaptive prior is most pronounced for extreme, unreliable observations, or observations with small amount of data. They are easier to overfit and, hence, benefit most from regularizing priors.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-48-1} \end{center}

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-49-1} \end{center}

\hypertarget{people-are-different-but-belong-to-a-group-within-a-population-multilevel-clusters-of-pooled-parameters}{%
\section{People are different but belong to a group within a population (multilevel clusters of pooled parameters)}\label{people-are-different-but-belong-to-a-group-within-a-population-multilevel-clusters-of-pooled-parameters}}

A logical extension of a two-level approach is to extended it with more levels: An individual belongs to a group (cluster) which, in turn, belongs to a population.

\[height_i \sim \alpha_i\\
\alpha_i \sim Normal(\alpha^{group}_i, \sigma^{group})\\
alpha^{group} \sim Normal(\alpha^{pop}, \sigma^{pop}) \\
\alpha^{pop} \sim Normal(178, 9.5)\\
\sigma^{\alpha}, \sigma^{group} \sim Exponential(1)\]

For an individual, it allows to pool information across a more relevant group. For example, males tend to be taller than females, so it would make more sense to use female sub-population/group/cluster to decide on just how typical a given female is. Same goes for other types of clusters: e.g., students from a particular school are probably more similar in their knowledge on a particular subject (such as math) as compared to students from other schools. At the same time, we can still pull information across clusters, making our inferences about them more reliable as well. And, of course, you are not limited in the number of levels you can create: students within a school, schools within a district, districts within a city, etc.

In the plots below, notice that sex cluster distributions are tighter than a single population distribution above. Also notice how individual observers are pulled toward the group mean and, again, the pull is proportional to how atypical a value is.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-50-1} \end{center}

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-51-1} \end{center}

\hypertarget{people-are-similar-to-some-but-different-to-others-gaussian-process}{%
\section{People are similar to some but different to others (Gaussian process)}\label{people-are-similar-to-some-but-different-to-others-gaussian-process}}

Multilevel approach with clusters is very useful but it works only if you have well-defined discrete cluster: sex, school a student belongs to, occupation, etc. However, sometimes there are no such clear boundaries. Rather, you can presume that similarity in a property that you are interested in (height, in our case) could be proportional to another well defined measure of similarity / distance between the individuals. For example, one could, perhaps naively, assume that individuals that are more similar in their genes have more similar height. In that case, we can compute distance between their genetic sequences and use that distance to define a population \emph{relative} to an individual.

\[height_i \sim \alpha^{pop} + \alpha_i\\
\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \dots \\ \alpha_N \end{pmatrix} \sim MVNormal(
 \begin{pmatrix}0, 0, \dots, 0 \end{pmatrix}, K)\\
K_{ij} = = \eta^2 exp(−\rho^2D^2_{ij}) + \delta_{ij}\sigma^2 \\
\alpha^{pop} \sim Normal(178, 9.5)\]

where \(D_{ij}\) is the distance between individuals (see the book for further details on the formula and Gaussian process in general).

Generally speaking, the idea is that contribution of observations is proportional to their distance to the individual in question. It also means that there are no discrete cluster. Rather, each observation gets its own distance-based population distribution which it is judged against. Note, however, that parameter \(\rho\) adjusts the effect of the distance making it more or less relevant. Thus, the distance can be ignored by the model (via smaller value of \(\rho\)) meaning that all observations/individuals contribute equally to the population distribution (relative to an individual), i.e., a given intercept is correlated with everybody else. Conversely, larger values of \(\rho\) mean that only nearest neighbors are taken into account.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-52-1} \end{center}

In the example below, the distance between individuals is determined by the distance on the vertical axis. The first plot use \(\rho=1\) that samples most observations (dot size reflect the \(K\) used in the MVNormal). It produces a fairly broad population distribution and shift the first participant to the \emph{right}.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-54-1} \end{center}

The second plot uses the same distance measure but \(\rho=10\) meaning that it is the close neighbors that affect the estimate. Here, the population distribution is much tighter and the estimate for the first participant is shifted to the \emph{left}.

\begin{center}\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-55-1} \end{center}

\hypertarget{people-are-different-but-belong-to-a-population-in-which-parameters-are-correlated-correlated-pooled-parameters}{%
\section{People are different but belong to a population in which parameters are correlated (correlated pooled parameters)}\label{people-are-different-but-belong-to-a-population-in-which-parameters-are-correlated-correlated-pooled-parameters}}

The idea is that if two (or more) parameters are correlated within the population --- e.g., taller people (intercept) grow more if they drink milk (slope, effect of the milk) --- then you can pool information across both parameters and use each to evaluate the other one.

\[height_i \sim \alpha^{pop} + \alpha_i + \beta_i \cdot Milk\\
\begin{pmatrix} \alpha_i \\ \beta_i \\ \dots \\ \alpha_N \end{pmatrix} \sim MVNormal(
 \begin{pmatrix}0, 0 \end{pmatrix}, K) \\
\alpha^{pop} \sim Normal(178, 9.5)\]

Here, the pulling forces are a bit more complicated than during a single parameter situation. Namely, parameters are not necessarily pulled towards the center of the distribution (population averages) but orthogonal to isolines, so that they are adjusted relative to each other. In the plot below, a black filled dot is average with respect to the slop but has a higher than average intercept. This means that, one the one hand, its intercept is too high for its average slope. One the other hand, its slope is too average for such a high slope. Both parameters pull on each other at the same time, which us why the intercept becomes more normal but the slope becomes higher (a more bit more extreme). The advantage is that you can use both parameters to judge them against the population.

\begin{center}\includegraphics[width=22.11in]{images/pooled-correlated-parameters} \end{center}

\hypertarget{incorporating-measurement-error-a-rubber-band-metaphor}{%
\chapter{Incorporating measurement error: a rubber band metaphor}\label{incorporating-measurement-error-a-rubber-band-metaphor}}

One way to think about \protect\hyperlink{loss-functions}{loss functions} is as if they are rubber bands\footnote{As Hook's law is a first-order linear approximation, this metaphor works fully only if we assume L1 distance, i.e., that error increases / probability decreases linearly with distance. Still you could imagine a better rubber band whose force will be proportional to the squared (L2) distance, as for the normal likelihood.} (black vertical segments) connecting observations (white dots) with its corresponding mean (red dots) on the regression line. Each band tries to pull the mean and, therefore, the regression line towards the observation and the final estimate is simply a minimal energy state: The total sum of pulling forces is minimal, so that any movement of the regression line would decrease the stretch for sum dots but will produce way more pulling for other dots, increasing the overall force. This is the ``usual'' way that we performed the regression analysis assuming that \emph{same} rubber bands connect every dot with the regression line.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-58-1.pdf}
Within this metaphor, measurement error is incorporated as additional inferred value (blue dots, true value of a measurement) that is connected to the observed value via a \emph{custom} rubber band (colored) whose strength depends on the measurement error. I.e., large measurement errors make for a very weak stretchable bands, whereas high certainty leads to very small and strong bands. Note the connectivity pattern: observed value --- (via custom rubber band) --- (estimated) true value --- (via common rubber band) --- mean on the regression line. Here, dots with smaller measurement error will pull the regression line much stronger towards the observed value, whereas dots with large error will tolerate having regression line even very far away.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-59-1.pdf}

Note that in some cases, information about measurement error is incorporated directly. For example, in binomial likelihood the number of successes and the total number of observations determine not only the proportion of success but also confidence about that value.

\hypertarget{generalized-additive-models-as-continuous-random-effects}{%
\chapter{Generalized Additive Models as continuous random effects}\label{generalized-additive-models-as-continuous-random-effects}}

The purpose of this note is to give an intuition about what generalized additive models (GAMs) are and, more importantly, when can they be handy for \emph{causal} inferences. Thus, I will leave most mathematical details out\footnote{You can find them all in a \href{https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331}{book} by Simon N. Wood.}.

\hypertarget{generalized-additive-models-an-uxfcber-brief-introduction}{%
\section{Generalized Additive Models: An Über-brief Introduction}\label{generalized-additive-models-an-uxfcber-brief-introduction}}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-60-1.pdf}

Imagine that you have a (completely made up!) data set shown aboe that describes probability of consuming an ice-cream as a function of age. As you can see, it is twisty curve, so a simpler linear regression won't do. If we do not have an idea about the process(es) that generated this data, we might at least smooth it to see the underlying relationship more clearly. You can do this in a number of ways using a running median, loess, etc., but we will focus on GAMs as they have an advantage of being (generalized) linear models, which makes it easy to incorporate them into bigger generalized linear models.

The idea of the GAM is to approximate a dependence as a sum of basis functions. In the example below, all basis functions are \emph{identical} but cover \emph{different} parts of the space. However, you can have it the other way around with \emph{different} basis functions covering \emph{identical} (the entire) space.

In the example below, basis functions are cubic regression splines that are centered on ``knots''. Each tent has its own coefficient that determines its height and, also, the transition between knots. A \emph{wigliness} parameter determines how different coefficients can be, the more different they are, the wigglier will be the line, hence the name. Accordingly, wigliness of \emph{zero} produces a straight horizontal line (all coefficients must be the same). Wigliness of infinity means that coefficients can take any value and fit the data as closely as possible. As you can see, smoothing makes it much easier to see the underlying relationship.
\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-61-1.pdf}

\hypertarget{what-are-gams-good-for}{%
\section{What are GAMs good for?}\label{what-are-gams-good-for}}

As mentioned above, GAMs are just smoothers. They approximate a smooth relationship in the data without any knowledge or assumptions about a generative process behind it. As such they have at least two clear usage cases: an exploratory data analysis and generating predictions. In the former case, the shape of the smoothed line might hint at the underlying mechanisms and facilitate development of a causal model. In the latter case, as long as we stick to interpolation, we can use smoothed line to predict unobserved values for future use.

But what about causal inference? Can GAMs be useful for in this case? Yes, they can play a useful and obvious role. But to make this ``obvious'' obvious, we must take a detour and talk about random factors.

\hypertarget{covariates-random-factors}{%
\section{Covariates / random factors}\label{covariates-random-factors}}

Imagine that we have data on ice-cream consumption and, among other things, we measured respondents' eye color. What you find is that people with different eye color tend to consume different amount of ice-cream (on average). Why? It is a made-up example, so I do not know. But it could be genetic, same group of genes being responsible for eye color and ice-cream craving. Or it could be something else. Point is, we do not have a good (or any) causal model of a generative process that would link these two variables. Because of that we are not really interested in this link but we also cannot ignore it. Any systematic relationship that we ignore will we leave unexplained variance that, it turn, will make it harder for us to see a causal effect for predictors that we are interested in. What do we do? We include eye color either as a covariate (fixed effect) or a random effect. For frequentist models, the difference is whether intercept for each group are estimated independently (the former, flat priors, no shrinkage) or by pooling information across groups (the latter, produces shrinkage). For Bayesian models, the difference is between fixed (covariates) and adaptative (random effect) priors. Still, the idea is to stress that we have no idea why this particular variable is correlated with the outcome and that we have no causal generative model.

\hypertarget{gams-as-continuous-random-factors}{%
\section{GAMs as continuous random factors}\label{gams-as-continuous-random-factors}}

Let us get back to ice-cream consumption as a function of age. It is easy to that we are in the same situation as with eye color: We have no idea about the generative process but the dependence is strong and should not be ignored. Thus, we must include age in the bigger model, even if we do not care \emph{how} the age causes ice-cream consumption. But how can we do it? The relationship is not linear or even monotonic, so we cannot use age as a linear covariate. And, unlike categorical data for eye color, the data is continuous and we do not have well-defined few levels to estimate values for (unless we introduce some artificial discrete levels). Plus, the latter approach would actually \emph{ignore} the dependence, because will ignore the fact that nearby levels should be more similar than more distance ones. So, what do we do?

It is GAMs to the rescue! By pooling information from nearby data samples, GAMs produce a smooth estimate for each age that can be used in the model. The smooth estimate is, effectively, a continuous random effect! Thus, you should use GAMs whenever you have a \emph{continuous} variable that is correlated with an outcome, you have no idea about the underlying causal link but the relationship is not simple and monotonic enough to use conventional methods.

\hypertarget{flat-priors-the-strings-attached}{%
\chapter{Flat priors: the strings attached}\label{flat-priors-the-strings-attached}}

I have a feeling that one of the biggest issues that a lot of people have when they consider Bayesian statistics is priors. This is something that all the students worry and what they feel insecure about. As a result, they would rather stay in the frequentist world of flat priors.

And flat priors have a lot going for them. They are convenient for mathematicians as they make analytical derivations simpler. They are convenient for users of statistics because they do not have to worry about or even think about priors. And, if you do think about priors, flat priors look superior because they feel impartial. They do not impose any a priori knowledge and allow the results to be determined by the data alone. Thus, whatever results you get, you can claim that they are objective and untainted by the person who did the analysis.

However, flat priors have some strings attached. This may not be a deal-breaker (although it probably should) but this is definitely something you should be aware of when you use them.

\hypertarget{flat-priors-are-silly}{%
\section{Flat priors are silly}\label{flat-priors-are-silly}}

Consider experimental data from a domain you are knowledgeable in. What is the biggest effect that you feel is borderline realistic? I.e., an effect already so ridiculously large that anything larger must come from malfunctioning equipment or software, error in the analysis, etc. For example, evoked potentials in EEG are measured in microvolts, so we can safely assume that any difference between them, which is out of microvolts range, must be artificial. Let's say our threshold for the ``real effect'' is a way-way overly optimistic 1 millivolt, which is 1000 μV. To a specialist that already sounds ridiculous but when we use flat priors we \emph{explicitly} state that we believe equally strongly in difference between evoked potential that is on the scale of microvolts, millivolts, volts, or even billions of volts. Is an a priory belief that human brain is equally capable of generating evoked current of microvolts and billions of volts silly? It sure sound silly to me. Forcing it down on a model does not make this belief less silly.

\hypertarget{flat-priors-make-you-pretend-that-you-are-nauxefve}{%
\section{Flat priors make you pretend that you are naïve}\label{flat-priors-make-you-pretend-that-you-are-nauxefve}}

The other way to look at this is that by using flat priors you explicitly claim to be naïve with respect to the domain. You act as if you have no prior knowledge about the phenomenon, discarding any experience that you acquired. Do you really feel that all these years of studying the subject are of no relevance? Do you really think that you do not have good predictions about at least the realistic range of the effect? You probably do. Even if you are very uncertain about them, the range from minus to plus infinity is awfully large and you can certainly do better than that. And yet, use of flat priors implies that your are completely clueless and there is not a nugget of wisdom that you have that could aid your analysis.

\hypertarget{flat-priors-tend-to-overfit}{%
\section{Flat priors tend to overfit}\label{flat-priors-tend-to-overfit}}

Even under best circumstances (more on this below), flat priors do not restrict the model in fitting the sample as close as possible. This means that such models will almost certainly overfit the data. This might or might not be a big issue in each particular case, as it will depend on whether noise exaggerates or belittles the actual effect. I suspect that flat priors combined with a fortunate noise are partially responsible for the plethora of reported strong effects that we cannot replicate. This is definitely something to keep in mind.

\hypertarget{flat-priors-are-an-exception}{%
\section{Flat priors are an exception}\label{flat-priors-are-an-exception}}

Although for most people flat priors probably feel like a norm, they are applicable only in very specific cases of (relatively) few predictors and plenty of data. Remember all the advice about having that many observations per variable? That is because you need them to afford flat priors and the lack of regularization in general. However, that magic sweet spot is fairly small and flat priors become extremely dangerous as soon as you step out of it.

Do you have an observational study with very little data? See for example ``The Ecological Detective'' by Hilborn and Mangel who give plenty of situations where this is unavoidable.
Flat priors will lead to extreme overfitting to the point of models being not just useless but dangerously misleading. The book mentioned above shows how usage of proper priors can rescue the analysis.

Do you have a lot of data but also a lot of predictors? You probably will end up overfitting. The field of machine learning invests a lot of time and energy into regularization. Given the sheer number of predictors, they cannot set priors by hand and, instead, use other forms of method-specific batch regularization such as lasso or ridge regression penalties on coefficient weight, pruning trees, dropping out neurons, etc.

In short, you can afford flat priors and no regularization only if you keep yourself to fairly specific kinds of data sets. But this is not a norm, this is an exception.

\hypertarget{the-irony-of-power-analysis}{%
\section{The irony of power analysis}\label{the-irony-of-power-analysis}}

Even if you are fond of using flat priors and you are not worried about any issues raised above, you still need to think about proper priors once in a while. Specifically, whenever you need to perform a power analysis. Here, you cannot postulate silly things while remaining ``objective and impartial'' but do need to use your domain knowledge to formulate a sign and a magnitude of an effect in order to estimate the sample size you need. Which is why the power analysis is either extremely easy, if you know priors, or extremely hard, if you do not know them. Thus, even ``flat priors'' people cannot avoid using the proper ones and I suspect that, as in most cases, regular thinking about priors in your domain makes it much easier to define them for the power analysis.

\hypertarget{conclusions-1}{%
\section{Conclusions}\label{conclusions-1}}

My hope is that notes above were able to show that flat priors are neither universal, nor the best prior, nor the norm. They are something you can afford under very specific circumstances. Of course you can use them but you should at least make a mental note to yourself of why do you think they are applicable in that particular case, what advantages they have over alternatives, and what are the costs for using in them in the analysis.

\hypertarget{unbiased-mean-versus-biased-variance-in-plain-english}{%
\chapter{Unbiased mean versus biased variance in plain English}\label{unbiased-mean-versus-biased-variance-in-plain-english}}

One of the things I have learned during my statistics course is that mean is an \emph{unbiased} estimator whereas variance is a \emph{biased} estimator and, therefore, requires a correction\footnote{Note that \texttt{var()} function in R does not compute variance of the sample but is an estimator, so it applies the correction automatically. If you want variance of the sample itself, you need to undo the correct or write your own function.}. Here I attempt to provide an intuition for why that is the case using as few formulas as possible.

We start by noting that a \emph{sample} mean (mean of the data that you have) is (almost) always different from the \emph{population} ``true'' mean you are interested in. This is a trivial consequence of sampling variance. It would be pretty unlikely that you would hit \emph{exactly} the ``true'' population mean with your limited sample. This means that your sample mean is wrong but it is a wrong in a balanced way. It is equally likely to be larger and smaller than the ``true'' mean\footnote{Assuming that sampling distribution for the mean is approximately normal.}. Therefore, if you would draw infinite number of samples of the same size and compute their \emph{sample means} these random deviations to the left and to the right from the true mean would cancel each other out and \emph{on average} your mean estimate will correspond to the true mean. In short, all \emph{sample} means are wrong individually but correct on average. Thus, they are not wrong in a systematic way and, in other words, mean is an unbiased estimator.

What about variance? Variance is just an average squared distance to the \emph{true population} mean \(\mu\): \(\frac{1}{N}\sum\limits^{N}_{i=1}{(x_i-\mu)^2}\). Unfortunately, you do not know that true population mean and, therefore, you compute variance (a.k.a. average squared/ L2 distance) relative to the \emph{sample} mean \(\bar{x}\): \(\frac{1}{N}\sum\limits^{N}_{i=1}{(x_i-\bar{x})^2}\) and \emph{that} makes all the difference. Recall that if you use squared distance as your \protect\hyperlink{loss-functions}{loss function}, sample mean is the point that has minimal average distance to all points in the sample\footnote{This is effectively a definition of the mean, take a look at the notes on loss functions to see why this is the case}. To put it differently, sample mean is the point that \emph{minimizes} computed variance. If you pick \emph{any} other point but the \emph{sample} mean, the average distance / variance will necessarily be larger. However, we already established that \emph{true population} mean is different from the \emph{sample} mean and if we would compute \emph{sample} variance relative to the true mean, it would be larger (again, because it is always larger for any point that is not a sample mean). How much larger will depend on how wrong the sample mean is (something we cannot know) but it will \emph{always} be larger. Thus, variance computed relative to the sample mean is systematically smaller than than the ``correct'' variance, i.e.~it is a \emph{biased} estimator. Hence the \(\frac{1}{n-1}\) instead of \(\frac{1}{n}\) that attempts to correct this bias ``on average''. As with the mean, even a corrected variance for the sample is wrong (not equal to the true variance of the hidden distribution that we are trying to measure) but, at least, it is not systematically wrong.

\hypertarget{probability-mass-versus-probability-density}{%
\chapter{Probability mass versus probability density}\label{probability-mass-versus-probability-density}}

When you first encounter continuous distribution, one of the initially confusing things is the concept of probability density that \emph{looks like} probability of an outcome (but is not) and the fact that it can be any positive value, not just within 0 and 1. To make understanding easier, let us start with a simple concept of \emph{probability mass}. Here, each outcome gets a probability that must be between 0 and 1 and probabilities for all outcomes must add up to 1 (makes these values probability rather than just plausibility). Imagine the simplest case when all events are equally likely. For example, I have four cubes and you must guess the height of the tower that I have built. There are five possible towers (zero-height tower is also a tower, just not a particularly good one) and without any prior knowledge you can assume that each tower height is equally likely: 1/N, where N=5, so 1/5 = 0.2 (or 20\%, if you like percentages more).

There is another way of thinking about this via \emph{cumulative mass function}. It tells a cumulative (total) probability of observing a height of a tower that is equal or smaller than a chosen value.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-63-1.pdf}
As you can see, a cumulative probability of observing a tower of zero height (or lower) is 0.2. We if consider height of 1, it bumps cumulative probability to 0.4 (Pr(height=0) + Pr(height=1) = 0.2 + 0.2 = 0.4). Going up to 2 makes it 0.6, to 3 --- 0.8 and, finally, the cumulative probability of me building a tower of height of 4 \emph{or lower} is 1 (100\%!). The latter includes \emph{all} possible tower heights, so the probability you observe one of them is 100\%. Note that for each height the \emph{probability mass} tells you by how much the cumulative probability will increase. This probability mass as an change of cumulative probability will become important later.

Note that cumulative probability can only grow and \emph{must} be between 0 and 1, as all probabilities cannot be negative and must sum up to 1 (otherwise, we call them plausibilities). Below is the same plot but now it includes impossible heights of -1 and 5. Their probabilities are 0, so the cumulative probability does not grow.
\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-64-1.pdf}

But what if I build a ``tower'' out of \emph{very fine sand}? For simplicity, let us assume that its height is also within a 0 to 4 (cubes height) range. But now, there are way more heights that my tower can have. If my sand is super fine, as is individual grains are infinitesimally small, there are \emph{infinite} numbers of possible heights. Knowing that they are all equally likely sort of helps but you cannot compute a probability for each individual height: 1/N, where N=∞ mean 1/∞ ≈ 0. This is the annoying thing about infinities, they make computing things really hard\footnote{People tend to be scared of calculus which is all about limits when things become either infinitesimally small or large.}. So, what do you do if you cannot compute a probability for an individual event/value (height)? You can still compute the \emph{cumulative probability} of a tower being smaller or equal to a particular height! This is the cumulative function that you saw earlier, but the steps are now so fine that you cannot talk about sums but only about integrals. This is why it is now called the cumulative \emph{density} function (CDF).
\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-65-1.pdf}

Note that it looks fairly similar to the previous plot with cumulative probability for discrete events. It still grows from 0 to 1 but the steps are much finer. Yet we can still tell the probability of observing a tower of a height that is equal to or less than this. We start of at zero, because you cannot observe towers of negative height, so \emph{all} tower heights from negative infinity up to zero have a total cumulative probability of 0\footnote{Keep in mind that these values are based on our example of me building a tower. In other circumstances, negative values are possible.}. If we pick the height of 4, then the \emph{cumulative} probability of observing a tower that is equal-or-less is 1 because \emph{all} must be that or smaller (that is how we defined it, there cannot be a tower higher than 4!). If we pick the middle of the interval (height = 2), the \emph{cumulative} probability of observing a tower that is equal-or-smaller is 0.5. This is because we are using a uniform distribution, so half splitting height in half also gives us 50\% chance. As you can see, cumulative probability density behaves the same way as the cumulative probability mass in the example above. Both are fairly straightforward, as long as you appreciate that they are about \emph{all} outcomes up to the point of interest, not just a single event.

What about the probability \emph{density} function (PDF)? Recall that the probability mass function tells you how much the cumulative probability \emph{changes} when you ``move to the right'' to include the next outcome. Same thing here but for continuous cases a function that describes the rate of change is called a derivative. The formula for our continuous uniform distribution is
\[CDF = \frac{1}{4} \cdot height\]
(note that we are currently only thinking about a range of 0 to 4 to make things simpler). You can check that this is indeed the case by plugging in different heights. Its derivative with respect to height is
\[\frac{\delta CDF}{\delta height} = \frac{1}{4}\]

Thus, we can now plot both CDF and PDF.
\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-66-1.pdf}
You can reverse the logic and say that the cumulative density function is an \emph{integral} of PDF, i.e., area under the curve up to each point. So, the total area under PDF (blue line) should end up being 1 (final value of CDF). This is easy to check for a rectangular like that, just multiply its width (height range of 4) by its height (probability density value of 0.25) and get that 1. Note that if you restrict your height=2 then CDF values is 0.5 and the area under blue line (PDF) is also 0.5 (compute!).

In the example above, probability density is constant at 0.25 and that makes it look ``normal'', at least not surprising. But what if we restrict my tower-building, so I cannot build anything taller than 0.5 cubes (or meters, units are of little relevance here). Now, the CDF formula is (check!):
\[CDF = 2 \cdot height\]

and for PDF:
\[\frac{\delta CDF}{\delta height} = 2\]

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-67-1.pdf}

Our constant PDF value is 2! Way above one, how come? Think about the slope and think about the area of rectangle: if its 0.5 units wide it must be 2 units tall to make are equal to 1. The point of this is probability density \emph{is not} a probability and that you should not care about \emph{absolute} values of probability density, only about \emph{relative} values. If you feel that ``adding up'' values larger than 1 should not give you 1 at the end, yes, it is very counter-intuitive. Integrals, as other things that have to do with infinitely small or large numbers, are counter-intuitive. Thus, ignore the units, ignore the absolute values, just keep in mind that whenever it is higher, the more probable that specific value is. (But, again, there is no way to compute a meaningful probability for a \emph{single} value).

\hypertarget{effective-degrees-of-freedom-number-of-parameters}{%
\chapter{Effective degrees of freedom / number of parameters}\label{effective-degrees-of-freedom-number-of-parameters}}

One thing that your learn once you start using linear mixed models (multulevel) models is that degrees of freedom used up by the model is less than number of parameters, so the total number is called number of \emph{effective} parameters / degrees of freedom. This is because each parameter (whether it is formally ``fixed'' or ``random'') can use up 1 degree of freedom. It will use full one degree of freedom, if it can take \emph{any} value. In other words, a parameter with flat prior takes up 1 degree of freedom. In yet other words, a parameter takes one full degree of freedom if its prior distribution has \emph{infinite} variance (a different way to say that it can take any value and its prior is flat). Conversely, if prior distribution for the parameter has \emph{zero} variance, then it is fixed (a constant!) and take zero degrees of freedom. Finally, if prior distribution for the parameter has non-zero \emph{finite} variance (whatever it is), it uses up a fraction of the degree of freedom. How much it uses up depends on the variance.

This is why fixed parameters in frequentist statistics use full degree of freedom (flat priors!) but same ``fixed'' parameters will use less than one in Bayesian models with non-flat priors (even weakly regularizing priors mean that you are not enjoying the full freedom). Similarly, a random effect in repeated measures ANOVA uses one degree of freedom because it has flat priors (can take any value, no shrinkage) but only a fraction in linear mixed models, because in LMM random effects come from a Gaussian distribution with finite variance (shrinkage!).

\hypertarget{multiple-regression---masked-relationship}{%
\chapter{Multiple regression - Masked relationship}\label{multiple-regression---masked-relationship}}

These notes are on section 5.2 ``Masked relationship'' from chapter 5 ``The Many Variables \& The Spurious Waffles''. This section introduces is a mirror twin of section 5.1 on \protect\hyperlink{spurious-association}{spurious associations}. The latter explores how each of two predictors can have a strong relationship with an outcome variable \emph{if fitted individually} but how only one of them retains that relationship when both predictors are used in multiple regression. Section 5.2 is concerned with the exact opposite. When we look at the association between each predictor variable (\emph{neocortex percent} and \emph{log body mass}) and the richness of milk \emph{individually}, we find nothing. However, when you use \emph{both} simultaneously via multiple regression, we find that both are strongly associated with the outcome variable.

The key to understanding this are figure 5.9 (particularly, the bottom counterfactual plots) and an explanation on page 151 that coefficients of regression model shows that ``species that have high neocortex percent \emph{for their body mass} have higher milk energy'' and ``species with high body mass \emph{for their neocortex percent} have lower milk energy''. In other words, if you take several species with very similar body mass, you expect those with higher percent of neocortex to have richer milk. Conversely, if you handpick several species with very similar neocortex percentage, you expect that the larger ones will have a less energetic milk. But because these two predictor variables are correlated, this relationship is easy to see only when you fix one of them. Figure 5.9 tries to show that but we can do better by (artificially) grouping the data looking at relationship between each predictor variable and the milk energy individually.

Let us start by replicating top subplots in figure 5.9. Note that 1) I used frequentist linear model fit with flat priors via \texttt{geom\_smooth()} and, therefore, 2) the stripes correspond to standard error rather than 89\% credible interval. However, the two are close enough in this case for illustration purposes.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(milk)}
\NormalTok{milk\_df }\OtherTok{\textless{}{-}} 
\NormalTok{  milk }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# standardize}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{K =} \FunctionTok{standardize}\NormalTok{(kcal.per.g),}
         \AttributeTok{N =} \FunctionTok{standardize}\NormalTok{(neocortex.perc),}
         \AttributeTok{M =} \FunctionTok{standardize}\NormalTok{(}\FunctionTok{log}\NormalTok{(mass))) }\SpecialCharTok{\%\textgreater{}\%}
  
  \CommentTok{\# keep only complete cases}
  \FunctionTok{na.omit}\NormalTok{()}

\NormalTok{NK\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ N, }\AttributeTok{y=}\NormalTok{K)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"neocortex percent (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_equal}\NormalTok{()}

\NormalTok{MK\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ M, }\AttributeTok{y=}\NormalTok{K)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"log body mass (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_equal}\NormalTok{()}

\NormalTok{NK\_plot }\SpecialCharTok{|}\NormalTok{ MK\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-69-1.pdf}

So, once again, we see that there is no clear relationship is evident if we consider neocortex percent while ignoring body mass of an animal and, vice versa, when we look only at the log body mass ignoring the cortex. Let us reduce our state of ignorance by splitting individual species into three groups based on their similarity in neocortex percentage. Note that I have put no extra care into the grouping process apart from ensuring that all groups have roughly the same number of species. Also note that connecting lines have no specific meaning and I've added them only to make visual grouping of dots of the same color easier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{milk\_df }\OtherTok{\textless{}{-}} 
\NormalTok{  milk\_df }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# split data into three groups}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Mass group}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{cut\_number}\NormalTok{(M, }\DecValTok{3}\NormalTok{), }
         \StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{cut\_number}\NormalTok{(N, }\DecValTok{3}\NormalTok{))}

\NormalTok{NK\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ N, }\AttributeTok{y=}\NormalTok{K, }\AttributeTok{color=}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"neocortex percent (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{col =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{coord\_equal}\NormalTok{()}

\NormalTok{MK\_plot }\OtherTok{\textless{}{-}} 
  \FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ M, }\AttributeTok{y=}\NormalTok{K, }\AttributeTok{color=}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"log body mass (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{col =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{coord\_equal}\NormalTok{()}

\NormalTok{NK\_plot }\SpecialCharTok{|}\NormalTok{ MK\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-70-1.pdf}

Once we group species either by their their neocortex percent, you can immediately see the pattern. The pattern on the left is trivial as it merely shows our grouping based on low (red), medium (green), and high (blue) percentage of neocortex. However, when we look at the right plot, we see how species with similar neocortex percentage cluster nicely together and this way you can see a nice negative correlation between log body mass and milk energy \emph{within each group}. This is because the same \emph{absolute} log body mass corresponds to different \emph{relative} log body mass with each group. I.e., a vertical slice through body mass leads to different values of milk energy \emph{because} these values come from different neocortex percent groups. To summarize, we can see that log body mass is informative \emph{only} if we know neocortex percent of an animal.

Let us try to visualize what multiple regression is doing. Keep in mind that this will only be a rough approximation because we will use just three means (one for each neocortex percent group), whereas our model computes an appropriate mean for every single specie. We will emulate the model check of whether ``species that have \emph{particular} neocortex percent \emph{for their body mass} have lower milk energy'' by centering each group, i.e., subtracting each groups' mean from body mass of each specie from that group. In other words, we align each group so that average bogy mass species \emph{for each group} are in the center, the larger species \emph{within each group} are on the right and the smaller species \emph{within each group} are on the left. This way we remove a positive correlation between neocortex percent and body mass and, therefore, can see the effect of the body mass on the milk energy \emph{alone}. And, even in this approximate way the dependence in very clear.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{milk\_df }\OtherTok{\textless{}{-}} 
\NormalTok{  milk\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{cM =}\NormalTok{ M }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(M))}

\FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ cM, }\AttributeTok{y=}\NormalTok{K)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{formula=}\NormalTok{y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x, }\AttributeTok{color=}\StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"log body mass {-} group mean log body mass (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"right"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{col =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_equal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-71-1.pdf}

The plot above expresses the same idea as a counterfactual right bottom plot in figure 5.9. Where, the question was ``what is the effect of log body mass for \emph{all} species with an average neocortex percent'', so single very specific group of animals. Our plot merely extended that idea to three groups. To appreciate this the link, let us fit the model and plot same counterfactuals for each group that we identified.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m5}\FloatTok{.7} \OtherTok{\textless{}{-}} \FunctionTok{quap}\NormalTok{(}
  \FunctionTok{alist}\NormalTok{(}
\NormalTok{    K }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{( mu , sigma ) ,}
\NormalTok{    mu }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ bN}\SpecialCharTok{*}\NormalTok{N }\SpecialCharTok{+}\NormalTok{ bM}\SpecialCharTok{*}\NormalTok{M ,}
\NormalTok{    a }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{( }\DecValTok{0}\NormalTok{ , }\FloatTok{0.2}\NormalTok{ ) ,}
\NormalTok{    bN }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{( }\DecValTok{0}\NormalTok{ , }\FloatTok{0.5}\NormalTok{ ) ,}
\NormalTok{    bM }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dnorm}\NormalTok{( }\DecValTok{0}\NormalTok{ , }\FloatTok{0.5}\NormalTok{ ) ,}
\NormalTok{    sigma }\SpecialCharTok{\textasciitilde{}} \FunctionTok{dexp}\NormalTok{( }\DecValTok{1}\NormalTok{ )}
\NormalTok{  ),}
  \AttributeTok{data=}\NormalTok{milk\_df)}


\CommentTok{\# function to compute counterfactuals}
\NormalTok{compute\_counterfactual }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(quap\_fit, neocortex, logmass)\{}
\NormalTok{  mu }\OtherTok{\textless{}{-}} \FunctionTok{link}\NormalTok{(quap\_fit, }\AttributeTok{data=}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{N=}\NormalTok{neocortex, }\AttributeTok{M=}\NormalTok{logmass))}
\NormalTok{  mu\_PI }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(mu,}\DecValTok{2}\NormalTok{,PI)}
  \FunctionTok{tibble}\NormalTok{(}\AttributeTok{M =}\NormalTok{ logmass,}
         \AttributeTok{N =}\NormalTok{ neocortex,}
         \AttributeTok{K =} \FunctionTok{apply}\NormalTok{(mu,}\DecValTok{2}\NormalTok{,mean),}
         \AttributeTok{KLower =}\NormalTok{ mu\_PI[}\DecValTok{1}\NormalTok{, ],}
         \AttributeTok{KUpper =}\NormalTok{ mu\_PI[}\DecValTok{2}\NormalTok{, ])}
\NormalTok{\}}



\CommentTok{\# defining x{-}ticks as all milk values at regular intervals}
\NormalTok{x\_seq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{( }\AttributeTok{from=}\FunctionTok{min}\NormalTok{(milk\_df}\SpecialCharTok{$}\NormalTok{M)}\SpecialCharTok{{-}}\FloatTok{0.15}\NormalTok{ , }\AttributeTok{to=}\FunctionTok{max}\NormalTok{(milk\_df}\SpecialCharTok{$}\NormalTok{M)}\SpecialCharTok{+}\FloatTok{0.15}\NormalTok{ , }\AttributeTok{length.out=}\DecValTok{30}\NormalTok{)}

\CommentTok{\# mean neocortex percent for each neocortex percent group}
\NormalTok{neocortex\_counterfactuals\_df }\OtherTok{\textless{}{-}}
  \CommentTok{\# compute average neocortex for each group}
\NormalTok{  milk\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{MeanPercent =} \FunctionTok{mean}\NormalTok{(N), }\AttributeTok{.groups=}\StringTok{"keep"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  
  \CommentTok{\# compute counterfactual predictions for each group}
  \FunctionTok{group\_modify}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{compute\_counterfactual}\NormalTok{(m5}\FloatTok{.7}\NormalTok{, }\AttributeTok{neocortex=}\NormalTok{.x}\SpecialCharTok{$}\NormalTok{MeanPercent, }\AttributeTok{logmass=}\NormalTok{x\_seq)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
  
\CommentTok{\# plotting data}
\FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ M, }\AttributeTok{y=}\NormalTok{K, }\AttributeTok{color=}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_ribbon}\NormalTok{(}\AttributeTok{data=}\NormalTok{neocortex\_counterfactuals\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{KLower, }\AttributeTok{ymax=}\NormalTok{KUpper, }\AttributeTok{fill=}\StringTok{\textasciigrave{}}\AttributeTok{Neocortex group}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{color=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.25}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data=}\NormalTok{neocortex\_counterfactuals\_df) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape=}\DecValTok{21}\NormalTok{, }\AttributeTok{fill=}\StringTok{"white"}\NormalTok{, }\AttributeTok{size=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"log body mass (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"right"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{col =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{coord\_equal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-72-1.pdf}

And we can do this the other way around grouping species by their log body mass.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logmass\_counterfactuals\_df }\OtherTok{\textless{}{-}}
\NormalTok{  milk\_df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Mass group}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{MeanMass =} \FunctionTok{mean}\NormalTok{(M), }\AttributeTok{.groups=}\StringTok{"keep"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  
  \CommentTok{\# compute counterfactual predictions for each group}
  \FunctionTok{group\_modify}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{compute\_counterfactual}\NormalTok{(m5}\FloatTok{.7}\NormalTok{, }\AttributeTok{neocortex=}\NormalTok{x\_seq, }\AttributeTok{logmass=}\NormalTok{.x}\SpecialCharTok{$}\NormalTok{MeanMass)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}
  
\CommentTok{\# plotting data}
\FunctionTok{ggplot}\NormalTok{(milk\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ N, }\AttributeTok{y=}\NormalTok{K, }\AttributeTok{color=}\StringTok{\textasciigrave{}}\AttributeTok{Mass group}\StringTok{\textasciigrave{}}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_ribbon}\NormalTok{(}\AttributeTok{data=}\NormalTok{logmass\_counterfactuals\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{KLower, }\AttributeTok{ymax=}\NormalTok{KUpper, }\AttributeTok{fill=}\StringTok{\textasciigrave{}}\AttributeTok{Mass group}\StringTok{\textasciigrave{}}\NormalTok{), }\AttributeTok{color=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.25}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data=}\NormalTok{logmass\_counterfactuals\_df) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape=}\DecValTok{21}\NormalTok{, }\AttributeTok{fill=}\StringTok{"white"}\NormalTok{, }\AttributeTok{size=}\DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"neocortex percent (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"kilocal per g (std)"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"right"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{guides}\NormalTok{(}\AttributeTok{col =} \FunctionTok{guide\_legend}\NormalTok{(}\AttributeTok{nrow =} \DecValTok{3}\NormalTok{, }\AttributeTok{byrow =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{coord\_equal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-73-1.pdf}

\hypertarget{a-warning-message}{%
\section{A warning message}\label{a-warning-message}}

Do these two examples (spriouos associations and masked relationship) mean that putting \emph{all} variables into a model is always a good idea because ``magic''? Unfortunately, no. These examples were handpicked by McElreath to show the power of multiple regression. He can also handpick an example when mindlessly throwing variables into a model would lead to a disaster, a.k.a. \href{https://elevanth.org/blog/2021/06/15/regression-fire-and-dangerous-things-1-3/}{Causal Salad} (watch his \href{https://youtu.be/KNPYUVmY3NM}{\textbf{three hour talk}} on that on YouTube). The take home message is same as before: Models are golems, they can help you understand the process you are investigating but they won't understand it for you! It is your job to think about causal relationship using statistical inferences merely as an aid, not as an oracle.

\hypertarget{ordered-categorical-data-i.e.-likert-scales}{%
\chapter{Ordered Categorical Data, i.e., Likert-scales}\label{ordered-categorical-data-i.e.-likert-scales}}

One of a very popular type of response in psychology and social sciences are so-called Likert-scale response. For example, you may be asked to respond on how attractive you find a person in a photo from 1 (very unattractive) to 7 (very attractive). Or to respond how satisfied you are with a service from 1 (very unsatisfied) to 4 (very satisfied). Or rate your confidence on a 5-point scale, etc. Likert-scale responses are extremely common and are quite often analyzed via linear models (i.e., a \emph{t}-test, a repeated measures ANOVA, or linear-mixed models) assuming that response levels correspond directly to real numbers. The purpose of these notes is to document both conceptual and technical problems this approach entails.

\hypertarget{conceptualization-of-responses-internal-continuous-variable-discritized-into-external-responses-via-a-set-of-cut-points}{%
\section{Conceptualization of responses: internal continuous variable discritized into external responses via a set of cut-points}\label{conceptualization-of-responses-internal-continuous-variable-discritized-into-external-responses-via-a-set-of-cut-points}}

First, let us think what behavioral responses correspond to as it will become very important once we discuss conceptual problems with a common ``direct'' approach of using linear models for Likert-scale data.

When we ask a participant to respond ``On scale from 1 to 7, how attractive do you find the face in the photo?'', we assume that there is a \emph{continuous} internal variable (for example, encoded via a neural ensemble) that represents attractiveness of a face (our satisfaction with service, our confidence, etc.). The strength of that representation varies in a continuous manner from its minimum (e.g., baseline firing rate, if we assume that strength is encoded by spiking rate) to maximum (maximum firing rate for that neural ensemble). When we impose a seven-point scale on participants, we force them to discretize (bin) this continuous variable, creating a \emph{many-to-one} mapping. In other words, a participant decides that values (intensities) within a particular range all get mapped on \(1\), a different but adjacent range of higher values corresponds to \(2\), etc. You can think about it as values within that range being ``rounded''\footnote{regressed?} towards the mean that defines the response. Or, equivalently, you can think in terms of cut points that define range for individual values. This is how the discretization is depicted in the figure below. If the signal is below the first cut point, our participant's response is ``1''. When it is between first and second cut points, the response is ``2'' and so on. When it is to the right of the last sixth cut point, it is ``7''. This conceptualization means that responses are an ordered categorical variable, as any underlying intensity for a response ``1'' is necessarily smaller than \emph{any} intensity for response ``2'' and both are smaller than, again, \emph{any} intensity for response ``3'', etc.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-75-1.pdf}

As per usual, even when we use the same stimulus and ask the same question, participant's internal continuous response varies from trial to trial due to noise. We model this by assuming that on a given trial a value is drawn from a normal distribution centered at the ``true'' intensity level\footnote{Here, I will assume that cut points are fixed and it is parameters of the normal distribution that get adjusted. The actual implementation of ordered logit/probit models has it the other way around, so that intensity always comes from a normal distribution centered at \(0\) and with a standard deviation of \(1\) and it is cut-points that get adjusted. The two are mostly mathematically equivalent but I find the former to be more intuitive.}. When the noisy intensity is converted to discrete responses, their variability will depend on the location (mean) and the width (standard deviation) of this distribution. The broader this distribution and / or closer it is to a cut point, the more activity will ``spill over'' a cut point into adjacent regions and more variable discrete responses will be.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-76-1.pdf}

Given this conceptualization, our goal is to recover cut points and model shifts of the mean for the \emph{continuous} internal variable (in response to our experimental manipulation) using only observed \emph{discrete} responses.

\hypertarget{conceptual-problem-with-linear-models-we-change-our-mind-about-what-responses-correspond-to.}{%
\section{Conceptual problem with linear models: we change our mind about what responses correspond to.}\label{conceptual-problem-with-linear-models-we-change-our-mind-about-what-responses-correspond-to.}}

A very common approach is to fit Likert-scale data using a linear model (a \emph{t}-test, a repeated-measures ANOVA, linear-mixed models, etc.) while assuming that responses correspond directly to real numbers. In other words, when participants responded ``very unattractive'', or ``not confident at all'', or ``do not agree at all'' they literally meant a real number \(1.0\). When they used the middle (let's say the third on a five-point scale) option ``neither agree, nor disagree'' they literally meant \(3.0\).

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-77-1.pdf}

This assumption appears to simplify our life dramatically but at the expense of changing the narrative. Recall that our original (and, for me, very intuitive) conceptualization was that responses reflect a many-to-one mapping between an underlying continuous variable and a discrete (ordered categorical) response. But by converting them directly to real numbers and using them as an outcome variable of a linear model we assume a \emph{one-to-one} mapping between the \emph{continuous} real-valued internal variable and \emph{continuous}(!) real-valued observed responses. This means that from a linear model point of view, for a 7-point Likert scale \emph{any} real value is a valid and possible response and therefore a participant \emph{could have} responded with 6.5, 3.14, or 2.71828 but, for whatever reason (sheer luck?), we only observed a handful of (integer) values.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-78-1.pdf} \includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-78-2.pdf}

Notice that this is \emph{not} how we thought participants behave. I think everyone\footnote{Never say always!} would object to the idea that the limited repertoire of responses is due to endogenous processing rather than exogenous limitations imposed by an experimental design. Yet, this is how a \emph{linear model} ``thinks'' about it given the outcome variable you gave it and, if you are not careful, it is easy to miss this change in the narrative. It is, however, important as it means that estimates produced by such model are about that alternative one-to-one kind of continuous responses, not the many-to-one discrete ones that you had in mind! That alternative is not a bad story per se, it is just a \emph{different} story that should not be confused with the original one.

This change in the narrative of what responses correspond to is also a problem if you want to use a (fitted) linear model to generate predirection and simulate the data. It will happily spit out real valued responses like 6.5, 3.14, or 2.71828\footnote{If you feel lucky enough to expect an integer response, you'd better use this luck on playing an actual lottery}. You have two options. You can bite the bullet and take them at their face value, sticking to ``response is a real-valued variable'' and one-to-one mapping between an internal variable and an observed response. That lets you keep the narrative but means that real and ideal observers play by different rules. Their responses are different and that means your conclusions based on an ideal observer behavior are of limited use. Alternatively, you can round real-valued responses off to a closest integer getting discrete categorical-like responses. Unfortunately, that means changing the narrative yet again. In this case, you fitted the model assuming a one-to-one mapping but you use its predictions assuming many-to-one. Not good. It is really hard to understand what is going on, if you keep changing your mind on what responses mean. A linear model will also generate out-of-range responses, like -1 or 8. Here, you have little choice but to clip them into the valid range, forcing the many-to-one mapping on at least some responses. Again, change of a narrative means that model fitting and model interpretation rely on different conceptualizations of what response is.

This may sound too conceptual but I suspect that few people who use linear models on Likert-scale data directly realize that their model is not doing what they think it is doing and, erroneously!, interpret one-to-one linear-model estimates as many-to-one. The difference may or may not be crucial but, unfortunately, one cannot know how important it is without comparing two kind of models directly. And that raises a question: Why employ a model that does something different to what you need to to being with? Remember, using an appropriate model and interpreting it correctly is \emph{your} job, not that of a mathematical model, nor is it a job of a software package.

\hypertarget{a-technical-problem-data-that-bunches-up-near-a-range-limit.}{%
\section{A technical problem: Data that bunches up near a range limit.}\label{a-technical-problem-data-that-bunches-up-near-a-range-limit.}}

When you use a linear model, you assume that residuals are normally distributed. This is something that you may not be sure of \emph{before} you fit a specific model, as it is residuals not the data that must be normally distributed. However, in some cases you may be fairly certain that this will not be the case, such as when a variable has only a limited range of values and the mean (a model prediction) is close to one of these limits. Whenever you have observations that are close to that hard limit, they will ``bunch up'' against it because they cannot go lower or higher than that. See the figure below for an illustration of how it happens if a \emph{continuous} variable \(x\) is restricted to 1 to 7 range\footnote{Note that for skewed distributions their mode is different from the mean!}.
\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-79-1.pdf}

The presence of a limit is not a deal breaker for using linear models per se. Most physical measures cannot be negative\footnote{Try coming up with one and don't say ``temperature''!} but as long your observations are sufficiently far away from zero, you are fine. You cannot have a negative height but you certainly can use linear models for adult height as, for example, an average female height in USA is 164±6.4 cm. In other words, the mean is more than 25 standards deviations away from the range limit of zero and the latter can be safely ignored.

Unfortunately, Likert-scale data combines an extremely limited range with a very coarse step. Even a 7-point Likert scale does not give you much of a wiggle room and routinely used 5-point scales are even narrower. This means that unless the mean is smack in the middle (e.g., at four for a 7-point scale), you are getting closer to one of the limits and your residuals become either positively (when approaching a lower limit) or negatively (for the upper one) skewed. In other words, the residuals are \emph{systematically} not normally distributed and their distributions depends on the mean. This clearly violates an assumption of normality of residuals and of their conditional i.i.d. (Independent and Identically Distributed). This is a deal breaker for parametric frequentist statistics (a \emph{t}-test, a repeated-measures ANOVA, linear-mixed models), as their inferences are built on that assumptions and, therefore, become unreliable and should not to be trusted.

\hypertarget{another-technical-problem-can-we-assume-that-responses-correspond-to-real-numbers-that-we-picked}{%
\section{Another technical problem: Can we assume that responses correspond to real numbers that we picked?}\label{another-technical-problem-can-we-assume-that-responses-correspond-to-real-numbers-that-we-picked}}

The skewed residuals described above are a fundamental problem for parametric frequentist methods but is not critical if you use Bayesian or non-parametric bootstrapping/permutation linear models. Does this mean it is safe to use them? Probably not. When you use responses directly, you assume a direct correspondence between a response label (e.g., ``agree'') and a real number \(4.0\). If your responses do correspond to the real numbers you have picked, you can perform the usual arithmetic with them. E.g., you can assume that \((4.0 + 4.0) / 2\) is equal to \((3.0 + 5.0) / 2\) to \((2.0 + 6.0) / 2\) to \((1.0 + 7.0)/ 2\). However, what if this is \emph{not} the case, what if the responses do not correspond to the real numbers that you've picked? Then our basic arithmetic stops working the way you think! Take a look at the figure below where ``real value'' of responses is not an integer that we have picked for it.

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-80-1.pdf}

Unless you \emph{know} that response levels correspond to the selected real number and that the simple arithmetic holds, you are in danger of computing nonsense. This problem is more obvious when individual response levels are labelled, e.g., \texttt{"Strongly\ disagree"}, \texttt{"Disagree"}, \texttt{"Neither\ disagree,\ nor\ agree"}, \texttt{"Agree"}, \texttt{"Strongly\ agree"}. What is an average of \texttt{"Strongly\ disagree"} and \texttt{"Strongly\ agree"}? Is it the same as an average of \texttt{"Disagree"} and \texttt{"Agree"}? Is increase from \texttt{"Strongly\ disagree"} to \texttt{"Disagree"} identical to that from \texttt{"Neither\ disagree,\ nor\ agree"} to \texttt{"Agree"}? The answer is ``who knows?!'' but in my experience scales are rarely truly linear as people tend to avoid extremes and have their own idea about range of internal variable levels that correspond to a particular response.

As noted above, even when scale levels are explicitly named, it is very common to ``convert'' them to numbers because you cannot ask computer to compute an average of \texttt{"Disagree"} and \texttt{"Agree"} (it will flatly refuse to do this) but it will compute an average of \(2\) and \(4\). And there will be no error message! And it will return \(3\)! Problem solved, right? Not really. Yes, the computer will not complain but this is because it has no idea what \(2\) and \(4\) stand for. You give it real numbers, it will do the math. So, if you pretend that \texttt{"Disagree"} and \texttt{"Agree"} correspond directly to \(2\) and \(4\) it will certainly \emph{look like} normal math. And imagine that responses are \texttt{"Disagree"} and \texttt{"Strongly\ agree"}, so the numbers are \(2\) and \(5\) and the computer will return an average value of \(3.5\). It will be even easier to convince yourself that your responses are real numbers (see, there is a \emph{decimal point} where!), just like linear models assume. Unfortunately, you are not fooling the computer (it seriously does not care), you are fooling yourself. Your math might check out, if responses do correspond to the real numbers you have picked, or it might not. And in both cases, there will be no warning or an error message, just some numbers that you will interpret at their face value and reach possibly erroneous conclusions. Again, the problem is that you wouldn't know whether the numbers you are looking at are valid or nonsense and the same dilemma (valid or nonsense?) will be applicable to any inferences and conclusions that you draw from them. In short, a direct correspondence between response levels and specific real numbers is a \emph{very} strong assumption that should be validated, not taken on pure faith.

\hypertarget{solution-an-ordered-logitprobit-model}{%
\section{Solution: an ordered logit/probit model}\label{solution-an-ordered-logitprobit-model}}

So far I have summarized problems of using linear models when assuming that responses correspond to real numbers. How can we solve them? By using ordered \href{https://en.wikipedia.org/wiki/Ordered_logit}{logistic}/\href{https://en.wikipedia.org/wiki/Ordered_probit\#:~:text=In\%20statistics\%2C\%20ordered\%20probit\%20is,fair\%2C\%20good\%2C\%20excellent}{probit} models. They are built using the many-to-one mapping between a continuous variable that has a limited range (for simplicity it ranges from 0 to 1) and is discretized to match behavioral responses using a set of cut points. In principle, the latter can be fixed but in most cases they should be fitted as part of the model. Both logit and probit models assume that the sampling distribution of the underlying continuous variable is a standard normal and, therefore, both the continuous variable and cut points live on the infinite real number line that is transformed to 0..1 range via either logit or probit link function. Strictly speaking, the latter step is not necessary but makes things easier both for doing math and for understanding.

From a mathematical point of view, using logit and probit makes it easy to compute the area under the curve between two cut points. Logit or probit are cumulative functions, so for a standard normal distribution (centered at \(0\) with standard deviation of \(1\)) they compute an area under the curve starting from \(-\infty\) up to some point \(k_i\).

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-81-1.pdf} \includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-81-2.pdf}

Therefore, if we want to compute an area between two cut points \(k_{i-1}\) and \(k_i\), we can do it as \(logit(k_{i})-logit(k_{i-1})\) (same goes for probit).

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-82-1.pdf}

Because both logit and probit are non-linear transformations cut points evenly distributed on a 0..1 range will be not evenly distributed on the real numbers line and vice versa. Transforming from real space to 0..1 range also makes it easier to understand relative positions of cut points and changes in continuous variable (that we translate into discrete responses via cut points).

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-83-1.pdf}

\includegraphics{notes-on-statistical-rethinking_files/figure-latex/unnamed-chunk-84-1.pdf}

\hypertarget{using-ordered-logitprobit-models}{%
\section{Using ordered logit/probit models}\label{using-ordered-logitprobit-models}}

There are several R packages that implement regression models for ordinal data including specialized packages \href{https://cran.r-project.org/package=ordinal}{ordinal}, \href{https://cran.r-project.org/package=oglmx}{oglmx}, as well as via a \href{https://cran.r-project.org/web/packages/brms/vignettes/brms_monotonic.html}{ordered} option in \href{https://cran.r-project.org/package=brms}{brms} package.

From coding point of view, fitting an ordered logit/probit the model is easy as fitting any other regression model. However, the presence of the link function complicates its understanding as all parameters interact\footnote{The usual curse of generalized linear models and, probably, one of the reasons while people tend to stick to linear models even if they are potentially or clearly invalid.}. My current approach not to try to interpret the parameters directly but to plot a triptych.

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Compute posterior predictions and compare their distribution with behavioral data to understand how well the model fits the data.
\end{enumerate}

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.8\linewidth]{images/ordinal-predictions}

\}

\textbackslash caption\{Behavioral data (circles and error bars depict group average and bootstrapped 89\% confidence intervals) versus model posterior predictions (lines and ribbons depict mean and 89\% compatibility intervals).\}\label{fig:unnamed-chunk-85}
\textbackslash end\{figure\}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Visualize cut points on a 0..1 range to understand the mapping between continuous intensity and discrete responses as well as uncertainty about their position.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/ordinal-cutpoints} 

}

\caption{Posterior distribution for cut points transformed to 0..1 range.}\label{fig:unnamed-chunk-86}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Visualize and compare changes in continuous intensity on a 0..1 range adding cut points to facilitate understanding.
\end{enumerate}

\textbackslash begin\{figure\}

\{\centering \includegraphics[width=0.5\linewidth]{images/ordinal-change}

\}

\textbackslash caption\{Posterior distribution and change for continuous intensity variable transformed to 0..1 range. Text above plot show mean and 89\% credible interval for the change.\}\label{fig:unnamed-chunk-87}
\textbackslash end\{figure\}

  \bibliography{book.bib}

\end{document}
