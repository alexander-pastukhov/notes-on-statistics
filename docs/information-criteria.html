<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Information Criteria | Notes on Statistics</title>
<meta name="author" content="Alexander Pastukhov">
<meta name="description" content="These are notes on information criteria. Their purpose is to provide some intuition about the information criteria, supplementing information presented in chapter 7 of the Statistical Rethinking...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 6 Information Criteria | Notes on Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="These are notes on information criteria. Their purpose is to provide some intuition about the information criteria, supplementing information presented in chapter 7 of the Statistical Rethinking...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Information Criteria | Notes on Statistics">
<meta name="twitter:description" content="These are notes on information criteria. Their purpose is to provide some intuition about the information criteria, supplementing information presented in chapter 7 of the Statistical Rethinking...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="bs4_style.css%20-%20style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Notes on Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Precis</a></li>
<li><a class="" href="loss-functions.html"><span class="header-section-number">2</span> Loss functions</a></li>
<li><a class="" href="directed-acyclic-graphs-and-causal-reasoning.html"><span class="header-section-number">3</span> Directed Acyclic Graphs and Causal Reasoning</a></li>
<li><a class="" href="spurious-association.html"><span class="header-section-number">4</span> Multiple regression - Spurious association</a></li>
<li><a class="" href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></li>
<li><a class="active" href="information-criteria.html"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="" href="bayesian-vs.-fequentist-statisics.html"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></li>
<li><a class="" href="mixtures.html"><span class="header-section-number">8</span> Mixtures</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">9</span> Instrumental Variables</a></li>
<li><a class="" href="parameters-combining-information-from-an-individual-with-population.html"><span class="header-section-number">10</span> Parameters: combining information from an individual with population</a></li>
<li><a class="" href="incorporating-measurement-error-a-rubber-band-metaphor.html"><span class="header-section-number">11</span> Incorporating measurement error: a rubber band metaphor</a></li>
<li><a class="" href="generalized-additive-models-as-continuous-random-effects.html"><span class="header-section-number">12</span> Generalized Additive Models as continuous random effects</a></li>
<li><a class="" href="flat-priors-the-strings-attached.html"><span class="header-section-number">13</span> Flat priors: the strings attached</a></li>
<li><a class="" href="unbiased-mean-versus-biased-variance-in-plain-english.html"><span class="header-section-number">14</span> Unbiased mean versus biased variance in plain English</a></li>
<li><a class="" href="probability-mass-versus-probability-density.html"><span class="header-section-number">15</span> Probability mass versus probability density</a></li>
<li><a class="" href="effective-degrees-of-freedom-number-of-parameters.html"><span class="header-section-number">16</span> Effective degrees of freedom / number of parameters</a></li>
<li><a class="" href="multiple-regression---masked-relationship.html"><span class="header-section-number">17</span> Multiple regression - Masked relationship</a></li>
<li><a class="" href="ordered-categorical-data-i.e.-likert-scales.html"><span class="header-section-number">18</span> Ordered Categorical Data, i.e., Likert-scales</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alexander-pastukhov/notes-on-statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="information-criteria" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Information Criteria<a class="anchor" aria-label="anchor" href="#information-criteria"><i class="fas fa-link"></i></a>
</h1>
<p>These are notes on information criteria. Their purpose is to provide some intuition about the information criteria, supplementing information presented in chapter 7 of the <em>Statistical Rethinking</em> book by Richard McElreath. I deliberately oversimplified and overgeneralized certain aspects to paint a “bigger picture”.</p>
<div id="deviance" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Deviance<a class="anchor" aria-label="anchor" href="#deviance"><i class="fas fa-link"></i></a>
</h2>
<p>In the chapter, deviance is introduced as an estimate for KL-divergence, which in turn is a relative entropy, i.e., the difference between cross-entropy and actual entropy of events. Keep that in mind but you could look at deviance itself as a straightforward goodness-of-fit measure, similar to squared residuals (RMSE, Root Mean Square Error) and coefficient of determination <span class="math inline">\(R^2\)</span>. In both cases, you have difference between model prediction (the regression line) and an actual data point. In ordinary least squares (OLS) approach, you quantify this imperfection of prediction by squaring residuals. You sum up all residuals to get the sum of squared residuals (<span class="math inline">\(SS_{res}\)</span>) and then you can compute <span class="math inline">\(R^2\)</span> by comparing it to the total sum of residuals in the data (<span class="math inline">\(SS_{total}\)</span>):
<span class="math display">\[R^2 = 1 - \frac{SS_{res}}{SS_{total}} = \frac{SS_{total} - SS_{res}}{SS_{total}}\]</span>
As the total sum of residuals <span class="math inline">\(SS_{res}\)</span> gets close to zero, the fraction gets close to 1. The disadvantage of squared residuals and <span class="math inline">\(R^2\)</span> is that tricky to use with non-metric data, such as binomial, ordered categorical data, etc., or when deviations from the prediction might be asymmetric (binomial data, response times, etc.). Instead, you can use likelihood to compute the probability that a data point comes from a distribution defined by a model. Then, you compute the (total) joint probability by multiplying all probabilities or, better still, by computing the sum of their log-transform (log likelihood)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Mathematically equivalent but the sum of logs is far more numerically stable.&lt;/p&gt;"><sup>15</sup></a>. Then, you can compare the observed log likelihood to the highest theoretically possible log likelihood for a <em>saturated model</em> (<span class="math inline">\(\Theta_s\)</span>), which has as many parameters as there are data points, so that it predicts each point perfectly. This is the original definition of (total) <em>deviance</em>:
<span class="math display">\[D = -2 \cdot (log(p(y|\Theta)) - log(p(y|\Theta_s)))\]</span></p>
<p>Recall that <span class="math inline">\(log(\frac{a}{b}) = log(a) - log(b)\)</span>, so we can rearrange it and see that it is a log-ratio of likelihoods:
<span class="math display">\[D = -2 \cdot  log \left(\frac{p(y|\Theta)}{p(y|\Theta_s)}\right)\]</span>
As <span class="math inline">\(p(y|\Theta)\)</span> increases, the fraction inside get closer to 1. The <span class="math inline">\(log()\)</span> bit flips and non-linearly scales it. The minus sign flips it back and we end up with smaller numbers meaning better fit.
<img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-25-1.png" width="672"></p>
<p>The <span class="math inline">\(2\)</span> is there to facilitate significance testing for <em>nested</em> models. Models <span class="math inline">\(\Theta1\)</span> and <span class="math inline">\(\Theta2\)</span> are nested, if <span class="math inline">\(\Theta2\)</span> has all predictors of <span class="math inline">\(\Theta1\)</span> plus <em>k</em> predictors. E.g., model <span class="math inline">\(Divorce = Marriage~Rate\)</span> is nested inside <span class="math inline">\(Divorce = Marriage~Rate + Marriage~Age\)</span> with later model having 1 more parameter. Your actual model <span class="math inline">\(\Theta\)</span> is nested inside the saturated model <span class="math inline">\(\Theta_s\)</span> that has <span class="math inline">\(k = n - k_{model}\)</span> more parameters, where <span class="math inline">\(n\)</span> is sample size and <span class="math inline">\(k_{model}\)</span> is number of parameters in your model. It turns out that in this case, you can determine whether the difference in goodness-of-fit between two models is significant using <span class="math inline">\(\chi^2\)</span> distribution with <em>k</em> degrees of freedom. The only catch is that log-ratio is half the magnitude, so you need that <span class="math inline">\(2\)</span> to match things up<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;I did not follow the derivation of that correspondence yet, so I cannot comment on how and why.&lt;/p&gt;"><sup>16</sup></a>.</p>
<p>At this point, you might remember that the definition of deviance for a single model in the book was:
<span class="math display">\[D = -2 \cdot log(p(y|\Theta))\]</span></p>
<p>Unfortunately, same term can be used both ways, to refer to a log likelihood of a single model or, technically more correctly, to the log-ratio you saw above. In reality, you will mostly see the deviance defined as in the book because it is used to compare nested models via <span class="math inline">\(\chi^2\)</span> distribution as I’ve described above, with only difference that you compare any two nested models, not just to a saturated one. This is how nested models are frequently compared, for example, see <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/anova.html">anova()</a> function in R.</p>
<p>Note that a deviance for a single model still expresses the same idea of goodness-of-fit but is merely not normalized by the deviance of the saturated model. Thus deviance as in the book <span class="math inline">\(D = -2 \cdot log(p(y|\Theta))\)</span> corresponds to sum of residuals (absolute values mean nothing but you can use them to compare two models on the same data), whereas <em>total</em> deviance corresponds to the <span class="math inline">\(R^2\)</span> (values are directly interpretable).</p>
</div>
<div id="general-idea-information-criteria-as-miles-per-gallon" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> General idea: information criteria as miles-per-gallon<a class="anchor" aria-label="anchor" href="#general-idea-information-criteria-as-miles-per-gallon"><i class="fas fa-link"></i></a>
</h2>
<p>The general formula for all information criteria discussed below is
<span class="math display">\[-2\cdot log \left( \frac{goodness~of~fit}{model~complexity} \right)\]</span></p>
<p>The goodness-of-fit in the numerator is the likelihood, the joint probability of observing the model given each data point <span class="math inline">\(\prod_i p(\Theta|y_i)\)</span>. The denominator expresses model complexity, i.e., its flexibility in fitting the sample and, therefore, its tendency to overfit. Thus, the fraction itself is <em>goodness-of-fit per unit of model complexity</em>. This is like miles-per-gallon for car efficiency, so better models are more efficient models, churning out more goodness per complexity.</p>
<p>The numerator is the same<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Almost, the minor difference is whether you use the entire posterior averaging over samples (DIC, WAIC, PSIS-LOO) or only a single maximum a posteriori sample (AIC, BIC).&lt;/p&gt;"><sup>17</sup></a> for all information criteria discussed below and they differ only in how they compute the model complexity.</p>
</div>
<div id="akaike-information-criterion-aic" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Akaike Information Criterion (AIC)<a class="anchor" aria-label="anchor" href="#akaike-information-criterion-aic"><i class="fas fa-link"></i></a>
</h2>
<p>The formula most commonly used<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;And the one you should use to actually compute it, as it offers a better computation stability.&lt;/p&gt;"><sup>18</sup></a> is
<span class="math display">\[ AIC = -2\cdot log(p(\Theta|y)) + 2k \]</span>
where <span class="math inline">\(k\)</span> is the number of parameters of the model. If you are not a mathematician who is used to translate back-and-forth using logarithms, you may fail spot the ratio I was talking about earlier. For this, you need to keep in mind that <span class="math inline">\(log(\frac{a}{b}) = log(a) - log(b)\)</span> and that <span class="math inline">\(a = log(exp(a))\)</span>. Let us re-arrange a bit to get the log-ratio back:
<span class="math display">\[ AIC = -2\cdot log(p(\Theta|y)) + 2k \]</span>
<span class="math display">\[ AIC = -2 (log(p(\Theta|y)) - k)\]</span>
<span class="math display">\[ AIC = -2 (log(p(\Theta|y)) - log(exp(k))\]</span>
<span class="math display">\[ AIC = -2 \cdot log \left(\frac{p(\Theta|y)}{exp(k)} \right)\]</span>
And here it is, the log-ratio I’ve promised! As you can see, AIC assumes that model complexity grows exponentially with the number of parameters.</p>
<p>If you are to use AIC, the current recommendation is to <em>correct it</em> with an extra penalty for the size of the sample
<span class="math display">\[AICc = AIC + \frac{2k^2 + 2k}{n - k - 1}\]</span>
where <span class="math inline">\(n\)</span> is the sample size. I won’t do it here but you should be able to work out how it is added to the exponent in the denominator.</p>
</div>
<div id="bayesian-information-criterion-bic" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Bayesian information criterion (BIC)<a class="anchor" aria-label="anchor" href="#bayesian-information-criterion-bic"><i class="fas fa-link"></i></a>
</h2>
<p>A.k.a. Schwarz information criterion (SIC, SBC, SBIC)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;McElreath writes that “BIC…it’s not actually an “information criterion.”“. So far, I was not able to figure out why.&lt;/p&gt;"><sup>19</sup></a>. The motivation is the same as with AIC but the penalty (complexity term), in addition to the number of parameters, also reflects the sample size <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[BIC = -2\cdot log(p(\Theta|y)) + log(n) k \]</span></p>
<p>Let us do re-arranging again
<span class="math display">\[BIC = -2\cdot log(p(\Theta|y)) + log(n) k \]</span>
<span class="math display">\[BIC = -2 \left( log(p(\Theta|y)) + log(n) \frac{k}{2} \right) \]</span>
<span class="math display">\[BIC = -2 \left( log(p(\Theta|y)) - log \left(exp(log(n) \cdot \frac{k}{2} \right) \right) \]</span></p>
<p>For the complexity term, we need to keep in mind that <span class="math inline">\(exp(a \cdot b) = exp(a)^b\)</span>. Thus,
<span class="math display">\[exp \left(log(n) \cdot \frac{k}{2} \right)= exp(log(n)) ^ \frac{k}{2} = n^\frac{k}{2}\]</span>
Putting the complexity term back, we get
<span class="math display">\[BIC = -2 \left( log(p(\Theta|y)) - log \left(n^\frac{k}{2} \right) \right)\]</span>
<span class="math display">\[BIC = -2 \cdot log \left(\frac{p(\Theta|y)}{n^\frac{k}{2}} \right)\]</span>
Thus, we end up with very similar power law complexity term which uses sample size instead of Euler’s number as the base.</p>
</div>
<div id="problem-of-aic-and-bic-one-size-may-not-fit-all" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Problem of AIC and BIC: one size may not fit all<a class="anchor" aria-label="anchor" href="#problem-of-aic-and-bic-one-size-may-not-fit-all"><i class="fas fa-link"></i></a>
</h2>
<p>Both AIC and BIC assume that model complexity and flexibility, that leads to overfitting, is reflected in the number of parameters <em>k</em>. However, this is a fairly indirect measure of model flexibility, based on how models <em>in general</em> tend to overfit data <em>in general</em>. But you probably want to know how <em>your specific</em> model uses its parameters to fit <em>your specific</em> sample and how much overfitting you should expect in that specific case. Because even if a parameter is present in the model, it may not be able to fully use it in case of regularization or multilevel (adaptive regularization) models.</p>
<p>Regularization, in form of strong priors, lasso/ridge regression, etc., restricts the range of values that a given parameter can take. Thus, a model cannot exploit it as much as <em>other</em> parameters and will be less able to use it to improve fit to the sample. Similarly, in hierarchical multilevel modeling, you may have dozens or hundreds of parameters that describe intercepts and/or slopes for individual participants (random factors, in general), but most of them could be trivially zero (same as or very similar to the group average) and contribute little to the actual fit. In these cases, a simple raw count, which treats all parameters as equals, will overestimate model complexity.</p>
<p>The desire to go beyond one-size-fits-all approach and be as model- and data-specific led to development of deviance information criterion (DIC) and widely-applicable information criterion (WAIC). Both use the entire posterior distribution of <em>in-sample</em> deviance and base their penalty on how <em>variable</em> this posterior distribution is. Higher variance, meaning that a model produces very different fits ranging from excellent to terrible, hints that model is too flexible for its own good and leads to higher complexity estimate (penalty for overfitting). Conversely, very similar posterior deviance (low variance) means that model is too restricted to fine-tune itself to the sample and its complexity is low.</p>
<p>If you understand why variance of the posterior distribution of divergence is related to models’ flexibility and, therefore, to the number of effective parameters, just skip the next section. If not, I came up with a musical instruments metaphor that I and at least some people I’ve tested it upon found useful.</p>
</div>
<div id="musical-instruments-metaphor" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Musical instruments metaphor<a class="anchor" aria-label="anchor" href="#musical-instruments-metaphor"><i class="fas fa-link"></i></a>
</h2>
<p>Imagine that you are trying to play a song that you have just heard. But the only instrument you have is a triangle. This is not a particularly flexible instruments pitch-wise, so your rendition of that song will not be very good (your model underfits the data). The good news is that even if you do your worst and do not really try, no one will notice because your worst performance will sound very much like your best one. Simply because it is very hard to make songs sound different using a triangle. Thus, if you play that song many times, trying different versions of it with us judging how close you are to the original, the score we will give you will probably be not particularly high but very similar (low variance of the deviance for fitting to sample).</p>
<p>What if I give you an instrument that can vary the pitch at least a bit, like a xylophone for children. Now you have more freedom and your version of the music will sound much more like the original. But, it also gives you an opportunity to make a mess of it, so your rendition might sound nothing like the music you’ve just heard. In other words, a more flexbile instrument increases the difference between the best and the worst possible performance, so the variance of your performances (on how close they are to the original) also increases (higher variance of the deviance). A more flexible instrument will make the difference even bigger. Think violin or trombone which are not restricted to the scale, so you can play any sound in-between and you can match the music you just heard exactly. Imagine that the music your just heard has odd off-the-scale sounds. Was it a defect of the turntable, which cannot go at constant speed, so overall pitch wobbles overtime (noise)? Or is it an experimental music piece that was deliberately designed to sound odd (signal)? If you do not know for sure, you will try to play as close to the original music your heard as possible, matching those off-scale sounds. And, because you can play any sound, your range of possible performance is even larger from one-to-one to “please, have mercy and stop!” (even larger variance of deviance).</p>
<p>In short, variance of your performance (posterior divergence) reflects how flexible your instrument is. But why is it indicative of the <em>effective</em> number of parameters? Here are regularizing priors in the world of music instrument metaphor. Imagine that in addition to the triangle, I also give you a rubber bell<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;It was used during a super-secret meeting in Stanislaw Lem’s Eleventh Voyage of Ijon Tichy, so that no one would hear when they ring that bell!&lt;/p&gt;"><sup>20</sup></a>. Now, technically you have two instruments (your number of parameters is 2) but that bell does not affect your performance (we put very strong regularizing priors so that coefficients are zero or something very-very close to zero). Thus, your ability to play the song did not change and your variance of performance stays the same. Two actual instruments, but only one “effective” one. Or, I give you a piano but allow you to use only one octave and only white keys. Yes, you have a piano but with this regularization it is as complex as as kids’ xylophone. The <em>potential</em> number of notes you can play is great (AIC and BIC would be very impressed and slap a heavy penalty on you) but the actual “effective” range is small. Or, you regularize yourself to play scale-only notes using violin (something you learn to do). In all of these cases, you deliberately restrict yourself. But why? Why not just play as you heard it? I.e., why not fit as well as you can? Because if the song you heard is short (sample is small), regularization based on your knowledge about real life helps you to ignore the noise that is always present. E.g., you know that song is for kids’ xylophone, so even if you heard notes outside of a single octave that was probably a problem with recording. Or, you never heard that piece for violin but you do know other works of this composer and they always use scale-only notes, so you should not use violin to play off-scale sounds <em>in that case</em>.</p>
<p>Multilevel models also limit the actual use of parameters. Imagine you heard a recording of a symphonic orchestra. Lots of violins but you figured out that most of them actually play the same melody. So you can get away with using one violin score (sample group average) and assume that most violins play like that (most participants are very close to group average). Any deviations from that group melody are probably mistakes by individual musicians, not the actual melody. Same goes if you hear a choir. Again, many people sing (lots of parameters!) but, mostly, in unison, so you do not need to create an individual score sheet (parameter) for each singer, just one per group of singers.</p>
<p>Wrapping up the metaphor, the more flexible your instrument is, the more variable your performance can be, the easier it is for you to mimic noise and imperfections of the recording that have nothing to do with the piece itself. But when you play it next time, matching the recording with all its noise and distortions perfectly, people who know the piece will be puzzled or may not even recognize it (poor out-of-sample predictions). Adopting the melody for a more limited instrument may make it easier for others to recognize the song! Thus, higher variance in performance accuracy (higher variance of deviance) indicates that you can overfit easily with that instrument (model) and you should be extra careful (impose higher penalty for complexity).</p>
</div>
<div id="deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic" class="section level2" number="6.7">
<h2>
<span class="header-section-number">6.7</span> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)<a class="anchor" aria-label="anchor" href="#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><i class="fas fa-link"></i></a>
</h2>
<p>The two are very similar, as both compute the model complexity based on posterior distribution of log likelihood. The key difference is that DIC sums the log likelihood for each model (sample) first and then computes the variance over samples. WAIC computes variance of log likelihood per point and then sums those variances up. In the musical instrument metaphor, for DIC you perform the piece many times (generate many posterior samples), compute accuracy for each performance (deviance for a single sample), and then compute how variable they are. For WAIC, you go note by note (observation by observation). For each note you compute variance over all samples to see how consistent you are in playing it. Then, you sum this up.</p>
<p><span class="math display">\[DIC = -2 \cdot \left( log(p(y|\Theta)) - var(\sum log(p(y|\Theta_i))) \right)\]</span>
<span class="math display">\[WAIC = -2 \cdot \left( log(p(y|\Theta)) - \sum_i var(log(p(y_i|\Theta))) \right)\]</span></p>
<p>The penalty replaces <span class="math inline">\(k\)</span> in AIC and, therefore, will go into the exponent inside the ratio. Again, same idea, that increase in variance of deviance (either per sample in DIC or per point in WAIC) leads to exponentially increasing estimate of complexity.</p>
<p>WAIC is more stable mathematically and is mode widely applicable (that’s what statisticians tell us). Moreover, its advantage is that it explicitly recognizes that not all data points in your sample are equal. Some (outliers) are much harder to predict than others. And it is variance of log likelihood for these points that determines how much your model can overfit. An inflexible model will make a poor but consistent job (triangles don’t care about pitch!), whereas a complex model can do anything from spot-on to terrible (violins can do anything). In short, you should use WAIC yourself but recognize DIC when you see it and think of it as somewhat less reliable WAIC, which is still better than AIC or BIC when you use regularizing priors and/or hierarchical models.</p>
</div>
<div id="importance-sampling" class="section level2" number="6.8">
<h2>
<span class="header-section-number">6.8</span> Importance sampling<a class="anchor" aria-label="anchor" href="#importance-sampling"><i class="fas fa-link"></i></a>
</h2>
<p>Importance sampling is mentioned in the chapter but is never explained, so here is a brief description. The core idea is to pretend that you sample from a distribution you need (but have no access to or sampling from it directly is very inefficient) by sampling from another distribution (the one you have access to and that you can sample efficiently) and “translating” the probabilities via <em>importance ratios</em>. What does this mean?</p>
<p>Imagine that you want to know an average total score for a given die after you throw it ten times. The procedure is as simple as it gets: you toss the die ten times, record the number you get on each throw, sum them up at the end. Repeat the same toss-ten-times-and-sum-it-up as many times as you want and compute your average. But what if you do not have access to that die because it is <em>the die</em> and is kept under lock in International Bureau of Weights and Measures? Well, you have <em>a die</em> which you can toss and you have a list of <em>importance ratios</em> for each number. These <em>important ratios</em> tell you how much more likely is the number for <em>the die</em> (the one you are after) compared to <em>a die</em> you have in your hand. Let’s say the importance ratio for <em>1</em> (so, number 1 comes up on top) is <code>3.0</code>. This means that whenever <em>your die</em> gives you <em>1</em>, you assume that <em>the die</em> came up <em>1</em> on <strong>three</strong> throws. If the importance ratio for <em>2</em> is 0.5, whenever you see <em>2</em> on your die, you record only half the throw (<em>2</em> comes up twice as rarely for real die than for your die, so two throws that give you <em>2</em> amount to a single throw). This way you can toss your die and every toss equates to different number of throws that generated the same number for <em>the die</em>. So, you sample your die but record outcomes for the other die. Funny thing is that you don’t even need to know how fair your die is and what is the probability of individual sides. As long as you know the importance ratios, keep tossing it and translating the probabilities, you will get the samples for <em>the die</em> you are interested in.</p>
<p>Note that if you toss your die ten times, the translated number of tosses for <em>the die</em> does not need to add up to ten. Imagine that, just by chance, you got <em>1</em> four times. Given the importance ratio of <code>3.0</code> that alone translates into twelve tosses. Solution? You <em>normalize</em> your result by <em>sum of importance ratios</em> and get back you ten tosses.</p>
<p>The very obvious catch is, <em>how do we know the importance ratios</em>? Well, that is situation specific. Sometimes, we can compute them because we know both distributions, it just that one is easier to sample than the target one, so, we optimize the use of computing power<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;A canonical example is using importance ratios to sample tails of a distribution, which you have access to but the data points from tails comes up by chance so rarely that sampling them directly is very inefficient.&lt;/p&gt;"><sup>21</sup></a>. Sometimes, as in case of PSIS/LOO below, we can use an approximation.</p>
</div>
<div id="pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo" class="section level2" number="6.9">
<h2>
<span class="header-section-number">6.9</span> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)<a class="anchor" aria-label="anchor" href="#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><i class="fas fa-link"></i></a>
</h2>
<p>The importance sampling I’ve described above is the key to the PSIS/LOO. The idea is the same, we want to sample from the posterior of the model that was fitted <em>without</em> a specific data point <span class="math inline">\(y_i\)</span> (we write it as <span class="math inline">\(p(\Theta_{-i,s}|y_i)\)</span>). But we do not really want to refit the model. Instead, we want to use what we already have, samples from the model that was fit on all the data, <em>including</em> point <span class="math inline">\(y_i\)</span>. So, wise minds figured out how to use the importance sampling trick, sampling from <span class="math inline">\(p(\Theta_s|y_i)\)</span> and translating it to <span class="math inline">\(p(\Theta_{-i,s}|y_i)\)</span>. The only thing we need are importance factors
<span class="math display">\[r_i = \frac{p(\Theta_{-i,s}|y_i)}{p(\Theta_s|y_i)} \propto \frac{1}{p(y_i|\Theta_s)}\]</span></p>
<p>The importance ratio tells you the worse you are at predicting a point <span class="math inline">\(y_i\)</span> <em>in-sample</em> (the smaller the <span class="math inline">\(p(y_i|\Theta_s)\)</span> is), the more important it is when you consider model’s performance <em>out-of-sample</em> (the larger is <span class="math inline">\(\frac{1}{p(y_i|\Theta_s)}\)</span>). Here is an intuition behind this. <em>Any</em> observation will be harder to predict, if it was not included into the data the model was trained on. This is because, in its absence the model will use its parameters to fit the data that is present, including fitting noise, if it has spare parameters. So, you expect that out-of-sample deviance (<span class="math inline">\(p(y_i|\Theta_{-i}\)</span>) should be always worse than in-sample deviance for the same observation (<span class="math inline">\(p(y_i|\Theta\)</span>). How much worse depends on how “typical” the observation is. If it is typical and “easy” for a model to predict, in its absence the model will still see many similar “typical” observations and will be well prepared to predict it. However, if the observations is atypical, an outlier, the model won’t see too many observations that are alike and will concentrate more on typical points.</p>
<p>More specifically, the penalty for a particular point based on the importance ratios across all samples reflects how <em>variable</em> <span class="math inline">\(p(y_i|\Theta_s)\)</span> is across the samples. Recall the definition in the book
<span class="math display">\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}r(\theta_s)p(y_i|\theta_s)}{\sum^S_{s=1}r(\theta_s)}\]</span>
Substituting the <span class="math inline">\(r_i = \frac{1}{p(y_i|\Theta_s)}\)</span> we get
<span class="math display">\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}p(y_i|\theta_s)}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]</span>
<span class="math display">\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}\frac{p(y_i|\theta_s)}{p(y_i|\Theta_s)}}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]</span>
<span class="math display">\[lppd_{IS} = \sum^N_{i=1} log \frac{\sum^S_{s=1}1}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]</span></p>
<p><span class="math display">\[lppd_{IS} = \sum^N_{i=1} log \frac{S}{\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]</span>
<span class="math display">\[lppd_{IS} = \sum^N_{i=1} log \frac{1}{\frac{1}{S}\sum^S_{s=1}\frac{1}{p(y_i|\Theta_s)}}\]</span></p>
<p>This looks like some bizarre re-arrangement of the terms but it ends up producing a point-wise variance-based penalty very similar to WAIC. You can build your intuition for this by playing with easy to compute numbers. Let us take a vector where all values are the same, i.e., variance is zero and compute <span class="math inline">\(lppd\)</span> and <span class="math inline">\(lppd_{IS}\)</span>.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span><span class="op">)</span> <span class="co"># mean = 0.2, variance = 0</span>
<span class="va">logp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>
<span class="va">lppd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span>
<span class="va">lppd_IS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="fl">1</span><span class="op">/</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## mean = 0.2, variance of log(p) = 0</code></pre>
<pre><code>## lppd    = -1.609438</code></pre>
<pre><code>## lppd_IS = -1.609438</code></pre>
<pre><code>## lppd - lppd_IS =  0</code></pre>
<p>Now let us keep <em>mean</em> the same but very slightly increase <em>variance</em>. As you can see that tiny increase in sample induces a small <em>decrease</em> in <span class="math inline">\(lppd_{IS}\)</span>, i.e., we assume that since the model is variable about this point, it probably has too much power leading to <em>in-sample</em> overfitting and, therefore, poor <em>out-of-sample</em> performance.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.19</span>, <span class="fl">0.2</span>, <span class="fl">0.21</span><span class="op">)</span>
<span class="va">logp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>
<span class="va">lppd</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span>
<span class="va">lppd_IS</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="fl">1</span><span class="op">/</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## mean = 0.2, variance of log(p) = 0.001669798</code></pre>
<pre><code>## lppd    = -1.609438</code></pre>
<pre><code>## lppd_IS = -1.611107</code></pre>
<pre><code>## lppd - lppd_IS =  0.001669449</code></pre>
<p>To understand where does the penalty comes from, it was helpful for me to derive the solution for the case of just two values <span class="math inline">\([p_1, p_2]\)</span>. In this case you can define them as difference to the their mean, just as we did above: <span class="math inline">\(p_1 = \mu - \epsilon\)</span> and <span class="math inline">\(p_2 = \mu + \epsilon\)</span>. The <span class="math inline">\(lppd\)</span> should be trivially equal to <span class="math inline">\(\mu\)</span>:
<span class="math display">\[lppd = \frac{p_1 + p_2}{2}\]</span>
<span class="math display">\[lppd =\frac{(\mu - \epsilon) + (\mu + \epsilon)}{2}\]</span>
<span class="math display">\[lppd =\frac{2 \cdot \mu}{2}\]</span>
<span class="math display">\[lppd = \mu\]</span></p>
<p>What about <span class="math inline">\(lppd_{IS}\)</span>?
<span class="math display">\[lppd_{IS} = \frac{2}{\frac{1}{p_1} + \frac{1}{p_2}}\]</span></p>
<p><span class="math display">\[lppd_{IS} = \frac{2}{\frac{1}{\mu - \epsilon} + \frac{1}{\mu + \epsilon}}\]</span></p>
<p>Bringing the two fractions to a common denominator we get
<span class="math display">\[lppd_{IS} = \frac{2}{\frac{(\mu + \epsilon) + (\mu - \epsilon)}{(\mu + \epsilon)(\mu - \epsilon)}}\]</span>
Opening the brackets in the numerator
<span class="math display">\[lppd_{IS} = \frac{2}{\frac{2 \cdot \mu }{(\mu + \epsilon)(\mu - \epsilon)}}\]</span>
Now we can get flip the bottom fraction
<span class="math display">\[lppd_{IS} = \frac{2 \cdot \frac{(\mu + \epsilon)(\mu - \epsilon)}{2 \cdot \mu}}{\frac{2 \cdot \mu }{(\mu + \epsilon)(\mu - \epsilon)} \cdot \frac{(\mu + \epsilon)(\mu - \epsilon)}{2 \cdot \mu}} \]</span></p>
<p><span class="math display">\[lppd_{IS} = 2 \cdot \frac{(\mu + \epsilon)(\mu - \epsilon)}{2 \cdot \mu}\]</span>
The two goes away
<span class="math display">\[lppd_{IS} = \frac{(\mu + \epsilon)(\mu - \epsilon)}{\mu}\]</span>
Opening the brackets in the numerator
<span class="math display">\[lppd_{IS} = \frac{\mu^2 + \mu\epsilon - \mu\epsilon - \epsilon^2}{\mu}\]</span>
Simplifying
<span class="math display">\[lppd_{IS} = \frac{\mu^2 - \epsilon^2}{\mu}\]</span>
<span class="math display">\[lppd_{IS} = \mu - \frac{\epsilon^2}{\mu}\]</span></p>
<p>Thus when variance (<span class="math inline">\(\epsilon\)</span>, deviation from the mean) increases, the <span class="math inline">\(lppd_{IS}\)</span> decreases</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.0</span>, <span class="fl">0.19</span>, length.out <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>

<span class="va">create_p</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">dp</span><span class="op">)</span> <span class="op">{</span><span class="fl">0.2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="va">dp</span>, <span class="fl">0</span>, <span class="va">dp</span><span class="op">)</span><span class="op">}</span>

<span class="va">lppd_df</span> <span class="op">&lt;-</span> <span class="fu">bind_rows</span><span class="op">(</span>
  <span class="fu">tibble</span><span class="op">(</span>Variance <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">delta</span>, <span class="op">~</span><span class="fu">var2</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">create_p</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
         LPPD <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">delta</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu">create_p</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu">create_p</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
         Kind <span class="op">=</span> <span class="st">"lppd"</span><span class="op">)</span>,
  <span class="fu">tibble</span><span class="op">(</span>Variance <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">delta</span>, <span class="op">~</span><span class="fu">var2</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu">create_p</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
       LPPD <span class="op">=</span> <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_dbl</a></span><span class="op">(</span><span class="va">delta</span>, <span class="op">~</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="op">(</span> <span class="op">(</span><span class="fl">1</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="fu">create_p</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span> <span class="fl">1</span><span class="op">/</span> <span class="fu">create_p</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,
       Kind <span class="op">=</span> <span class="st">"lppd_IS"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">mutate</span><span class="op">(</span>SD <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">Variance</span><span class="op">)</span><span class="op">)</span>

<span class="va">together_plot</span> <span class="op">&lt;-</span> 
  <span class="fu">ggplot</span><span class="op">(</span><span class="va">lppd_df</span>, <span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">Variance</span>, y<span class="op">=</span><span class="va">LPPD</span>, color<span class="op">=</span><span class="va">Kind</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">xlab</span><span class="op">(</span><span class="st">"var log p (WAIC penalty)"</span><span class="op">)</span>

<span class="va">difference_plot</span> <span class="op">&lt;-</span>
  <span class="va">lppd_df</span> <span class="op">%&gt;%</span>
  <span class="fu">pivot_wider</span><span class="op">(</span>names_from <span class="op">=</span> <span class="va">Kind</span>, values_from <span class="op">=</span> <span class="va">LPPD</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">mutate</span><span class="op">(</span>`lppd_IS - lppd` <span class="op">=</span> <span class="va">lppd_IS</span> <span class="op">-</span> <span class="va">lppd</span><span class="op">)</span> <span class="op">%&gt;%</span>
  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x<span class="op">=</span><span class="va">Variance</span>, y<span class="op">=</span><span class="va">`lppd_IS - lppd`</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu">xlab</span><span class="op">(</span><span class="st">"var log p (WAIC penalty)"</span><span class="op">)</span>

<span class="va">together_plot</span> <span class="op">+</span> <span class="va">difference_plot</span></code></pre></div>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-30-1.png" width="672"></div>
<p>Compare the <span class="math inline">\(var log p(y_i|\Theta_s)\)</span>, which is a WAIC penalty term, with the decrease due to variance-based importance ratios. As you can see the two are very close and, therefore, will produce very similar estimates of out-of-sample performance. A <em>Pareto smoothed</em> importance sampling has an advantage over WAIC is that it less keen on reducing the performance based on a few samples, which are smoothed away. Still, both methods should identify the same problematic (high variance) data points that you should pay closer attention to.</p>
</div>
<div id="bayes-factor" class="section level2" number="6.10">
<h2>
<span class="header-section-number">6.10</span> Bayes Factor<a class="anchor" aria-label="anchor" href="#bayes-factor"><i class="fas fa-link"></i></a>
</h2>
<p>Not an information criterion. However, it is a popular way to compare Bayesian models. Compared to information criteria, the logic is reversed. In case of the information criteria, we are asking which <strong>model fits data</strong> the best given the penalty we impose for its complexity. In case of Bayes Factor, we already have two models (could be different models with different number of parameters or just with different parameter values) and we are interested how well the <strong>data matches models</strong> we already have.</p>
<p>Let’s start with the Bayes theorem:
<span class="math display">\[Pr(M|D)=\frac {\Pr(D|M)\Pr(M)}{\Pr(D)}\]</span>
where, <em>D</em> is data and <em>M</em> is the model (hypothesis). The tricky part is the marginal probability (prior) of data <span class="math inline">\(Pr(D)\)</span>. We hardly ever know it for sure, making computing the “correct” value for <span class="math inline">\(Pr(M|D)\)</span> problematic. When using posterior sampling, we side-step the issue by ignoring it and normalizing the posterior by the sum of the posterior distribution. Alternatively, when comparing two models, you can compute their ratio:
<span class="math display">\[{\frac {\Pr(D|M_{1})}{\Pr(D|M_{2})}}={\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}} \cdot {\frac {\Pr(M_{1})}{\Pr(M_{2})}}\]</span>
here <span class="math inline">\(\frac{\Pr(D|M_{1})}{\Pr(D|M_{2})}\)</span> are <strong>posterior odds</strong>, <span class="math inline">\(\frac {\Pr(M_{1}|D)}{\Pr(M_{2}|D)}\)</span> is <strong>Bayes Factor</strong>, and <span class="math inline">\(\frac {\Pr(M_{2})}{\Pr(M_{1})}\)</span> are <strong>prior odds</strong>. The common <span class="math inline">\(Pr(D)\)</span> nicely cancels out!</p>
<p>If you assume that both hypotheses/models are equally likely (you have flat priors), the prior odds are 1:1 and your posterior odds are equal to Bayes Factor or, vice versa, Bayes Factor is equal to posterior odds. This means you can just pick their likelihoods from the posterior sampled distribution and compute the ratio.</p>
<p>I am not a big fan of Bayes Factor for conceptual reasons. Although it can compare any two models (as long as the sample is the same), it looks a lot like a Bayesian version of a p-value and, therefore, lends itself naturally to the null-hypothesis testing. And, as far as my reading of literature in my field is concerned, this is how people most frequently use it, as a cooler Bayesian way of null-hypothesis testing. You have no worries about multiple comparisons (it is Bayesian, so no need for error correction!) and it can prove null hypothesis (it is the ratio, so flip it and see how much stronger <em>H0</em> is)! There is nothing wrong with this per se but the advantage of Bayesian statistics and information criteria is that you do not <em>need</em> to think in terms of null hypothesis testing and nested models. Adopting Bayes Factor may prevent you from seeing this and will allow you to continue doing same analysis just in a differently colored wrapper. Again, there is nothing wrong with exploratory analysis using null hypothesis testing until you can formulate a better model. But it should not be the <em>only</em> way you approach modeling.</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></div>
<div class="next"><a href="bayesian-vs.-fequentist-statisics.html"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#information-criteria"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="nav-link" href="#deviance"><span class="header-section-number">6.1</span> Deviance</a></li>
<li><a class="nav-link" href="#general-idea-information-criteria-as-miles-per-gallon"><span class="header-section-number">6.2</span> General idea: information criteria as miles-per-gallon</a></li>
<li><a class="nav-link" href="#akaike-information-criterion-aic"><span class="header-section-number">6.3</span> Akaike Information Criterion (AIC)</a></li>
<li><a class="nav-link" href="#bayesian-information-criterion-bic"><span class="header-section-number">6.4</span> Bayesian information criterion (BIC)</a></li>
<li><a class="nav-link" href="#problem-of-aic-and-bic-one-size-may-not-fit-all"><span class="header-section-number">6.5</span> Problem of AIC and BIC: one size may not fit all</a></li>
<li><a class="nav-link" href="#musical-instruments-metaphor"><span class="header-section-number">6.6</span> Musical instruments metaphor</a></li>
<li><a class="nav-link" href="#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><span class="header-section-number">6.7</span> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)</a></li>
<li><a class="nav-link" href="#importance-sampling"><span class="header-section-number">6.8</span> Importance sampling</a></li>
<li><a class="nav-link" href="#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><span class="header-section-number">6.9</span> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)</a></li>
<li><a class="nav-link" href="#bayes-factor"><span class="header-section-number">6.10</span> Bayes Factor</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alexander-pastukhov/notes-on-statistics/blob/master/05-information-criteria.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alexander-pastukhov/notes-on-statistics/edit/master/05-information-criteria.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Notes on Statistics</strong>" was written by Alexander Pastukhov. It was last built on 2022-04-29.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
