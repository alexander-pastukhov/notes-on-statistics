<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Instrumental Variables | Notes on and Solutions for Statistical Rethinking</title>
  <meta name="description" content="Notes and exercise solutions for Statistical Rethinking book." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Instrumental Variables | Notes on and Solutions for Statistical Rethinking" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and exercise solutions for Statistical Rethinking book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Instrumental Variables | Notes on and Solutions for Statistical Rethinking" />
  
  <meta name="twitter:description" content="Notes and exercise solutions for Statistical Rethinking book." />
  

<meta name="author" content="Alexander Pastukhov" />


<meta name="date" content="2021-07-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mixtures.html"/>
<link rel="next" href="parameters-combining-information-from-an-individual-with-population.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes on Statistical Rethinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Precis</a></li>
<li class="chapter" data-level="2" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>2</b> Loss functions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="loss-functions.html"><a href="loss-functions.html#loss-function-the-concept"><i class="fa fa-check"></i><b>2.1</b> Loss function, the concept</a></li>
<li class="chapter" data-level="2.2" data-path="loss-functions.html"><a href="loss-functions.html#l0-mode"><i class="fa fa-check"></i><b>2.2</b> L0 (mode)</a></li>
<li class="chapter" data-level="2.3" data-path="loss-functions.html"><a href="loss-functions.html#l1-median"><i class="fa fa-check"></i><b>2.3</b> L1 (median)</a></li>
<li class="chapter" data-level="2.4" data-path="loss-functions.html"><a href="loss-functions.html#l2-mean"><i class="fa fa-check"></i><b>2.4</b> L2 (mean)</a></li>
<li class="chapter" data-level="2.5" data-path="loss-functions.html"><a href="loss-functions.html#l1-median-vs.-l2-mean"><i class="fa fa-check"></i><b>2.5</b> L1 (median) vs. L2 (mean)</a></li>
<li class="chapter" data-level="2.6" data-path="loss-functions.html"><a href="loss-functions.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.6</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.7" data-path="loss-functions.html"><a href="loss-functions.html#gaussian-in-frenquentist-versus-bayesian-statistics"><i class="fa fa-check"></i><b>2.7</b> Gaussian in frenquentist versus Bayesian statistics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html"><i class="fa fa-check"></i><b>3</b> Directed Acyclic Graphs and Causal Reasoning</a>
<ul>
<li class="chapter" data-level="3.1" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html#peering-into-a-black-box"><i class="fa fa-check"></i><b>3.1</b> Peering into a black box</a></li>
<li class="chapter" data-level="3.2" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html#turning-unconditional-dependence-into-conditional-independence"><i class="fa fa-check"></i><b>3.2</b> Turning unconditional dependence into conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collider-bias.html"><a href="collider-bias.html"><i class="fa fa-check"></i><b>4</b> Collider bias</a>
<ul>
<li class="chapter" data-level="4.1" data-path="collider-bias.html"><a href="collider-bias.html#multicollinearity"><i class="fa fa-check"></i><b>4.1</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.2" data-path="collider-bias.html"><a href="collider-bias.html#back-to-spurious-association"><i class="fa fa-check"></i><b>4.2</b> Back to spurious association</a></li>
<li class="chapter" data-level="4.3" data-path="collider-bias.html"><a href="collider-bias.html#chain-dag"><i class="fa fa-check"></i><b>4.3</b> Chain DAG</a></li>
<li class="chapter" data-level="4.4" data-path="collider-bias.html"><a href="collider-bias.html#take-home-message"><i class="fa fa-check"></i><b>4.4</b> Take-home message</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-haunted-dag.html"><a href="the-haunted-dag.html"><i class="fa fa-check"></i><b>5</b> The haunted DAG</a></li>
<li class="chapter" data-level="6" data-path="information-criteria.html"><a href="information-criteria.html"><i class="fa fa-check"></i><b>6</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="6.1" data-path="information-criteria.html"><a href="information-criteria.html#deviance"><i class="fa fa-check"></i><b>6.1</b> Deviance</a></li>
<li class="chapter" data-level="6.2" data-path="information-criteria.html"><a href="information-criteria.html#general-idea-information-criteria-as-miles-per-gallon"><i class="fa fa-check"></i><b>6.2</b> General idea: information criteria as miles-per-gallon</a></li>
<li class="chapter" data-level="6.3" data-path="information-criteria.html"><a href="information-criteria.html#akaike-information-criterion-aic"><i class="fa fa-check"></i><b>6.3</b> Akaike Information Criterion (AIC)</a></li>
<li class="chapter" data-level="6.4" data-path="information-criteria.html"><a href="information-criteria.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>6.4</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="6.5" data-path="information-criteria.html"><a href="information-criteria.html#problem-of-aic-and-bic-one-size-may-not-fit-all"><i class="fa fa-check"></i><b>6.5</b> Problem of AIC and BIC: one size may not fit all</a></li>
<li class="chapter" data-level="6.6" data-path="information-criteria.html"><a href="information-criteria.html#musical-instruments-metaphor"><i class="fa fa-check"></i><b>6.6</b> Musical instruments metaphor</a></li>
<li class="chapter" data-level="6.7" data-path="information-criteria.html"><a href="information-criteria.html#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><i class="fa fa-check"></i><b>6.7</b> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)</a></li>
<li class="chapter" data-level="6.8" data-path="information-criteria.html"><a href="information-criteria.html#importance-sampling"><i class="fa fa-check"></i><b>6.8</b> Importance sampling</a></li>
<li class="chapter" data-level="6.9" data-path="information-criteria.html"><a href="information-criteria.html#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><i class="fa fa-check"></i><b>6.9</b> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)</a></li>
<li class="chapter" data-level="6.10" data-path="information-criteria.html"><a href="information-criteria.html#bayes-factor"><i class="fa fa-check"></i><b>6.10</b> Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html"><i class="fa fa-check"></i><b>7</b> Bayesian versus Frequentist Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#choice-of-likelihood-both"><i class="fa fa-check"></i><b>7.1</b> Choice of likelihood (both)</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#linear-model-both"><i class="fa fa-check"></i><b>7.2</b> Linear model (both)</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#priors-optional-for-bayesian"><i class="fa fa-check"></i><b>7.3</b> Priors (optional for Bayesian)</a></li>
<li class="chapter" data-level="7.4" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#maximum-likelihood-estimate-both"><i class="fa fa-check"></i><b>7.4</b> Maximum-likelihood estimate (both)</a></li>
<li class="chapter" data-level="7.5" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#uncertainty-about-estimates-different-but-comparable"><i class="fa fa-check"></i><b>7.5</b> Uncertainty about estimates (different but comparable)</a></li>
<li class="chapter" data-level="7.6" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#model-comparison-via-information-criteria-both"><i class="fa fa-check"></i><b>7.6</b> Model comparison via information criteria (both)</a></li>
<li class="chapter" data-level="7.7" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#generating-predictions-both"><i class="fa fa-check"></i><b>7.7</b> Generating predictions (both)</a></li>
<li class="chapter" data-level="7.8" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#conclusions"><i class="fa fa-check"></i><b>7.8</b> Conclusions</a></li>
<li class="chapter" data-level="7.9" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#take-home-message-1"><i class="fa fa-check"></i><b>7.9</b> Take home message</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixtures.html"><a href="mixtures.html"><i class="fa fa-check"></i><b>8</b> Mixtures</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mixtures.html"><a href="mixtures.html#beta-binomial"><i class="fa fa-check"></i><b>8.1</b> Beta Binomial</a></li>
<li class="chapter" data-level="8.2" data-path="mixtures.html"><a href="mixtures.html#negative-binomial-a.k.a.-gamma-poisson"><i class="fa fa-check"></i><b>8.2</b> Negative binomial, a.k.a. Gamma Poisson</a></li>
<li class="chapter" data-level="8.3" data-path="mixtures.html"><a href="mixtures.html#ordered-categorical"><i class="fa fa-check"></i><b>8.3</b> Ordered categorical</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="instrumental-variables.html"><a href="instrumental-variables.html"><i class="fa fa-check"></i><b>9</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#disclaimer"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#can-we-estimate-an-effect-of-military-experience-on-wages"><i class="fa fa-check"></i>Can we estimate an effect of military experience on wages</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#draft-as-an-instrumental-variable"><i class="fa fa-check"></i>Draft as an instrumental variable</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#two-stage-least-squares"><i class="fa fa-check"></i>Two-stage least squares</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#covarying-residuals"><i class="fa fa-check"></i>Covarying residuals</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html"><i class="fa fa-check"></i><b>10</b> Parameters: combining information from an individual with population</a>
<ul>
<li class="chapter" data-level="10.1" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#everyone-is-the-same-single-parameter"><i class="fa fa-check"></i><b>10.1</b> Everyone is the same (single parameter)</a></li>
<li class="chapter" data-level="10.2" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#everyone-is-unique-independent-parameters"><i class="fa fa-check"></i><b>10.2</b> Everyone is unique (independent parameters)</a></li>
<li class="chapter" data-level="10.3" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-different-but-belong-to-a-population-pooled-parameters"><i class="fa fa-check"></i><b>10.3</b> People are different but belong to a population (pooled parameters)</a></li>
<li class="chapter" data-level="10.4" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-different-but-belong-to-a-group-within-a-population-multilevel-clusters-of-pooled-parameters"><i class="fa fa-check"></i><b>10.4</b> People are different but belong to a group within a population (multilevel clusters of pooled parameters)</a></li>
<li class="chapter" data-level="10.5" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-similar-to-some-but-different-to-others-gaussian-process"><i class="fa fa-check"></i><b>10.5</b> People are similar to some but different to others (Gaussian process)</a></li>
<li class="chapter" data-level="10.6" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-different-but-belong-to-a-population-in-which-parameters-are-correlated-correlated-pooled-parameters"><i class="fa fa-check"></i><b>10.6</b> People are different but belong to a population in which parameters are correlated (correlated pooled parameters)</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="incorporating-measurement-error-a-rubber-band-metaphor.html"><a href="incorporating-measurement-error-a-rubber-band-metaphor.html"><i class="fa fa-check"></i><b>11</b> Incorporating measurement error: a rubber band metaphor</a></li>
<li class="divider"></li>
<li><a href="https://alexander-pastukhov.github.io/">Alexander (Sasha) Pastukhov</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on and Solutions for Statistical Rethinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="instrumental-variables" class="section level1" number="9">
<h1><span class="header-section-number">Chapter 9</span> Instrumental Variables</h1>
<div id="disclaimer" class="section level2 unnumbered">
<h2>Disclaimer</h2>
<p>My understanding is that instrumental variables are unicorns. They are immensely powerful and can magically turn an observational study into, effectively, a randomize experiment enabling you to infer causality. But they are rare, perhaps, even rarer than unicorns themselves, which is why most example you find are based on the same (or very similar) data.They are also often described as a source of “natural randomization,” yet, the best examples I have found (effect of military experience on career/wages, effect of studying in a charter school on math skills) involved <em>deliberate</em> randomization in a form of lottery that you can conveniently use.</p>
</div>
<div id="can-we-estimate-an-effect-of-military-experience-on-wages" class="section level2 unnumbered">
<h2>Can we estimate an effect of military experience on wages</h2>
<p>One of the examples, which you can find online, is an effect of military experience on wages. Conceptually, everything is simple, you just want to know a direct effect of military experience on wages.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>However, in practice both a decision to join military <em>and</em> choices of career can be cause by the same set of unobserved factors. If you come from a family with military background, this will affect both your decision join military and the kind of career you want to pursue. If you are patriotic and want to serve the country, you might opt for both military and lower-earning career in public sector. Conversely, if you are interested in becoming a successful lawyer, going into military might be more of a hindrance than of help. In short, we have a backdoor path through unobserved variables and we have no easy way to close it.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-32-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="draft-as-an-instrumental-variable" class="section level2 unnumbered">
<h2>Draft as an instrumental variable</h2>
<p>If we would be able to perform a <em>randomized</em> experiment, things would be easy: Take similar people and randomly send / not send them to military, observe the effect of this on their career and wages. Now, <em>we</em> cannot do it but, perhaps, we can find a situation it was actually done and use it for our advantage. This makes the whole situation a sort of a hybrid: we <em>observe</em> an outcome of <em>randomization</em> that someone else did for their own purposes</p>
<p>In this case, during the Vietnam war people were drafted based on their assigned draft numbers. The latter were determined by your date of birth but the relationship between the birth day within a year and the draft number was <a href="https://en.wikipedia.org/wiki/Draft_lottery_(1969)">randomized through the lottery</a>. In addition, the order <em>within</em> a day was randomized through lottery of initials. The purpose was to create a perfect randomization where any background (apart from you being male and being eligible for military service) had no effect on whether or not your were drafted. And, although the lottery was created to address inequalities, it created an almost perfect randomized experiment for us to use. Almost, because some people with early numbers avoided being drafted through various means, whereas some people with later draft numbers volunteered and were enlisted.</p>
<p>The DAG it creates is the following</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-33-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As you can see, we are still not able to close the backdoor path between <em>military experience</em> and <em>wages</em> because the randomization is not perfect. You will see how we can still overcome this problem using two-stage least squares (the default approach you find almost everywhere, referred to as “two-stage worst squares” by McElreath) and using covariance of residuals (as in the book).</p>
</div>
<div id="two-stage-least-squares" class="section level2 unnumbered">
<h2>Two-stage least squares</h2>
<p>The main idea is two split our DAG into two halves and estimated the effects one by one, hence, “two-stage.” First, use draft number to <em>predict</em> the military experience.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The linear model is
<span class="math display">\[ME \sim Normal(\widehat{ME}, \sigma_{ME})\\
\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN\]</span></p>
<p>where <span class="math inline">\(\alpha_M\)</span> is an arbitrary intercept term, <span class="math inline">\(\beta_{DN}\)</span> encode the main effect of draft number, and <span class="math inline">\(\sigma\)</span> is the standard deviation of residuals. You can also write <em>the same</em> model in a different format, which you are likely to encounter in other tutorials and lectures:
<span class="math display">\[\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN + \epsilon_{ME}\]</span>
Here, the difference between the <em>predicted</em> military experience (note the hat above <span class="math inline">\(\widehat{ME}\)</span>) and the actual one is described by <span class="math inline">\(\epsilon_{ME}\)</span>: residuals that come from a normal distribution that is centered at 0 with standard deviation of <span class="math inline">\(sigma_{ME}\)</span>, which is implicit in this format. Note that <em>both</em> versions express same variables and encode the same dependence but differ in whether the residuals (<span class="math inline">\(epsilon\)</span>) or the standard deviation of the distribution they come from (<span class="math inline">\(sigma\)</span>) are implicit or explicit.</p>
<p>As draft number is independent from unobserved variables due to the lottery, the <em>predicted</em> military experience is <em>also</em> independent from them. All the dependence gets transferred to the <em>residuals</em> instead, because residuals are variance that our <em>signal</em> cannot explain (that point will become important later on).</p>
<p>Now, for the second stage, we use <em>predicted</em> military experience, as it is independent of the unobserved variables and, therefore, we do not need to close the backdoor path!</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The model is, again, simple but do note that in two-stage least squares we are using <em>predicted</em> military experience <span class="math inline">\(\widehat{ME}\)</span>!
<span class="math display">\[CW \sim Normal(\widehat{CW}, \sigma_{CW})\\
\widehat{CW} = \alpha_{CW} + \beta_{ME} \cdot \widehat{CW}\\
or\\
\widehat{CW} = \alpha_{CW} + \beta_{ME} \cdot \widehat{ME} + \epsilon_{CW}
\]</span></p>
<p>Again, all the dependence between unobserved variables and career/wages gets offloaded into the residuals <span class="math inline">\(\epsilon_{CW}\)</span> and you get you nice and clean <em>direct</em> effect of <em>predicted</em> military experience. Importantly, for you interpretation you <em>equate</em> the effect of the predicted military experience with an effect of an actual one. This is, of course, one big elephant in the room because you need to be certain that <span class="math inline">\(\widehat{ME}\)</span> is indeed accurate. Unfortunately, if it is not, your inferences are incorrect and you are back to square one.</p>
</div>
<div id="covarying-residuals" class="section level2 unnumbered">
<h2>Covarying residuals</h2>
<p>This is the approach presented in the book that allows you to use <em>actual</em> military experience as a predictor and still solve the problem of the backdoor path. The initial idea is the same: Let us use our instrumental variable to compute a “pure” military experience, uncontaminated by unobserved variables.</p>
<p><span class="math display">\[\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN + \epsilon_{ME}\]</span></p>
<p>Again, all the variance due to unobserved variables is accumulated by residuals (<span class="math inline">\(\epsilon_{ME}\)</span>), which are <em>correlated with them</em>. The latter part is trivially true because residuals are <em>always</em> correlated with “noise,” precisely because we defined it as influence of unobserved variables we did not measure and, therefore, cannot explicitly account for. If we could measure and include them, we would observe <em>same correlation</em> explicitly. Alas, we did not measure them and, therefore, in our typical model the fact that residuals are correlated with unobserved variables is of no immediate practical value (beyond checking for their exchangeability).</p>
<p>Now, imagine that we <em>could</em> compute not just a prediction from our first stage but a <em>pure randomized</em> military experience that is not correlated with unobserved variables. Obviously, we cannot, otherwise we would not worry about the backdoor path, but what do we know about a linear model that includes that magic <em>pure randomized</em> military experience as a predictor? That its <em>true</em> direct effect on career/wages will be <em>independent</em> from unobserved variables and, therefore, all their influence will be offloaded into residuals (<span class="math inline">\(\epsilon_{\widehat{CW}}\)</span>, note the hat that differentiates them the residuals that you get if you use observed military experience), which will be correlated with those unobserved variables. Just like the residuals of our <em>first stage model</em> (<span class="math inline">\(\epsilon_{ME}\)</span>)! But if both sets of residuals are correlated with unobserved variables (<span class="math inline">\(\epsilon_{ME} \propto UV\)</span> and <span class="math inline">\(\epsilon_{\widehat{CW}} \propto UV\)</span>), the two sets of residuals are <em>also</em> correlated: <span class="math inline">\(\epsilon_{ME} \propto \epsilon_{\widehat{CW}}\)</span>.</p>
<p>These correlated residuals sound wonderfully entertaining but why should we care about them given that <span class="math inline">\(\epsilon_{\widehat{CW}}\)</span> are <em>hypothetical</em> residuals given the <em>hypothetical</em> randomized military experience that we do not have access to? Because we can make them real by using their covariance with <span class="math inline">\(\epsilon_{ME}\)</span> that we <em>can</em> compute! Here is the idea: let us use two simple linear models to predict military experience from draft number and career/wages from <em>actual</em> military experience but allow the residuals of two models to be correlated. Again, given that <span class="math inline">\(\epsilon_{ME} \propto UV\)</span>, if we make <span class="math inline">\(\epsilon_{ME} \propto \epsilon_{CW}\)</span>, therefore, <span class="math inline">\(\epsilon_{CW} \propto UV\)</span>. In other words, <span class="math inline">\(\epsilon_{CW} \approx \epsilon_{\widehat{CW}}\)</span>. But this means that since <span class="math inline">\(\epsilon_{CW}\)</span> accounts for the variance due to unobserved variables, it must not be accounted for by <em>other</em> terms of the linear model and our effect of military experience (<span class="math inline">\(\beta^{true}_{ME}\)</span>) expresses just the direct path:
<span class="math display">\[\widehat{CW} = \alpha_{CW} + \beta^{true}_{ME} \cdot ME + \epsilon_{\widehat{CW}}\]</span></p>
<p>This is a very elegant trick: We do not know what the correlation between residuals and unobserved variables is but we do know that it must be the same (very similar) and we account for it by enforcing the correlation between the residuals that we deduced must exist. On the one hand, there is an obvious elephant in the room in that the whole thing works well only if the assumption that residuals are correlated is true. On the other hand, because we are fitting the covariance matrix as part of the model, we can check whether our assumption about that correlation was supported by data. If it was not, our instrumental variable was probably not as good as hoped for.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mixtures.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="parameters-combining-information-from-an-individual-with-population.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes-on-statistical-rethinking.pdf", "notes-on-statistical-rethinking.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
