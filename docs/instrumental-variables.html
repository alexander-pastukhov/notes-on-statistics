<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Instrumental Variables | Notes on Statistics</title>
<meta name="author" content="Alexander Pastukhov">
<meta name="description" content="Disclaimer My understanding is that instrumental variables are unicorns. They are immensely powerful and can magically turn an observational study into, effectively, a randomize experiment...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="Chapter 8 Instrumental Variables | Notes on Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="Disclaimer My understanding is that instrumental variables are unicorns. They are immensely powerful and can magically turn an observational study into, effectively, a randomize experiment...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Instrumental Variables | Notes on Statistics">
<meta name="twitter:description" content="Disclaimer My understanding is that instrumental variables are unicorns. They are immensely powerful and can magically turn an observational study into, effectively, a randomize experiment...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="bs4_style.css%20-%20style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Notes on Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Precis</a></li>
<li><a class="" href="loss-functions.html"><span class="header-section-number">2</span> Loss functions</a></li>
<li><a class="" href="directed-acyclic-graphs-and-causal-reasoning.html"><span class="header-section-number">3</span> Directed Acyclic Graphs and Causal Reasoning</a></li>
<li><a class="" href="collider-bias.html"><span class="header-section-number">4</span> Collider bias</a></li>
<li><a class="" href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></li>
<li><a class="" href="information-criteria.html"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="" href="mixtures.html"><span class="header-section-number">7</span> Mixtures</a></li>
<li><a class="active" href="instrumental-variables.html"><span class="header-section-number">8</span> Instrumental Variables</a></li>
<li><a class="" href="parameters-combining-information-from-an-individual-with-population.html"><span class="header-section-number">9</span> Parameters: combining information from an individual with population</a></li>
<li><a class="" href="incorporating-measurement-error-a-rubber-band-metaphor.html"><span class="header-section-number">10</span> Incorporating measurement error: a rubber band metaphor</a></li>
<li><a class="" href="generalized-additive-models-as-continuous-random-effects.html"><span class="header-section-number">11</span> Generalized Additive Models as continuous random effects</a></li>
<li><a class="" href="flat-priors-the-strings-attached.html"><span class="header-section-number">12</span> Flat priors: the strings attached</a></li>
<li><a class="" href="unbiased-mean-versus-biased-variance-in-plain-english.html"><span class="header-section-number">13</span> Unbiased mean versus biased variance in plain English</a></li>
<li><a class="" href="probability-mass-versus-probability-density.html"><span class="header-section-number">14</span> Probability mass versus probability density</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alexander-pastukhov/notes-on-statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="instrumental-variables" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Instrumental Variables<a class="anchor" aria-label="anchor" href="#instrumental-variables"><i class="fas fa-link"></i></a>
</h1>
<div id="disclaimer" class="section level2 unnumbered">
<h2>Disclaimer<a class="anchor" aria-label="anchor" href="#disclaimer"><i class="fas fa-link"></i></a>
</h2>
<p>My understanding is that instrumental variables are unicorns. They are immensely powerful and can magically turn an observational study into, effectively, a randomize experiment enabling you to infer causality. But they are rare, perhaps, even rarer than unicorns themselves, which is why most example you find are based on the same (or very similar) data.They are also often described as a source of “natural randomization,” yet, the best examples I have found (effect of military experience on career/wages, effect of studying in a charter school on math skills) involved <em>deliberate</em> randomization in a form of lottery that you can conveniently use.</p>
</div>
<div id="can-we-estimate-an-effect-of-military-experience-on-wages" class="section level2 unnumbered">
<h2>Can we estimate an effect of military experience on wages<a class="anchor" aria-label="anchor" href="#can-we-estimate-an-effect-of-military-experience-on-wages"><i class="fas fa-link"></i></a>
</h2>
<p>One of the examples, which you can find online, is an effect of military experience on wages. Conceptually, everything is simple, you just want to know a direct effect of military experience on wages.</p>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;"></div>
<p>However, in practice both a decision to join military <em>and</em> choices of career can be cause by the same set of unobserved factors. If you come from a family with military background, this will affect both your decision join military and the kind of career you want to pursue. If you are patriotic and want to serve the country, you might opt for both military and lower-earning career in public sector. Conversely, if you are interested in becoming a successful lawyer, going into military might be more of a hindrance than of help. In short, we have a backdoor path through unobserved variables and we have no easy way to close it.</p>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-32-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="draft-as-an-instrumental-variable" class="section level2 unnumbered">
<h2>Draft as an instrumental variable<a class="anchor" aria-label="anchor" href="#draft-as-an-instrumental-variable"><i class="fas fa-link"></i></a>
</h2>
<p>If we would be able to perform a <em>randomized</em> experiment, things would be easy: Take similar people and randomly send / not send them to military, observe the effect of this on their career and wages. Now, <em>we</em> cannot do it but, perhaps, we can find a situation it was actually done and use it for our advantage. This makes the whole situation a sort of a hybrid: we <em>observe</em> an outcome of <em>randomization</em> that someone else did for their own purposes</p>
<p>In this case, during the Vietnam war people were drafted based on their assigned draft numbers. The latter were determined by your date of birth but the relationship between the birth day within a year and the draft number was <a href="https://en.wikipedia.org/wiki/Draft_lottery_(1969)">randomized through the lottery</a>. In addition, the order <em>within</em> a day was randomized through lottery of initials. The purpose was to create a perfect randomization where any background (apart from you being male and being eligible for military service) had no effect on whether or not your were drafted. And, although the lottery was created to address inequalities, it created an almost perfect randomized experiment for us to use. Almost, because some people with early numbers avoided being drafted through various means, whereas some people with later draft numbers volunteered and were enlisted.</p>
<p>The DAG it creates is the following</p>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-33-1.png" width="672" style="display: block; margin: auto;"></div>
<p>As you can see, we are still not able to close the backdoor path between <em>military experience</em> and <em>wages</em> because the randomization is not perfect. You will see how we can still overcome this problem using two-stage least squares (the default approach you find almost everywhere, referred to as “two-stage worst squares” by McElreath) and using covariance of residuals (as in the book).</p>
</div>
<div id="two-stage-least-squares" class="section level2 unnumbered">
<h2>Two-stage least squares<a class="anchor" aria-label="anchor" href="#two-stage-least-squares"><i class="fas fa-link"></i></a>
</h2>
<p>The main idea is two split our DAG into two halves and estimated the effects one by one, hence, “two-stage.” First, use draft number to <em>predict</em> the military experience.</p>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;"></div>
<p>The linear model is
<span class="math display">\[ME \sim Normal(\widehat{ME}, \sigma_{ME})\\
\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN\]</span></p>
<p>where <span class="math inline">\(\alpha_M\)</span> is an arbitrary intercept term, <span class="math inline">\(\beta_{DN}\)</span> encode the main effect of draft number, and <span class="math inline">\(\sigma\)</span> is the standard deviation of residuals. You can also write <em>the same</em> model in a different format, which you are likely to encounter in other tutorials and lectures:
<span class="math display">\[\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN + \epsilon_{ME}\]</span>
Here, the difference between the <em>predicted</em> military experience (note the hat above <span class="math inline">\(\widehat{ME}\)</span>) and the actual one is described by <span class="math inline">\(\epsilon_{ME}\)</span>: residuals that come from a normal distribution that is centered at 0 with standard deviation of <span class="math inline">\(sigma_{ME}\)</span>, which is implicit in this format. Note that <em>both</em> versions express same variables and encode the same dependence but differ in whether the residuals (<span class="math inline">\(epsilon\)</span>) or the standard deviation of the distribution they come from (<span class="math inline">\(sigma\)</span>) are implicit or explicit.</p>
<p>As draft number is independent from unobserved variables due to the lottery, the <em>predicted</em> military experience is <em>also</em> independent from them. All the dependence gets transferred to the <em>residuals</em> instead, because residuals are variance that our <em>signal</em> cannot explain (that point will become important later on).</p>
<p>Now, for the second stage, we use <em>predicted</em> military experience, as it is independent of the unobserved variables and, therefore, we do not need to close the backdoor path!</p>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;"></div>
<p>The model is, again, simple but do note that in two-stage least squares we are using <em>predicted</em> military experience <span class="math inline">\(\widehat{ME}\)</span>!
<span class="math display">\[CW \sim Normal(\widehat{CW}, \sigma_{CW})\\
\widehat{CW} = \alpha_{CW} + \beta_{ME} \cdot \widehat{CW}\\
or\\
\widehat{CW} = \alpha_{CW} + \beta_{ME} \cdot \widehat{ME} + \epsilon_{CW}
\]</span></p>
<p>Again, all the dependence between unobserved variables and career/wages gets offloaded into the residuals <span class="math inline">\(\epsilon_{CW}\)</span> and you get you nice and clean <em>direct</em> effect of <em>predicted</em> military experience. Importantly, for you interpretation you <em>equate</em> the effect of the predicted military experience with an effect of an actual one. This is, of course, one big elephant in the room because you need to be certain that <span class="math inline">\(\widehat{ME}\)</span> is indeed accurate. Unfortunately, if it is not, your inferences are incorrect and you are back to square one.</p>
</div>
<div id="covarying-residuals" class="section level2 unnumbered">
<h2>Covarying residuals<a class="anchor" aria-label="anchor" href="#covarying-residuals"><i class="fas fa-link"></i></a>
</h2>
<p>This is the approach presented in the book that allows you to use <em>actual</em> military experience as a predictor and still solve the problem of the backdoor path. The initial idea is the same: Let us use our instrumental variable to compute a “pure” military experience, uncontaminated by unobserved variables.</p>
<p><span class="math display">\[\widehat{ME} = \alpha_{ME} + \beta_{DN} \cdot DN + \epsilon_{ME}\]</span></p>
<p>Again, all the variance due to unobserved variables is accumulated by residuals (<span class="math inline">\(\epsilon_{ME}\)</span>), which are <em>correlated with them</em>. The latter part is trivially true because residuals are <em>always</em> correlated with “noise,” precisely because we defined it as influence of unobserved variables we did not measure and, therefore, cannot explicitly account for. If we could measure and include them, we would observe <em>same correlation</em> explicitly. Alas, we did not measure them and, therefore, in our typical model the fact that residuals are correlated with unobserved variables is of no immediate practical value (beyond checking for their exchangeability).</p>
<p>Now, imagine that we <em>could</em> compute not just a prediction from our first stage but a <em>pure randomized</em> military experience that is not correlated with unobserved variables. Obviously, we cannot, otherwise we would not worry about the backdoor path, but what do we know about a linear model that includes that magic <em>pure randomized</em> military experience as a predictor? That its <em>true</em> direct effect on career/wages will be <em>independent</em> from unobserved variables and, therefore, all their influence will be offloaded into residuals (<span class="math inline">\(\epsilon_{\widehat{CW}}\)</span>, note the hat that differentiates them the residuals that you get if you use observed military experience), which will be correlated with those unobserved variables. Just like the residuals of our <em>first stage model</em> (<span class="math inline">\(\epsilon_{ME}\)</span>)! But if both sets of residuals are correlated with unobserved variables (<span class="math inline">\(\epsilon_{ME} \propto UV\)</span> and <span class="math inline">\(\epsilon_{\widehat{CW}} \propto UV\)</span>), the two sets of residuals are <em>also</em> correlated: <span class="math inline">\(\epsilon_{ME} \propto \epsilon_{\widehat{CW}}\)</span>.</p>
<p>These correlated residuals sound wonderfully entertaining but why should we care about them given that <span class="math inline">\(\epsilon_{\widehat{CW}}\)</span> are <em>hypothetical</em> residuals given the <em>hypothetical</em> randomized military experience that we do not have access to? Because we can make them real by using their covariance with <span class="math inline">\(\epsilon_{ME}\)</span> that we <em>can</em> compute! Here is the idea: let us use two simple linear models to predict military experience from draft number and career/wages from <em>actual</em> military experience but allow the residuals of two models to be correlated. Again, given that <span class="math inline">\(\epsilon_{ME} \propto UV\)</span>, if we make <span class="math inline">\(\epsilon_{ME} \propto \epsilon_{CW}\)</span>, therefore, <span class="math inline">\(\epsilon_{CW} \propto UV\)</span>. In other words, <span class="math inline">\(\epsilon_{CW} \approx \epsilon_{\widehat{CW}}\)</span>. But this means that since <span class="math inline">\(\epsilon_{CW}\)</span> accounts for the variance due to unobserved variables, it must not be accounted for by <em>other</em> terms of the linear model and our effect of military experience (<span class="math inline">\(\beta^{true}_{ME}\)</span>) expresses just the direct path:
<span class="math display">\[\widehat{CW} = \alpha_{CW} + \beta^{true}_{ME} \cdot ME + \epsilon_{\widehat{CW}}\]</span></p>
<p>This is a very elegant trick: We do not know what the correlation between residuals and unobserved variables is but we do know that it must be the same (very similar) and we account for it by enforcing the correlation between the residuals that we deduced must exist. On the one hand, there is an obvious elephant in the room in that the whole thing works well only if the assumption that residuals are correlated is true. On the other hand, because we are fitting the covariance matrix as part of the model, we can check whether our assumption about that correlation was supported by data. If it was not, our instrumental variable was probably not as good as hoped for.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="mixtures.html"><span class="header-section-number">7</span> Mixtures</a></div>
<div class="next"><a href="parameters-combining-information-from-an-individual-with-population.html"><span class="header-section-number">9</span> Parameters: combining information from an individual with population</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#instrumental-variables"><span class="header-section-number">8</span> Instrumental Variables</a></li>
<li><a class="nav-link" href="#disclaimer">Disclaimer</a></li>
<li><a class="nav-link" href="#can-we-estimate-an-effect-of-military-experience-on-wages">Can we estimate an effect of military experience on wages</a></li>
<li><a class="nav-link" href="#draft-as-an-instrumental-variable">Draft as an instrumental variable</a></li>
<li><a class="nav-link" href="#two-stage-least-squares">Two-stage least squares</a></li>
<li><a class="nav-link" href="#covarying-residuals">Covarying residuals</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alexander-pastukhov/notes-on-statistics/blob/master/08-instrumental-variables.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alexander-pastukhov/notes-on-statistics/edit/master/08-instrumental-variables.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Notes on Statistics</strong>" was written by Alexander Pastukhov. It was last built on 2021-11-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
