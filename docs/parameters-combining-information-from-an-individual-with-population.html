<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Parameters: combining information from an individual with population | Notes on and Solutions for Statistical Rethinking</title>
  <meta name="description" content="Notes and exercise solutions for Statistical Rethinking book." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Parameters: combining information from an individual with population | Notes on and Solutions for Statistical Rethinking" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Notes and exercise solutions for Statistical Rethinking book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Parameters: combining information from an individual with population | Notes on and Solutions for Statistical Rethinking" />
  
  <meta name="twitter:description" content="Notes and exercise solutions for Statistical Rethinking book." />
  

<meta name="author" content="Alexander Pastukhov" />


<meta name="date" content="2021-06-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="instrumental-variables.html"/>

<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes on Statistical Rethinking</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Precis</a></li>
<li class="chapter" data-level="2" data-path="loss-functions.html"><a href="loss-functions.html"><i class="fa fa-check"></i><b>2</b> Loss functions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="loss-functions.html"><a href="loss-functions.html#loss-function-the-concept"><i class="fa fa-check"></i><b>2.1</b> Loss function, the concept</a></li>
<li class="chapter" data-level="2.2" data-path="loss-functions.html"><a href="loss-functions.html#l0-mode"><i class="fa fa-check"></i><b>2.2</b> L0 (mode)</a></li>
<li class="chapter" data-level="2.3" data-path="loss-functions.html"><a href="loss-functions.html#l1-median"><i class="fa fa-check"></i><b>2.3</b> L1 (median)</a></li>
<li class="chapter" data-level="2.4" data-path="loss-functions.html"><a href="loss-functions.html#l2-mean"><i class="fa fa-check"></i><b>2.4</b> L2 (mean)</a></li>
<li class="chapter" data-level="2.5" data-path="loss-functions.html"><a href="loss-functions.html#l1-median-vs.-l2-mean"><i class="fa fa-check"></i><b>2.5</b> L1 (median) vs. L2 (mean)</a></li>
<li class="chapter" data-level="2.6" data-path="loss-functions.html"><a href="loss-functions.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>2.6</b> Choosing a likelihood</a></li>
<li class="chapter" data-level="2.7" data-path="loss-functions.html"><a href="loss-functions.html#gaussian-in-frenquentist-versus-bayesian-statistics"><i class="fa fa-check"></i><b>2.7</b> Gaussian in frenquentist versus Bayesian statistics</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html"><i class="fa fa-check"></i><b>3</b> Directed Acyclic Graphs and Causal Reasoning</a>
<ul>
<li class="chapter" data-level="3.1" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html#peering-into-a-black-box"><i class="fa fa-check"></i><b>3.1</b> Peering into a black box</a></li>
<li class="chapter" data-level="3.2" data-path="directed-acyclic-graphs-and-causal-reasoning.html"><a href="directed-acyclic-graphs-and-causal-reasoning.html#turning-unconditional-dependence-into-conditional-independence"><i class="fa fa-check"></i><b>3.2</b> Turning unconditional dependence into conditional independence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="collider-bias.html"><a href="collider-bias.html"><i class="fa fa-check"></i><b>4</b> Collider bias</a>
<ul>
<li class="chapter" data-level="4.1" data-path="collider-bias.html"><a href="collider-bias.html#multicollinearity"><i class="fa fa-check"></i><b>4.1</b> Multicollinearity</a></li>
<li class="chapter" data-level="4.2" data-path="collider-bias.html"><a href="collider-bias.html#back-to-spurious-association"><i class="fa fa-check"></i><b>4.2</b> Back to spurious association</a></li>
<li class="chapter" data-level="4.3" data-path="collider-bias.html"><a href="collider-bias.html#chain-dag"><i class="fa fa-check"></i><b>4.3</b> Chain DAG</a></li>
<li class="chapter" data-level="4.4" data-path="collider-bias.html"><a href="collider-bias.html#take-home-message"><i class="fa fa-check"></i><b>4.4</b> Take-home message</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-haunted-dag.html"><a href="the-haunted-dag.html"><i class="fa fa-check"></i><b>5</b> The haunted DAG</a></li>
<li class="chapter" data-level="6" data-path="information-criteria.html"><a href="information-criteria.html"><i class="fa fa-check"></i><b>6</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="6.1" data-path="information-criteria.html"><a href="information-criteria.html#deviance"><i class="fa fa-check"></i><b>6.1</b> Deviance</a></li>
<li class="chapter" data-level="6.2" data-path="information-criteria.html"><a href="information-criteria.html#general-idea-information-criteria-as-miles-per-gallon"><i class="fa fa-check"></i><b>6.2</b> General idea: information criteria as miles-per-gallon</a></li>
<li class="chapter" data-level="6.3" data-path="information-criteria.html"><a href="information-criteria.html#akaike-information-criterion-aic"><i class="fa fa-check"></i><b>6.3</b> Akaike Information Criterion (AIC)</a></li>
<li class="chapter" data-level="6.4" data-path="information-criteria.html"><a href="information-criteria.html#bayesian-information-criterion-bic"><i class="fa fa-check"></i><b>6.4</b> Bayesian information criterion (BIC)</a></li>
<li class="chapter" data-level="6.5" data-path="information-criteria.html"><a href="information-criteria.html#problem-of-aic-and-bic-one-size-may-not-fit-all"><i class="fa fa-check"></i><b>6.5</b> Problem of AIC and BIC: one size may not fit all</a></li>
<li class="chapter" data-level="6.6" data-path="information-criteria.html"><a href="information-criteria.html#musical-instruments-metaphor"><i class="fa fa-check"></i><b>6.6</b> Musical instruments metaphor</a></li>
<li class="chapter" data-level="6.7" data-path="information-criteria.html"><a href="information-criteria.html#deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic"><i class="fa fa-check"></i><b>6.7</b> Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)</a></li>
<li class="chapter" data-level="6.8" data-path="information-criteria.html"><a href="information-criteria.html#importance-sampling"><i class="fa fa-check"></i><b>6.8</b> Importance sampling</a></li>
<li class="chapter" data-level="6.9" data-path="information-criteria.html"><a href="information-criteria.html#pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo"><i class="fa fa-check"></i><b>6.9</b> Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)</a></li>
<li class="chapter" data-level="6.10" data-path="information-criteria.html"><a href="information-criteria.html#bayes-factor"><i class="fa fa-check"></i><b>6.10</b> Bayes Factor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html"><i class="fa fa-check"></i><b>7</b> Bayesian versus Frequentist Statistics</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#choice-of-likelihood-both"><i class="fa fa-check"></i><b>7.1</b> Choice of likelihood (both)</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#linear-model-both"><i class="fa fa-check"></i><b>7.2</b> Linear model (both)</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#priors-optional-for-bayesian"><i class="fa fa-check"></i><b>7.3</b> Priors (optional for Bayesian)</a></li>
<li class="chapter" data-level="7.4" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#maximum-likelihood-estimate-both"><i class="fa fa-check"></i><b>7.4</b> Maximum-likelihood estimate (both)</a></li>
<li class="chapter" data-level="7.5" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#uncertainty-about-estimates-different-but-comparable"><i class="fa fa-check"></i><b>7.5</b> Uncertainty about estimates (different but comparable)</a></li>
<li class="chapter" data-level="7.6" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#model-comparison-via-information-criteria-both"><i class="fa fa-check"></i><b>7.6</b> Model comparison via information criteria (both)</a></li>
<li class="chapter" data-level="7.7" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#generating-predictions-both"><i class="fa fa-check"></i><b>7.7</b> Generating predictions (both)</a></li>
<li class="chapter" data-level="7.8" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#conclusions"><i class="fa fa-check"></i><b>7.8</b> Conclusions</a></li>
<li class="chapter" data-level="7.9" data-path="bayesian-versus-frequentist-statistics.html"><a href="bayesian-versus-frequentist-statistics.html#take-home-message-1"><i class="fa fa-check"></i><b>7.9</b> Take home message</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mixtures.html"><a href="mixtures.html"><i class="fa fa-check"></i><b>8</b> Mixtures</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mixtures.html"><a href="mixtures.html#beta-binomial"><i class="fa fa-check"></i><b>8.1</b> Beta Binomial</a></li>
<li class="chapter" data-level="8.2" data-path="mixtures.html"><a href="mixtures.html#negative-binomial-a.k.a.-gamma-poisson"><i class="fa fa-check"></i><b>8.2</b> Negative binomial, a.k.a. Gamma Poisson</a></li>
<li class="chapter" data-level="8.3" data-path="mixtures.html"><a href="mixtures.html#ordered-categorical"><i class="fa fa-check"></i><b>8.3</b> Ordered categorical</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="instrumental-variables.html"><a href="instrumental-variables.html"><i class="fa fa-check"></i><b>9</b> Instrumental Variables</a>
<ul>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#disclaimer"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#can-we-estimate-an-effect-of-military-experience-on-wages"><i class="fa fa-check"></i>Can we estimate an effect of military experience on wages</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#draft-as-an-instrumental-variable"><i class="fa fa-check"></i>Draft as an instrumental variable</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#two-stage-least-squares"><i class="fa fa-check"></i>Two-stage least squares</a></li>
<li class="chapter" data-level="" data-path="instrumental-variables.html"><a href="instrumental-variables.html#covarying-residuals"><i class="fa fa-check"></i>Covarying residuals</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html"><i class="fa fa-check"></i><b>10</b> Parameters: combining information from an individual with population</a>
<ul>
<li class="chapter" data-level="10.1" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#everyone-is-the-same-single-parameter"><i class="fa fa-check"></i><b>10.1</b> Everyone is the same (single parameter)</a></li>
<li class="chapter" data-level="10.2" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#everyone-is-unique-independent-parameters"><i class="fa fa-check"></i><b>10.2</b> Everyone is unique (independent parameters)</a></li>
<li class="chapter" data-level="10.3" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-different-but-belong-to-a-population-pooled-parameters"><i class="fa fa-check"></i><b>10.3</b> People are different but belong to a population (pooled parameters)</a></li>
<li class="chapter" data-level="10.4" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-different-but-belong-to-a-group-within-a-population-multilevel-clusters-of-pooled-parameters"><i class="fa fa-check"></i><b>10.4</b> People are different but belong to a group within a population (multilevel clusters of pooled parameters)</a></li>
<li class="chapter" data-level="10.5" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-similar-to-some-but-different-to-others-gaussian-process"><i class="fa fa-check"></i><b>10.5</b> People are similar to some but different to others (Gaussian process)</a></li>
<li class="chapter" data-level="10.6" data-path="parameters-combining-information-from-an-individual-with-population.html"><a href="parameters-combining-information-from-an-individual-with-population.html#people-are-different-but-belong-to-a-population-in-which-parameters-are-correlated-correlated-pooled-parameters"><i class="fa fa-check"></i><b>10.6</b> People are different but belong to a population in which parameters are correlated (correlated pooled parameters)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://alexander-pastukhov.github.io/">Alexander (Sasha) Pastukhov</li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on and Solutions for Statistical Rethinking</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parameters-combining-information-from-an-individual-with-population" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Parameters: combining information from an individual with population</h1>
<p>When we infer values of parameters, we must decide on how to combine information available for an individual “random effect” entry (e.g., participant) and information about the distribution of values for this parameter in the sample in general or in some group within that sample. The former — data for an individual — describes an individual itself but is, necessarily, noisy. The latter — data for the entire sample — has better signal-to-noise ratio, as it pools information across all individuals, but is informative about averages not individuals.</p>
<p>Below I list various strategies that were used throughout the book. The main purpose is to show that they differ primarily in the relative contribution of the two sources and how individuals are used to compute averages. I will talk primarily about intercepts, as this is the most often varied parameter, but the same logic is applicable to <em>any</em> free parameter in the model.</p>
<div id="everyone-is-the-same-single-parameter" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Everyone is the same (single parameter)</h2>
<p>The very first strategy that we used was to employ just a single intercept parameter in the model.</p>
<p><span class="math display">\[height_i \sim \alpha\\
\alpha \sim Normal(178, 9.5)\]</span></p>
<p>This is an extreme case of when we ignore the fact that people (monkeys, countries, etc.) are different and, effectively, model everyone as a single typical average meta-individual. The information about variance within the sample is discarded.</p>
<p>In the plot below, measurement for each individual (distinguished by their position on y-axis) is plotted on x-axis (black circles). But using a single intercept (vertical line) in the model, means all of them get an average height (blue open circles).</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Another way to view the same data is by plotting raw data (y-axis) vs. used estimates (x-axis). The vertical line denotes the population mean, whereas the diagonal line implies that estimates are equal to the data.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-39-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="everyone-is-unique-independent-parameters" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Everyone is unique (independent parameters)</h2>
<p>The other extreme is to assume that everyone is unique and should be judged (estimated) using only their own data. Here, the information about the population is discarded.</p>
<p><span class="math display">\[height_i \sim \alpha_i\\
\alpha_i \sim Normal(178, 9.5)\]</span></p>
<p>This is the approach taken by paired t-test and repeated measures ANOVA. Note that it is likely to overfit the data, as we allow limited and noisy data to fully determine intercepts. Use of weak or flat priors (as in frequentist approach) is likely to make out-of-sample performance even worse.</p>
<p>In the plot below, measurement for each individual (distinguished by their position on y-axis) is plotted on x-axis (black circles). You cannot see black circles because they are covered by open blue circles — the estimates used by the model.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-40-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here is another representation of the same data with all dots lying on the diagonal (estimate is equal to data). The vertical blue line still denotes the population mean this information is not used for estimates.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-41-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="people-are-different-but-belong-to-a-population-pooled-parameters" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> People are different but belong to a population (pooled parameters)</h2>
<p>A more balanced approach is to combine data from an individual and population. This is a two level (multilevel) approach, where an individual parameter value comes from a population (group) level distribution.</p>
<p><span class="math display">\[height_i \sim \alpha_i\\
\alpha_i \sim Normal(\alpha^{pop}, \sigma^{\alpha})\\
\alpha^{pop} \sim Normal(178, 9.5)\\
\sigma^{\alpha} \sim Exponential(1)\]</span></p>
<p>The basic idea is the “regression to the mean,” so that unusual-looking individuals are probably more average than they appear. In other words, they are so unusual because of the noise during that particular measurement. During a next measurement the noise will be different, probably not so extreme, and the individual will appear to be more normal. The pull of extreme observations toward the mean is the same as one from a static prior. The main difference is that we use <em>adaptive</em> priors and determine how typical/unusual an observation is based on all observations themselves, <em>including</em> the extreme ones. As with “normal” priors, the influence of adaptive prior is most pronounced for extreme, unreliable observations, or observations with small amount of data. They are easier to overfit and, hence, benefit most from regularizing priors.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-43-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="people-are-different-but-belong-to-a-group-within-a-population-multilevel-clusters-of-pooled-parameters" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> People are different but belong to a group within a population (multilevel clusters of pooled parameters)</h2>
<p>A logical extension of a two-level approach is to extended it with more levels: An individual belongs to a group (cluster) which, in turn, belongs to a population.</p>
<p><span class="math display">\[height_i \sim \alpha_i\\
\alpha_i \sim Normal(\alpha^{group}_i, \sigma^{group})\\
alpha^{group} \sim Normal(\alpha^{pop}, \sigma^{pop}) \\
\alpha^{pop} \sim Normal(178, 9.5)\\
\sigma^{\alpha}, \sigma^{group} \sim Exponential(1)\]</span></p>
<p>For an individual, it allows to pool information across a more relevant group. For example, males tend to be taller than females, so it would make more sense to use female sub-population/group/cluster to decide on just how typical a given female is. Same goes for other types of clusters: e.g., students from a particular school are probably more similar in their knowledge on a particular subject (such as math) as compared to students from other schools. At the same time, we can still pull information across clusters, making our inferences about them more reliable as well. And, of course, you are not limited in the number of levels you can create: students within a school, schools within a district, districts within a city, etc.</p>
<p>In the plots below, notice that sex cluster distributions are tighter than a single population distribution above. Also notice how individual observers are pulled toward the group mean and, again, the pull is proportional to how atypical a value is.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-44-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-45-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="people-are-similar-to-some-but-different-to-others-gaussian-process" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> People are similar to some but different to others (Gaussian process)</h2>
<p>Multilevel approach with clusters is very useful but it works only if you have well-defined discrete cluster: sex, school a student belongs to, occupation, etc. However, sometimes there are no such clear boundaries. Rather, you can presume that similarity in a property that you are interested in (height, in our case) could be proportional to another well defined measure of similarity / distance between the individuals. For example, one could, perhaps naively, assume that individuals that are more similar in their genes have more similar height. In that case, we can compute distance between their genetic sequences and use that distance to define a population <em>relative</em> to an individual.</p>
<p><span class="math display">\[height_i \sim \alpha^{pop} + \alpha_i\\
\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \dots \\ \alpha_N \end{pmatrix} \sim MVNormal(
 \begin{pmatrix}0, 0, \dots, 0 \end{pmatrix}, K)\\
K_{ij} = = \eta^2 exp(−\rho^2D^2_{ij}) + \delta_{ij}\sigma^2 \\
\alpha^{pop} \sim Normal(178, 9.5)\]</span></p>
<p>where <span class="math inline">\(D_{ij}\)</span> is the distance between individuals (see the book for further details on the formula and Gaussian process in general).</p>
<p>Generally speaking, the idea is that contribution of observations is proportional to their distance to the individual in question. It also means that there are no discrete cluster. Rather, each observation gets its own distance-based population distribution which it is judged against. Note, however, that parameter <span class="math inline">\(\rho\)</span> adjusts the effect of the distance making it more or less relevant. Thus, the distance can be ignored by the model (via smaller value of <span class="math inline">\(\rho\)</span>) meaning that all observations/individuals contribute equally to the population distribution (relative to an individual), i.e., a given intercept is correlated with everybody else. Conversely, larger values of <span class="math inline">\(\rho\)</span> mean that only nearest neighbors are taken into account.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-46-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In the example below, the distance between individuals is determined by the distance on the vertical axis. The first plot use <span class="math inline">\(\rho=1\)</span> that samples most observations (dot size reflect the <span class="math inline">\(K\)</span> used in the MVNormal). It produces a fairly broad population distribution and shift the first participant to the <em>right</em>.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The second plot uses the same distance measure but <span class="math inline">\(\rho=10\)</span> meaning that it is the close neighbors that affect the estimate. Here, the population distribution is much tighter and the estimate for the first participant is shifted to the <em>left</em>.</p>
<p><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-49-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="people-are-different-but-belong-to-a-population-in-which-parameters-are-correlated-correlated-pooled-parameters" class="section level2" number="10.6">
<h2><span class="header-section-number">10.6</span> People are different but belong to a population in which parameters are correlated (correlated pooled parameters)</h2>
<p>The idea is that if two (or more) parameters are correlated within the population — e.g., taller people (intercept) grow more if they drink milk (slope, effect of the milk) — then you can pool information across both parameters and use each to evaluate the other one.</p>
<p><span class="math display">\[height_i \sim \alpha^{pop} + \alpha_i + \beta_i \cdot Milk\\
\begin{pmatrix} \alpha_i \\ \beta_i \\ \dots \\ \alpha_N \end{pmatrix} \sim MVNormal(
 \begin{pmatrix}0, 0 \end{pmatrix}, K) \\
\alpha^{pop} \sim Normal(178, 9.5)\]</span></p>
<p>Here, the pulling forces are a bit more complicated than during a single parameter situation. Namely, parameters are not necessarily pulled towards the center of the distribution (population averages) but orthogonal to isolines, so that they are adjusted relative to each other. In the plot below, a black filled dot is average with respect to the slop but has a higher than average intercept. This means that, one the one hand, its intercept is too high for its average slope. One the other hand, its slope is too average for such a high slope. Both parameters pull on each other at the same time, which us why the intercept becomes more normal but the slope becomes higher (a more bit more extreme). The advantage is that you can use both parameters to judge them against the population.</p>
<p><img src="images/pooled-correlated-parameters.png" width="796" style="display: block; margin: auto;" /></p>

</div>
</div>


























            </section>

          </div>
        </div>
      </div>
<a href="instrumental-variables.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["notes-on-statistical-rethinking.pdf", "notes-on-statistical-rethinking.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
