<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 19 Intuition for how Cholesky decomposition makes possible to generate correlated random variables | Notes on Statistics</title>
<meta name="author" content="Alexander Pastukhov">
<meta name="description" content="In chapter 14 of the “Statistical Rethinking”, a Cholesky decomposition of a partial correlations matrix is used to generate correlated random variables with matching partial correlations. For...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 19 Intuition for how Cholesky decomposition makes possible to generate correlated random variables | Notes on Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="In chapter 14 of the “Statistical Rethinking”, a Cholesky decomposition of a partial correlations matrix is used to generate correlated random variables with matching partial correlations. For...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 19 Intuition for how Cholesky decomposition makes possible to generate correlated random variables | Notes on Statistics">
<meta name="twitter:description" content="In chapter 14 of the “Statistical Rethinking”, a Cholesky decomposition of a partial correlations matrix is used to generate correlated random variables with matching partial correlations. For...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<link rel="stylesheet" href="bs4_style.css%20-%20style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Notes on Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Precis</a></li>
<li><a class="" href="loss-functions.html"><span class="header-section-number">2</span> Loss functions</a></li>
<li><a class="" href="directed-acyclic-graphs-and-causal-reasoning.html"><span class="header-section-number">3</span> Directed Acyclic Graphs and Causal Reasoning</a></li>
<li><a class="" href="spurious-association.html"><span class="header-section-number">4</span> Multiple regression - Spurious association</a></li>
<li><a class="" href="the-haunted-dag.html"><span class="header-section-number">5</span> The haunted DAG</a></li>
<li><a class="" href="information-criteria.html"><span class="header-section-number">6</span> Information Criteria</a></li>
<li><a class="" href="bayesian-vs.-fequentist-statisics.html"><span class="header-section-number">7</span> Bayesian vs. fequentist statisics</a></li>
<li><a class="" href="mixtures.html"><span class="header-section-number">8</span> Mixtures</a></li>
<li><a class="" href="instrumental-variables.html"><span class="header-section-number">9</span> Instrumental Variables</a></li>
<li><a class="" href="parameters-combining-information-from-an-individual-with-population.html"><span class="header-section-number">10</span> Parameters: combining information from an individual with population</a></li>
<li><a class="" href="incorporating-measurement-error-a-rubber-band-metaphor.html"><span class="header-section-number">11</span> Incorporating measurement error: a rubber band metaphor</a></li>
<li><a class="" href="generalized-additive-models-as-continuous-random-effects.html"><span class="header-section-number">12</span> Generalized Additive Models as continuous random effects</a></li>
<li><a class="" href="flat-priors-the-strings-attached.html"><span class="header-section-number">13</span> Flat priors: the strings attached</a></li>
<li><a class="" href="unbiased-mean-versus-biased-variance-in-plain-english.html"><span class="header-section-number">14</span> Unbiased mean versus biased variance in plain English</a></li>
<li><a class="" href="probability-mass-versus-probability-density.html"><span class="header-section-number">15</span> Probability mass versus probability density</a></li>
<li><a class="" href="effective-degrees-of-freedom-number-of-parameters.html"><span class="header-section-number">16</span> Effective degrees of freedom / number of parameters</a></li>
<li><a class="" href="multiple-regression---masked-relationship.html"><span class="header-section-number">17</span> Multiple regression - Masked relationship</a></li>
<li><a class="" href="ordered-categorical-data-i.e.-likert-scales.html"><span class="header-section-number">18</span> Ordered Categorical Data, i.e., Likert-scales</a></li>
<li><a class="active" href="intuition-for-how-cholesky-decomposition-makes-possible-to-generate-correlated-random-variables.html"><span class="header-section-number">19</span> Intuition for how Cholesky decomposition makes possible to generate correlated random variables</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/alexander-pastukhov/notes-on-statistics">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="intuition-for-how-cholesky-decomposition-makes-possible-to-generate-correlated-random-variables" class="section level1" number="19">
<h1>
<span class="header-section-number">19</span> Intuition for how Cholesky decomposition makes possible to generate correlated random variables<a class="anchor" aria-label="anchor" href="#intuition-for-how-cholesky-decomposition-makes-possible-to-generate-correlated-random-variables"><i class="fas fa-link"></i></a>
</h1>
<p>In chapter 14 of the “Statistical Rethinking”, a Cholesky decomposition of a partial correlations matrix is used to generate <em>correlated</em> random variables with matching partial correlations. For uninitiated the whole procedure looks magic but “It would be magic, except that it is just algebra.” The purpose of these notes is to provide an intuition of how this works. I will present two explanations: A purely algebraic one and a more visual one. Unfortunately, both of them require knowledge of essentials of linear algebra. I would recommend the entire <a href="https://youtu.be/fNk_zzaMoSs">Essense of linear algebra</a> series by Grant Sanderson, a.k.a. <a href="https://www.3blue1brown.com/">3Blue1Brown</a> but first three videos are a must at least for the visual proof. These are bite-sized 10-15 minute videos that deep you a deep intuition about a single topic, so you can always watch them when you have a break.</p>
<div id="a-verbose-and-visual-proof" class="section level2" number="19.1">
<h2>
<span class="header-section-number">19.1</span> A verbose and visual proof<a class="anchor" aria-label="anchor" href="#a-verbose-and-visual-proof"><i class="fas fa-link"></i></a>
</h2>
<p>For me the critical part was to understand what exactly does the Cholesky decomposition perform, i.e., what information the matrix <span class="math inline">\(L\)</span> holds and what kind of linear transformation it represents. First, let us think what a correlations matrix is:
<span class="math display">\[\Sigma = \begin{vmatrix} corr(x, x) &amp; corr(x, y) \\ corr (y, x) &amp; corr(y, y) \end{vmatrix} \]</span></p>
<p>Now we need to recall some linear algebra, namely that a dot product between to vectors is <span class="math inline">\(x \cdot y = |x| |y| cos(\theta_{xy})\)</span>, where <span class="math inline">\(|x|\)</span> and <span class="math inline">\(|y|\)</span> are length of vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and <span class="math inline">\(\theta_{xy}\)</span> is an angle between them. Also note that <span class="math inline">\(cos(\theta)\)</span> corresponds to Pearson’s correlation <span class="math inline">\(\rho\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(v\)</span> are z-scores with mean of zero and standard deviation of one then their length is <span class="math inline">\(1\)</span> then their dot product computes Pearson’s correlation directly:
<span class="math display">\[x \cdot y = |x| |y| cos(\theta_{xy}) = 1 \times 1 \times cos(\theta_{xy}) = cos(\theta_{xy})\]</span></p>
<p>This means that we can rewrite our correlation matrix as a matrix of cosines of angles between individual vectors (variables):
<span class="math display">\[\Sigma = \begin{vmatrix} cos(\theta_{xx}) &amp; cos(\theta_{xy})) \\ cos(\theta_{yx}) &amp; cos(\theta_{yy})) \end{vmatrix} \]</span></p>
<p>Now it is time to get to Cholesky decomposition: <span class="math inline">\(\Sigma = L L^T\)</span>. Given that <span class="math inline">\(\Sigma\)</span> is a matrix of pairwise (cosines of) angles between vectors, Cholesky decomposition produces a set of vectors with exactly those angles in-between.</p>
<p>Let us visualize this for a 2D case. A correlation of <span class="math inline">\(0.5\)</span> corresponds to a <span class="math inline">\(cos(60 ^{\circ})\)</span></p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">L</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span>
<span class="va">L</span></code></pre></div>
<pre><code>##      [,1]      [,2]
## [1,]    1 0.5000000
## [2,]    0 0.8660254</code></pre>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-90-1.png" width="672"></div>
<p>Now for a negative correlation</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">L</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span>
<span class="va">L</span></code></pre></div>
<pre><code>##      [,1]       [,2]
## [1,]    1 -0.5000000
## [2,]    0  0.8660254</code></pre>
<div class="inline-figure"><img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-92-1.png" width="672"></div>
<p>The same logic applies to cases beyond 2D but are trickier to visualize.</p>
<p>Now that we know what matrix <span class="math inline">\(L\)</span> represents (set of vectors that are positioned at desired angles relative to each other), we can think about what <span class="math inline">\(LZ\)</span> achieves given that <span class="math inline">\(Z\)</span> is our <span class="math inline">\(N\times 2\)</span> matrix random numbers. Recall that a (square) matrix describes a linear transformation of space and can be understood by where it sends basis vectors (axes) <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> (watch <a href="https://youtu.be/kYB8IZa5AuE">this video</a> to get a visual feel for it). Our matrix <span class="math inline">\(L\)</span> sends <span class="math inline">\(\hat{i}\)</span> to a new location described by its first column<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Spoilers: it is the same location as before.&lt;/p&gt;"><sup>41</sup></a> and <span class="math inline">\(\hat{j}\)</span> to a location defined by its second column. This rotates and sheers the space. When applied to our random vectors, this means that although they started as independent (lying along orthogonal axes) they become correlated (as the axes they lie along stop being orthogonal).</p>
<p>Again let us visualize for the case of <span class="math inline">\(\rho=0.5\)</span>, i.e., <span class="math inline">\(\theta_{xy} = 60 ^\circ\)</span>:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">192405</span><span class="op">)</span>
<span class="va">Sigma</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">1</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">L</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chol.html">chol</a></span><span class="op">(</span><span class="va">Sigma</span><span class="op">)</span>
<span class="va">L</span></code></pre></div>
<pre><code>##      [,1]      [,2]
## [1,]    1 0.5000000
## [2,]    0 0.8660254</code></pre>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">200</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">Z</span><span class="op">)</span></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,] 1.00000000 0.01781367
## [2,] 0.01781367 1.00000000</code></pre>
<p>Below is the plot of the original data. Note two highlighted points, here, values of x (positive for a red dot, negative for a blue one) are uncorrelated with values of y (roughly zero, which is why I have picked these two points).
<img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-94-1.png" width="672">
Now the same plot but for <span class="math inline">\(M = ZL\)</span>. Here, knowing <span class="math inline">\(x\)</span> coordinate allows you to predict <span class="math inline">\(y\)</span> as well.
<img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-95-1.png" width="672"></p>
<p>Below is the plot for all points, showing their positions before and after the transformation. Note how the dots are only moved along the vertical axes. This is because Cholesky decomposition left the first basis vector <span class="math inline">\(\hat{i}\)</span> (x-axis) unchanged (still <span class="math inline">\((1, 0)\)</span>) but adjusted the <span class="math inline">\(\hat{j}\)</span> to achieve the desired angle of <span class="math inline">\(60^\circ\)</span>. Also note that we <em>could have</em> used a different vector for <span class="math inline">\(\hat{i}\)</span>, after all there are infinite pairs of <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> with <span class="math inline">\(60^\circ\)</span> between them. However, Cholesky decomposition needs a starting point — a first vector — and keeping <span class="math inline">\(\hat{i}\)</span> at its original position is as good of a starting point as any.
<img src="notes-on-statistical-rethinking_files/figure-html/unnamed-chunk-96-1.png" width="672"></p>
</div>
<div id="purely-algebraic-proof" class="section level2" number="19.2">
<h2>
<span class="header-section-number">19.2</span> Purely algebraic proof<a class="anchor" aria-label="anchor" href="#purely-algebraic-proof"><i class="fas fa-link"></i></a>
</h2>
<p>Here is an alternative purely algebraic take on it. Note that this is a very brief description, for further details please refer to an excellent <a href="https://mlisi.xyz/post/simulating-correlated-variables-with-the-cholesky-factorization">post</a> by <a href="https://mlisi.xyz/">Matteo Lisi</a>.</p>
<p>Given that we have a correlations matrix <span class="math inline">\(\Sigma\)</span>, we can use <a href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a>:
<span class="math display">\[\Sigma = LL^T\]</span>
Next, we can generate a matrix of <em>uncorrelated</em> random numbers <span class="math inline">\(Z\)</span> (e.g., z-scores from a standard normal distribution). Assuming that samples are uncorrelated then the covariance matrix is <span class="math inline">\(\mathbb{E}(ZZ^T) = I\)</span> (an identity matrix with <span class="math inline">\(1\)</span> on the diagonal and <span class="math inline">\(0\)</span> everywhere else). For the proof you need to remember that:</p>
<ol style="list-style-type: decimal">
<li>order of factor is reversed if you apply transposition or inverse to the individual factors of a matrix product <span class="math inline">\((AB)^T = B^TA^T\)</span>,</li>
<li>
<span class="math inline">\(\mathbb{E}(cX) = c\mathbb{E}(X)\)</span> an expected value of a scaled quantity is equal to an expected value of the unscaled quantity times the scaling factor (e.g., <span class="math inline">\(mean(3\cdot x) = 3\cdot mean(x)\)</span>)</li>
<li>matrix multiplication by an identity matrix does not change anything <span class="math inline">\(IM=M\)</span> (this is just like multiplying a scalar by <span class="math inline">\(1\)</span>: <span class="math inline">\(1\cdot m = m\)</span>),</li>
</ol>
<p>Now, given that <span class="math inline">\(M=LZ\)</span> (we matrix multiply our uncorrelated data with the Cholesky factor of the desired pairwise correlations matrix), we can compute partial correlations for these new values
<span class="math display">\[\mathbb{E}(MM^T) = \\
\mathbb{E}((LZ)(LZ)^T) =\]</span>
Applying rule #1 to open the brackets:
<span class="math display">\[\mathbb{E}(L Z Z^T L^T) =\]</span>
Applying rule #2 to move constant <span class="math inline">\(L\)</span> out of expected value operator:
<span class="math display">\[L\mathbb{E}(Z Z^T) L^T =\]</span>
Remembering that <span class="math inline">\(\mathbb{E}(ZZ^T) = I\)</span>
<span class="math display">\[L I L^T =\]</span>
Applying rule #3 to multiply by an identity matrix
<span class="math display">\[L L^T = \Sigma\]</span></p>
<p>The proof is very simple and you see that you must end up with a roughly the same<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Depending on how close &lt;span class="math inline"&gt;\(\mathbb{E}(ZZ^T)\)&lt;/span&gt; to &lt;span class="math inline"&gt;\(I\)&lt;/span&gt;.&lt;/p&gt;'><sup>42</sup></a> correlations. Unfortunately, although I can see <em>why</em> this should work (because algebra), I do not get any intuition about <em>how</em> it works, which is I came up with a slower visual explanation above.</p>

</div>
</div>
















































  <div class="chapter-nav">
<div class="prev"><a href="ordered-categorical-data-i.e.-likert-scales.html"><span class="header-section-number">18</span> Ordered Categorical Data, i.e., Likert-scales</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#intuition-for-how-cholesky-decomposition-makes-possible-to-generate-correlated-random-variables"><span class="header-section-number">19</span> Intuition for how Cholesky decomposition makes possible to generate correlated random variables</a></li>
<li><a class="nav-link" href="#a-verbose-and-visual-proof"><span class="header-section-number">19.1</span> A verbose and visual proof</a></li>
<li><a class="nav-link" href="#purely-algebraic-proof"><span class="header-section-number">19.2</span> Purely algebraic proof</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/alexander-pastukhov/notes-on-statistics/blob/master/18-cholesky.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/alexander-pastukhov/notes-on-statistics/edit/master/18-cholesky.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Notes on Statistics</strong>" was written by Alexander Pastukhov. It was last built on 2022-06-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
