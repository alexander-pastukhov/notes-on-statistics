[{"path":"index.html","id":"precis","chapter":"1 Precis","heading":"1 Precis","text":"collection notes statistics attempt provide details intuition topics, loss functions, information theory, information criteria, MCMC algorithms, etc. primary source notes “Statistical Rethinking” book Richard McElreath, mention chapter, mean chapter book.notes aimed primarily future-hope might useful others, material free use licensed Creative Commons Attribution-NonCommercial-NoDerivatives V4.0 International License.","code":""},{"path":"loss-functions.html","id":"loss-functions","chapter":"2 Loss functions","heading":"2 Loss functions","text":"purpose comment give intuition loss functions, mentioned chapter 3. particular, want understand different loss functions (L0, L1, L2) correspond different point-estimates (mode, median, mean). Plus, want understand can view choice likelihood function, picking Gaussian chapter 4, analogous picking loss function.afraid easiest way explain L2 loss results mean via derivative. , confident basic calculus skill, might useful first watch episodes Essense Calculus series Grant Sanderson, .k.. 3Blue1Brown. suggest watching least first three episodes (actually, recommend watch whole series) short time watch episode 21.","code":""},{"path":"loss-functions.html","id":"loss-function-the-concept","chapter":"2 Loss functions","heading":"2.1 Loss function, the concept","text":"Imagine selling ice-cream beach, can assume narrow strip sand , therefore, one-dimensional problem. hot, everyone wants ice-cream (obviously) want maximize number ice-creams sell (obviously). People distributed random (necessarily uniform symmetric) way along beach, question : put single ice-cream stand maximize profits? answer depends choice loss function describes distance particular person stand influences whether person buy ice-cream. words, describes cost getting stand, .e. walking --way sand heat. cost clearly depends distance simplest case, linearly proportional distance: need walk twice distance, costs getting ice-cream twice high. However, relationship distance cost simple linear many different loss / cost functions.can write loss/cost function formally \\(L(stand, person_i)\\) stand location stand person_i location particular ith person. cost can either zero positive, .e., assume benefit walking way, cost. , put ice-cream stand?","code":""},{"path":"loss-functions.html","id":"l0-mode","chapter":"2 Loss functions","heading":"2.2 L0 (mode)","text":"simplest loss function \\[L0(stand, person_i) =  \\begin{cases}\n0, stand == person_i \\\\\n\\infty, stand \\neq person_i\n\\end{cases}\\]function assumes everybody hates walking much, walk unbearable avoided. Thus, cost getting ice-cream people positioned right next stand. everybody else, even one meter away, costs walking infinite, won’t bother , therefore, won’t buy ice-cream. Still, business selling one, put stand given lazy customers ? Well, just find biggest group people put stand next . one else come least got biggest group customers . look distribution customers along beach highest peak (peak) called mode distribution.","code":""},{"path":"loss-functions.html","id":"l1-median","chapter":"2 Loss functions","heading":"2.3 L1 (median)","text":"next loss function, already mentioned, assumes simple linear relationship distance cost\n\\[L1(stand, person_i) = |person_i - stand|\\]\nwords, cost equal distance (need | | get absolute value, person “left ” stand, case person - stand distance negative). , put stand? Let us start fairly random location 3 customers left 7 right.\ncan, principle, compute actual cost simpler ask question whether can improve cost moving somewhere else? Imagine move left minority customers . Now 1 left 8 right (plus 2 location).\nproblem , moved away majority people total cost original cost - 3 (improvement due moving close minority) + 8 (increase loss due moving away majority), \\(\\Delta L1 = +5\\). Oops, made worse! moving right?\n\nNow move towards majority customers, four left six right (plus one location). change cost original cost + 4 (loss due moving away minority) - 6 (improvement due moving towards majority), \\(\\Delta L1 = -2\\). gives us idea: try get even closer majority keeping walking right! Eventually, get point 50/50. keep moving right? move left? move ?point moving left. just came moving right made things better. However, keep moving right, keep passing people, majority now left walking away majority, raising costs (losses). , get point half customers left half right, better. movement gets 50/50 means customers one side (say left, moved right) , already figured , best strategy move towards majority, gets back started 50/50 point. 50/50 points split, half customers / probability mass one side half , called median.","code":""},{"path":"loss-functions.html","id":"l2-mean","chapter":"2 Loss functions","heading":"2.4 L2 (mean)","text":"classic loss function Euclidean distance \\[L2(stand, person_i) = (person - stand)^2\\]\n, every next step becomes progressively harder customers. cost walking 1 meter 1 (unit effort). walking 2 \\(2^2 = 4\\) \\(3^2=9\\) 3 meters. Thus, penalty (cost/loss) away stand increases power law. Still, one needs sell ice-cream, one needs find best spot total cost minimal\\[L2(stand, person) = \\sum_{=1}^{N}{(person_i - stand)^2}\\]\n, can compute minimal average cost dividing sum total number customers N:\n\\[<L2(stand, person)> = \\frac{1}{N}\\sum_{=1}^{N}{(person_i - stand)^2}\\]Conceptually, find minimum walking along beach direction reduces cost hit point start going . strategy called gradient descent , generally speaking, computer finds minima computationally: make steps different directions see way keep going things start going . However, one-dimensional well-behaving case things even simpler can use calculus figure solution analytically. watched videos advertised , ’ll know derivative function zero extrema (minima maxima), just need differentiate average L2 position stand find zero2.\\[\\frac{\\partial L2}{\\partial stand} = -\\frac{2}{N}\\sum_{=1}^{N}{(person_i - stand)}\\]\nwant\n\\[\\frac{\\partial L2}{\\partial stand} = 0\\]\nstate\n\\[\\frac{2}{N}\\sum_{=1}^{N}{(person_i - stand)} = 0.\\]Opening brackets rearranging get\\[- \\frac{2}{N}\\sum_{=1}^{N}{person_i} + \\frac{2 \\cdot N}{N} \\cdot stand = 0 \\\\\n2 \\cdot stand = \\frac{2}{N}\\sum_{=1}^{N}{person_i} \\\\\nstand = \\frac{1}{N}\\sum_{=1}^{N}{person_i}\\], optimal location stand mean: average location people beach.","code":""},{"path":"loss-functions.html","id":"l1-median-vs.-l2-mean","chapter":"2 Loss functions","heading":"2.5 L1 (median) vs. L2 (mean)","text":"One problem mean sensitive outliers. costs grow power law, approach favors lot medium-sized distances lots smalls ones plus one really large one. Thus, single person far side beach big influence stand’s location (already saw difference example ). data analysis, means outliers pull estimates away majority responses. might good idea consider using median rather mean. distribution symmetric, difference negligible presence outliers median, point-estimate, robust.","code":""},{"path":"loss-functions.html","id":"choosing-a-likelihood","chapter":"2 Loss functions","heading":"2.6 Choosing a likelihood","text":"far talked selling ice-cream beach question choosing loss function applies trying fit distribution regression line, chapter 4. , also point-estimate (regression line point) try put way minimize costs data points line (distance point-estimate line data point called residual). classic way use L2 distance approach called ordinary least squares, try minimize squared residuals.Gaussian special uses L2 distance, see \\((x - \\mu)^2\\) inside exponential:\n\\[f(x) = \\frac{1}{\\sigma \\sqrt(2 \\pi)}e^{\\left(-\\frac{1}{2}\\frac{(x - \\mu)^2}{\\sigma^2}\\right)}\\]using equivalent fitting via ordinary least squares. However, McElreath hinted, can choose different likelihoods different distance--loss formula (like L1 different L2) also symmetry. L1 L2 (Gaussian) ignore sign distance. matter whether customers left right. distributions, Beta, Gamma, Log Normal symmetric, distance cost differently depending side customer .\nallows think choice likelihood distribution terms choosing loss function. describe tolerant points point estimate (regression line). example, t-distribution heavier tails Gaussian (want sound like real mathematician, say “leptokurtic”), losses outliers (penalty larger residuals) lower. Using instead Gaussian similar changing loss function L2 like L1 (e.g. \\(|person_i - stand|^{1.5}\\)). Conversely, can pick symmetric distribution narrower Gaussian make residuals penalty even higher (e.g. using \\((person_i - stand)^{4}\\)). can also consider properties: symmetric? operate within certain range (1..7 Likert scale, 0..1 proportions, positive values Gamma)? weight points equally? saw examples , picking different function moves cart (regression line), keep mind using different likelihood move regression line produce different estimates predictions.pick likelihood/loss function? depends kind data , knowledge process generated data, robustness inferences presence outliers, etc. However, real-life cases likely encounter covered distributions described book (Gaussian, exponential, binomial, Poisson, Gamma, etc.). finishing book, basic understanding appropiate typical cases. atypical cases ’ll research !","code":""},{"path":"loss-functions.html","id":"gaussian-in-frenquentist-versus-bayesian-statistics","chapter":"2 Loss functions","heading":"2.7 Gaussian in frenquentist versus Bayesian statistics","text":"Later book McElreath note erroneously assuming normal distribution residuals ruins inferences frequentist statistics Bayesian. picking distribution means different things frequentist Bayesian. wrote , Bayesian case, likelihood merely loss function translate distance data point regression line (residual) penalty (, determines just tolerant points line). Thus, using penalties observed residuals bad loss function make posterior distribution suboptimal still can make inferences still based actual residuals.contrast, frequentist statistics, stating observed residuals sample particular distribution, actual residuals used determine parameters distribution. , however, make inferences using distribution residuals . strong conjecture probably biggest leap faith frequentist statistics saying “know true distribution”. Problem , got likelihood function / distribution wrong, inferences based model describes something else data. example, proportion data assume Gaussian distribution residuals build model residuals always symmetrically distributed (squashed one side floor celing). model data, normally distributed something else. numbers something else may look good (bad) numbers interested . mistake remarkably easy computers won’t stop making . Think back Chapter 1: Golems don’t care! can abuse statistical model/test simply spit numbers, even tests completely unsuitable data. Making sure distribution correct right thing , Golem!","code":""},{"path":"directed-acyclic-graphs-and-causal-reasoning.html","id":"directed-acyclic-graphs-and-causal-reasoning","chapter":"3 Directed Acyclic Graphs and Causal Reasoning","heading":"3 Directed Acyclic Graphs and Causal Reasoning","text":"","code":""},{"path":"directed-acyclic-graphs-and-causal-reasoning.html","id":"peering-into-a-black-box","chapter":"3 Directed Acyclic Graphs and Causal Reasoning","heading":"3.1 Peering into a black box","text":"better understand relationship data statistical analysis one hand DAGs (directed acyclic graphs) hand, came electric engineering metaphor3. Imagine electric device take apart. However, certain points can connect multimeter check whether current flows two points. also schematics device idea whether accurate. names connector nodes match connectivity anybody’s guess. , ? look schematics identify two nodes current definitely flow measure whether case. signal? Good, means least respect connectivity two nodes schematics completely wrong. signal? Sorry, schematics good, find different one try making one . Conversely, can identify two nodes, current flow measure empirically. current? Good! Current? Bad, schematics wrong.relationship device schematics large (device) small (schematics) world. job iteratively adjust latter, matches former. need keep prodding black box predictions schematics start matching actual readings. can say understand device works can make predictions different conditions. Although testing based schematics can automated, generating schematics creating process. depends knowledge devices kind individual exposed nodes.Hopefully, can see maps observational experimental data (large world, readings device) DAGs (small world, presumed schematics). scientist, guess4 causal relationships individual variables draw schematics (DAG). Using DAG, identify pairs variables must dependent independent assuming DAG correct check data whether indeed case. ? Good, DAG (entirely) wrong! ? Bad news, causal relationship different (others prior work) came . need modify DAG draw completely different one, just like engineer must modify schematics.Note process opposite typical multivariate analysis approach using, say, ANOVA. latter case, throw everything together, including /interactions terms, try figuring causal relationship individual independent dependent variable based magnitude significance individual terms. , first run statistical analysis make inferences causal relationships. causal reasoning using DAG, first formulate causal relationships use statistical analysis check whether relationship correct. second approach much powerful, can run ANOVA can formulate many DAGs describe data test iteratively, refining understanding step--step. Moreover, ANOVA (regression model like ) identifying relationships invididual independent signle dependent variable (individual coefficients tell independent variables can used predict dependent). DAG-based analysis allows predict test relationship pairs dependent variables well, decomposing complex model testable verifyable chunks. involved time-consuming approach gives much deeper understanding process styding compared “throw everything ANOVA hope magically figure ”.Moreover, causal calculus via DAGs another trick sleeve. can use conditional probabilities (see ) flip relationship variables. two variables independent, may dependent conditioned third variable. vice versa, depedent variables can become conditionally independent. means predictions connectivity pairs variables can specific (e.g., related via third variable independently cause third variable) testable, now two opposite predictions pair variables! can check current flows (unconditional probability) flow condition third variable. tests match? DAG bad! One none match? Back drawing board!","code":""},{"path":"directed-acyclic-graphs-and-causal-reasoning.html","id":"turning-unconditional-dependence-into-conditional-independence","chapter":"3 Directed Acyclic Graphs and Causal Reasoning","heading":"3.2 Turning unconditional dependence into conditional independence","text":", see multiple regression can show conditional independence two variable case fork DAG divorce data. However, another general way understand concept terms conditional probabilities \\(Pr(M|)\\) \\(Pr(D|)\\). , helps understand condition probabilities filtering operation. compute conditional probability specific value \\(\\), means slice data, keeping whose values \\(M\\) \\(D\\) correspond chosen level \\(\\). easier understand, visualize conditional-probability--filtering synthetic data. illustration purposes, synthesize divorce data, keeping relationships space marriage age linearly generate data 20 data points age (makes easier see understand).can see, variables dependent , case original data. However, let us pick arbitrary value, say \\(=1\\)5 see dots left plot selected via filtering value.First, take top-left bottom-right plots plot, correspondingly, divorce marriage rate versus marriage age. Note ’ve flipped axes bottom-right plot, marriage rate always mapped x-axis. red dots plot correspond divorce marriage rates given (filtered ) marriage age 1. dots marked red top-right plot plotted separately bottom-right. can see, two variables conditional =1 uncorrelated. ? fully determined marriage age variation (spread red dots vertically horizontally top-left bottom-right plots) due noise. Therefore, plot bottom-left effectively plots noise marriage rate versus noise divorce rate , synthetic data design, noise two variable independent, hence, lack correlation.opportunity turn dependence independence conditioning two variables third heart causal reasoning6. draw DAG fork , know given educated guess causal relationship correct (small world!), data (large world!) show dependence conditional independence two variables (divorce marriage rates). ? E.g., two variables always dependent always independent, conditional ? already discussed , just means educated guess wrong relationship variables different thought . Thus, need go back drawing board, come another DAG, repeat analysis see new DAG supported data. variables, DAGs complex beauty causal reasoning can concentrate parts , picking three variables seeing whether guess three variables correct. way, can tinker causal model part--part, instead hoping got everything right first attempt.","code":"\nset.seed(84321169)\nN <- 180\nsigma_noise <- 0.5\n# we repeat each value of age 10 times to make filtering operation easier to see\nsim_waffles <- tibble(MedianAgeMarriage = rep(seq(-2, 2, length.out=9), 20), \n                      Divorce = rnorm(N, MedianAgeMarriage, sigma_noise),\n                      Marriage = -rnorm(N, MedianAgeMarriage, sigma_noise))\n\nMD_plot <- \n  ggplot(data=sim_waffles, aes(x=Marriage, y=Divorce)) + \n    geom_smooth(method=\"lm\", formula=y~x) + \n    geom_point() + \n    xlab(\"Marriage rate\") + \n    ylab(\"Divorce rate\")\n  \nAD_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Divorce)) + \n    geom_smooth(method=\"lm\", formula=y~x) + \n    geom_point() + \n    xlab(\"Median age marriage\") + \n    ylab(\"Divorce rate\")\n  \nAM_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Marriage)) + \n    geom_smooth(method=\"lm\", formula=y~x) + \n    geom_point() + \n    xlab(\"Median age marriage\") + \n    ylab(\"Marriage rate\")\n\nMD_plot | AD_plot | AM_plot\nMD_plot <- \n  ggplot(data=sim_waffles, aes(x=Marriage, y=Divorce)) + \n    geom_smooth(method=\"lm\", formula=y~x, alpha=0.1) + \n    geom_point(data=sim_waffles %>% filter(MedianAgeMarriage == 1), color=\"red\") +\n    geom_point(data=sim_waffles %>% filter(MedianAgeMarriage != 1), alpha=0.15) + \n    xlab(\"Marriage rate\") + \n    ylab(\"Divorce rate\") \nAD_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Divorce)) + \n    geom_smooth(method=\"lm\", formula=y~x) + \n    geom_point(data=sim_waffles %>% filter(MedianAgeMarriage == 1), color=\"red\") +\n    geom_point(data=sim_waffles %>% filter(MedianAgeMarriage != 1), alpha=0.15) + \n    xlab(\"Median age marriage\") + \n    ylab(\"Divorce rate\")\n  \nAM_plot <-\n  ggplot(data=sim_waffles, aes(x=Marriage, y=MedianAgeMarriage)) + \n    geom_smooth(method=\"lm\", formula=y~x) + \n    geom_point(data=sim_waffles %>% filter(MedianAgeMarriage == 1), color=\"red\") +\n    geom_point(data=sim_waffles %>% filter(MedianAgeMarriage != 1), alpha=0.15) + \n    ylab(\"Median age marriage\") + \n    xlab(\"Marriage rate\") \n\nMD_A1_plot <- \n  ggplot(data=sim_waffles %>% filter(MedianAgeMarriage == 1), \n         aes(x=Marriage, y=Divorce)) + \n    geom_smooth(method=\"lm\", formula=y~x, color=\"red\") + \n    geom_point(color=\"red\") +\n    xlab(\"Marriage rate\") + \n    ylab(\"Divorce rate\") +\n    labs(subtitle = \"Pr( |Mariage Age = 1)\")\n\n(AD_plot | MD_plot) /\n(MD_A1_plot |AM_plot)"},{"path":"spurious-association.html","id":"spurious-association","chapter":"4 Multiple regression - Spurious association","heading":"4 Multiple regression - Spurious association","text":"notes Chapter 5 “Many Variables & Spurious Waffles”, specifically, section 5.1 “Spurious association”. remember one thing chapter, “multiple regression easy, understanding interpreting hard”. see , fully understanding relationship variables complicate even working minimal multiple regression just two predictors.reading section 5.1 “Spurious association”, found relationships marriage age, marriage rate, divorce rate clear mysterious. one hand, everything correlated everything.hand, fit linear model predict divorce rate based median age marriage marriage rate, latter clearly irrelevant (output code 5.11 shows coefficient effectively zero, meaning ignored) , therefore, causal influence divorce rate.like me7, said “Huh! model know ?”. , least , explanations chapter help much. key figure 5.4, shows (omitting intercept slope symbols) median age marriage = marriage rate + *extra information* + noise marriage rate = median age marriage + noise. nutshell, variables code information marriage rate noisier version , ignored. Unfortunately, answer “?” still stands. figure 5.4, shows fits residuals illustrative, fit residuals, fit variables time without fitting ! Nowhere model 5.1.4 find \\[\\mu^{M}_{} = \\alpha_{} + \\beta_{} * A_i\\], ’s going ? know? understand , let us start issue multicollinearity.","code":""},{"path":"spurious-association.html","id":"multicollinearity","chapter":"4 Multiple regression - Spurious association","heading":"4.1 Multicollinearity","text":"make things easier understand, let us use simulated data. Imagine marriage divorce rate caused marriage age almost perfectly linearly dependent , \\(D_i = \\beta_A^{true} \\cdot A_i\\) (sake simplicity \\(\\beta_A^{true} = -1\\)) \\(M_i = -A_i\\). causal relationship modeling called fork:\n\\[Marriage~rate~\\leftarrow~Age~~marriage~\\rightarrow~Divorce~rate\\]\npretend variables already standardized, plots look something like .relationship plots , assumed almost perfect correlation, much spread around regression line. Still, definition constructed data, marriage divorce rate caused (computed ) median age , importantly, marriage rate never used compute divorce rate. happens analyze simulated data using model 5.1.3, able figure “marriage rate matter” ?Oh , broke ! \\(\\beta_M\\) now 1.25 rather zero \\(\\beta_A\\) around 0.27 rather -1, . Also note uncertainty associated values, overlap heavily zero8. , data generation process (Divorce rate ← Age → Marriage rate) model (changes priors particular impact case) “magic” inferring lack “causal arrow” Divorce rate  → Marriage rate gone! difference two data sets extra variance (noise) marriage rate variable, let us see absence extra noise simulated data breaks magic.two variables, marriage age rate case, (almost) perfectly correlated (\\(M = -\\)), means can substitute one another. Thus, can rewrite9\n\\[D = \\beta_A \\cdot + \\beta_M \\cdot M \\\\\nD = \\beta_A \\cdot + \\beta_M \\cdot (-) \\\\\nD = (\\beta_A - \\beta_M) \\cdot \\\\\nD = \\beta_A^{true} \\cdot \\]\n\n\\[ \\beta_A^{true} = (\\beta_A - \\beta_M)\\]last bit curse multicollinearity, two variable information, , effectively, fitting sum! equivalent fitting sum10 coefficients times one variables matter one, since identical. used know causes M. look precis output , see fit \\(\\beta_A^{true}\\)! Since bA = 0.27 bM = 1.25, plugging gives us\n\\[\\beta_A^{true} = \\beta_A - \\beta_M = 0.27 - 1.25 = -0.98\\]Hey, slope used construct divorce rate, fitting work! Moreover, can see little uncertainty \\(\\beta_A^{true}\\)uncertainty individual slopes? stems directly fact \\(\\beta_A^{true} = \\beta_A - \\beta_M = -1\\) (-1 case, course). infinite number pairs numbers whose difference give -1: \\(1-2\\), \\(2-3\\), \\((-200)-(-199)\\), \\(999.41-1000.41\\), etc. add (subtract ) -1, fitting procedure settle specific region parameter specific pair values. number , long one differs one.","code":"\ndata(\"WaffleDivorce\")\nset.seed(1212)\nN <- nrow(WaffleDivorce)\nsim_waffles <- tibble(MedianAgeMarriage = rnorm(N),\n                      Divorce = -rnorm(N, MedianAgeMarriage, 0.1),\n                      Marriage = -rnorm(N, MedianAgeMarriage, 0.01))\n\nMD_plot <- \n  ggplot(data=sim_waffles, aes(x=Marriage, y=Divorce)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(\"Marriage rate\") + \n  ylab(\"Divorce rate\")\n\nAD_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Divorce)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(\"Median age marriage\") + \n  ylab(\"Divorce rate\")\n\nAM_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Marriage)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(\"Median age marriage\") + \n  ylab(\"Marriage rate\")\n\nMD_plot | AD_plot | AM_plot\nsim_waffles <-\n  sim_waffles %>%\n  mutate(A = MedianAgeMarriage,\n         M = Marriage,\n         D = Divorce)\n\nsim_waffles_fit <- quap(\n  alist(\n    D ~ dnorm(mu , sigma) ,\n    mu <- a + bM*M + bA*A,\n    a ~ dnorm(0, 0.2),\n    bA ~ dnorm(0, 10),\n    bM ~ dnorm(0, 10),\n    sigma ~ dexp(1)\n  ), \n  data = sim_waffles,\n)\n\nprecis(sim_waffles_fit)##              mean          sd        5.5%      94.5%\n## a     0.002034578 0.012914402 -0.01860513 0.02267429\n## bA    0.267403927 1.188973843 -1.63280591 2.16761377\n## bM    1.245792079 1.187211226 -0.65160076 3.14318492\n## sigma 0.090129176 0.008996557  0.07575094 0.10450741\nposterior_samples <- \n  rethinking::extract.samples(sim_waffles_fit) %>%\n  mutate(bA_true = bA - bM)\n\nggplot(posterior_samples, aes(x=bA_true)) + \n  geom_histogram(bins=100) + \n  xlab(\"bA_true = bA - bM\")"},{"path":"spurious-association.html","id":"back-to-spurious-association","chapter":"4 Multiple regression - Spurious association","heading":"4.2 Back to spurious association","text":", learned two variable information, can fit get individual slopes. wasn’t case real data started ? Marriage age rate correlated, fitting used one (age) sum? answer extra noise marriage rate. real data marriage rate age plus noise: \\(M = -+ \\epsilon\\), \\(\\epsilon\\) traditionally used denote “noise”. extra noise change linear model divorce rate?\n\\[D = \\beta_A \\cdot + \\beta_M \\cdot M \\\\\nD = \\beta_A \\cdot + \\beta_M (- + \\epsilon) \\\\\nD = (\\beta_A  - \\beta_M ) \\cdot + \\beta_M \\cdot \\epsilon\\]definition, \\(\\epsilon\\) pure noise zero predictive value respect divorce rate. Thus, fit alone, expect get slope near zero, “significant relationship”.fitting alone, coefficient \\(\\beta_M\\) appears twice:\n\\[D = (\\beta_A  - \\beta_M) \\cdot + \\beta_M \\cdot \\epsilon\\]\nlatter part, \\(\\beta_M \\cdot \\epsilon\\), pushes \\(\\beta_M\\) towards zero slope, best solution pure noise, saw plot . former part, \\(\\beta_A - \\beta_M\\) needs add \\(\\beta_A^{true}\\), however fix \\(\\beta_M\\), \\(\\beta_A\\) can accommodate. Thus closer \\(\\beta_M\\) zero, closer \\(\\beta_A\\) \\(\\beta_A^{true}\\). ’s magic works! one variable variable plus noise, plus noise induces extra penalty (extra residuals) way reduce residuals ignore uncorrelated noise setting slope zero. Therefore, ignore variable well, merely noisy twin better variable. can see added noise “disambiguates” causal relationship11.stripes show uncertainty (estimate ± standard error) can appreciate quickly reduced marriage rate becomes noisier just little noise required “magic” start working converge true causal relationship12.","code":"\nset.seed(1231455)\nsim_waffles <- tibble(MedianAgeMarriage = rnorm(N),\n                      Divorce = rnorm(N, MedianAgeMarriage, 0.1),\n                      Marriage = -rnorm(N, MedianAgeMarriage, 0.01),\n                      epsilon = rnorm(N))\n\nggplot(sim_waffles, aes(x=epsilon, y=Divorce)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(expression(epsilon)) + \n  ylab(\"Marriage rate\")\nsimulate_waffles <- function(sigma_noise){\n  # generate same data but for noise in Marraige from Age relationship\n  set.seed(169084)\n  sim_df <- sim_waffles <- tibble(MedianAgeMarriage = rnorm(N),\n                                  Divorce = rnorm(N, MedianAgeMarriage, 0.1),\n                                  Marriage = -rnorm(N, MedianAgeMarriage, sigma_noise))\n  \n  # fit data using OLS and pulling out two slope coefficients\n  lm(Divorce ~ Marriage + MedianAgeMarriage, data=sim_df) %>% \n    summary() %>%\n    .$coefficients %>%\n    data.frame() %>%\n    rownames_to_column(\"Variable\") %>%\n    slice(-1) %>%\n    mutate(LowerCI = Estimate - Std..Error,\n           UpperCI = Estimate + Std..Error) %>%\n    select(Variable, Estimate, LowerCI, UpperCI)\n}\n\nsimulated_noise <- \n  tibble(epsilon =exp(seq(log(0.001), log(0.3), length.out = 100))) %>%\n  group_by(epsilon) %>%\n  do(simulate_waffles(.$epsilon))\n\nggplot(simulated_noise, aes(x=epsilon, y=Estimate)) + \n  geom_ribbon(aes(ymin=LowerCI, ymax=UpperCI, fill=Variable), alpha= 0.5) + \n  geom_line(aes(color=Variable)) + \n  scale_x_log10(name=expression(epsilon)) + \n  ylab(\"Slope estimate  ± standard error\") +\n  labs(subtitle = expression(paste(\"Marriage = MedianAgeMarriage + Normal(0, \", epsilon, \")\")))"},{"path":"spurious-association.html","id":"chain-dag","chapter":"4 Multiple regression - Spurious association","heading":"4.3 Chain DAG","text":", bit noise fix everything can know causal relationships three variables? necessarily! Consider another possible causal diagram:\n\\[Marriage~rate~\\rightarrow~Age~~marriage~\\rightarrow~Divorce~rate\\]\nNow marriage rate causes age marriage , turn, causes divorce rate. , let us use synthetic data, can sure causes . However, add fair amount noise make like real data avoid multicoliniarity.plots look similar , let us run model.model fit virtually identical real data , effectively, noisy simulated fork DAG. Yet, remember, underlying causal relationship marriage rate marriage age now opposite. fork DAG, age causing marriage rate. , chain DAG, marriage rate causing age. Moreover, way generated synthetic data, marriage rate causes divorce rate, although effect mediated via marriage age. Yet, looking parameter values \\(\\beta_M\\) might tempted conclude marriage rate effect divorce rate. understand , think relationship marriage rate age . designed , \\(= -M + \\epsilon\\) \\(D = -\\) (ignore noise latter part clarity). Substituting, get \\(D = M - \\epsilon\\) , since designed noise symmetric, can also write \\(D = M + \\epsilon\\). put differently, divorce rate based actual values age, include noise. , somewhat paradoxically, cleaner version original variable less correlated. still unclear, let try metaphor. Imagine friend sings song heard. \\(original \\rightarrow friend \\rightarrow \\). remembered wrong, version somewhat different original. used version song, original, finally hear , sounds wrong, distortions introduced friend.","code":"\ndata(\"WaffleDivorce\")\nset.seed(8973791)\nN <- nrow(WaffleDivorce)\nsim_waffles <- tibble(Marriage = rnorm(N),\n                      MedianAgeMarriage = -rnorm(N, Marriage, 0.2),\n                      Divorce = -rnorm(N, MedianAgeMarriage, 0.2))\n\nMD_plot <- \n  ggplot(data=sim_waffles, aes(x=Marriage, y=Divorce)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(\"Marriage rate\") + \n  ylab(\"Divorce rate\")\n\nAD_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Divorce)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(\"Median age marriage\") + \n  ylab(\"Divorce rate\")\n\nAM_plot <- \n  ggplot(data=sim_waffles, aes(x=MedianAgeMarriage, y=Marriage)) + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  geom_point() + \n  xlab(\"Median age marriage\") + \n  ylab(\"Marriage rate\")\n\nMD_plot | AD_plot | AM_plot\nsim_waffles <-\n  sim_waffles %>%\n  mutate(A = MedianAgeMarriage,\n         M = Marriage,\n         D = Divorce)\n\nsim_waffles_fit <- quap(\n  alist(\n    D ~ dnorm(mu , sigma) ,\n    mu <- a + bM*M + bA*A,\n    a ~ dnorm(0, 0.2),\n    bA ~ dnorm(0, 10),\n    bM ~ dnorm(0, 10),\n    sigma ~ dexp(1)\n  ), \n  data = sim_waffles,\n)\n\nprecis(sim_waffles_fit)##              mean         sd        5.5%       94.5%\n## a      0.01141493 0.02370058 -0.02646318  0.04929304\n## bA    -1.04399582 0.11019913 -1.22011532 -0.86787631\n## bM    -0.05011269 0.12299879 -0.24668851  0.14646313\n## sigma  0.16656407 0.01661272  0.14001374  0.19311440"},{"path":"spurious-association.html","id":"take-home-messages","chapter":"4 Multiple regression - Spurious association","heading":"4.4 Take-home messages","text":", two DAGs, two differently generated data sets, yet, one statistical model. DAGs agree direct causal path marriage age divorce opposite assumptions causal relationship marriage age rate. conclude ? might impossible understand causal relationships variables even simplest case just two predictors. thing embrace ambiguity aware whenever interpret regression models. , use DAGs exactly purpose understanding many ways can generate observed data. understand simple unambiguous story far appealing13 Jochen Braun Magdeburg says: “fall propaganda!”. Selecting just one story / DAG make true. Considering possible DAGs likely lead insights based predictions make. Even current data decide , different predictions future experiments help solve puzzle eventually.Another take home message: Regression models magic. can quantify correctional relationship signal--noise ratios know causality won’t tell model “true” model. Imagine never measured marriage age, model divorce rate marriage rate quite pleased results. , given noisy real data , probably consider important variables, presence might render age irrelevant marriage rate now. , assuming chain DAG\n\\[marriage~rate \\rightarrow age \\rightarrow variable~~missed \\rightarrow divorce~rate\\]\nStatistical models work small world created . Models golems, know large world. , understand limitations power. Models can help understand process investigating won’t understand !","code":""},{"path":"the-haunted-dag.html","id":"the-haunted-dag","chapter":"5 The haunted DAG","heading":"5 The haunted DAG","text":"notes section 6.3.2 haunted DAG demonstrates collider bias can arise, one variables unobserved realize collider DAG. DAG , ’ve changed Unobserved Neighborhood.example book shows include Grandparents “controlling ” influence Parents ignore influence Neighborhood, influence Grandparents negative rather zero. zero book can originally positive negative, point different inferred model. , importantly, estimated effect Parents also different true value. book shows visually found case algebra helpful understand level “regression model know”.Let us start full model. sake simplicity, ignore coefficients computing Parents variable Grandparents Neighborhood change general picture.\\[ P = G + N\\]\nThus, linear model child education \n\\[C = \\beta_P P + \\beta_G G + \\beta_N N\\]Substituting $ P = G + N$ gives us\n\\[C = \\beta_P (G + N) + \\beta_G G + \\beta_N N\\]Rearranging terms little\n\\[C = (\\beta_P + \\beta_G) G + (\\beta_P  + \\beta_N) N\\]Note means fit individual coefficients, cases fit sum two. , multicollinearity, individual coefficients can wrong place unreliable long add “true” coefficient value. Thus, ignoring noise concealing effect Parents, might well fit\n\\[C = \\beta_G^\\prime G + \\beta_N^\\prime N\\]\n, \\(\\beta_G^\\prime = \\beta_P + \\beta_G\\) \\(\\beta_N^\\prime = \\beta_P + \\beta_N\\) total effect grandparents neighborhood child education.ignore Neighborhood? means explicitly set \\(\\beta_N = 0\\) point sum coefficients starts causing us problems. Recall model fits \\(\\beta_P + \\beta_N\\) term separately. Thus, setting one 0 upset model, can always compensate coefficient. , coefficient \\(\\beta_P\\), value now “true” sum: \\(\\beta_P = \\beta_P^{true} + \\beta_N^{true}\\).Unfortunately us, \\(\\beta_P\\) appears two places, also used quantify effect grandparents:\n\\[(\\beta_P + \\beta_G) G\\]Originally, reflected influence parents, problem. now artificially inflated 14 stands influence parents neighborhood. Problem? model fits sum. make sure sum still adds ? change coefficients! , can still wiggle \\(\\beta_G\\) everything adds . Given \n\\[\\beta_P = \\beta_P^{true} + \\beta_N^{true}\\]\nmodel just needs subtract \\(\\beta_N^{true}\\) get sum back. \n\\[\\beta_P + \\beta_G = (\\beta_P^{true} + \\beta_N^{true}) + (\\beta_G^{true} - \\beta_N^{true})\\]Thus, explicitly model effect neighborhood, sneaks nonetheless, hiding inside parent grandparent coefficients. makes really problematic confusing effect opposite two terms: add \\(\\beta_N^{true}\\) one place, must subtract ., supposed new knowledge? take fitted model coefficients face value. always fine-print “assuming small world correct probably ” back head. think several models think ways tease true relationship. DAGs statistical models can help , magic tell “real truth” .","code":""},{"path":"information-criteria.html","id":"information-criteria","chapter":"6 Information Criteria","heading":"6 Information Criteria","text":"notes information criteria. purpose provide intuition information criteria, supplementing information presented chapter 7 Statistical Rethinking book Richard McElreath. deliberately oversimplified overgeneralized certain aspects paint “bigger picture”.","code":""},{"path":"information-criteria.html","id":"deviance","chapter":"6 Information Criteria","heading":"6.1 Deviance","text":"chapter, deviance introduced estimate KL-divergence, turn relative entropy, .e., difference cross-entropy actual entropy events. Keep mind look deviance straightforward goodness--fit measure, similar squared residuals (RMSE, Root Mean Square Error) coefficient determination \\(R^2\\). cases, difference model prediction (regression line) actual data point. ordinary least squares (OLS) approach, quantify imperfection prediction squaring residuals. sum residuals get sum squared residuals (\\(SS_{res}\\)) can compute \\(R^2\\) comparing total sum residuals data (\\(SS_{total}\\)):\n\\[R^2 = 1 - \\frac{SS_{res}}{SS_{total}} = \\frac{SS_{total} - SS_{res}}{SS_{total}}\\]\ntotal sum residuals \\(SS_{res}\\) gets close zero, fraction gets close 1. disadvantage squared residuals \\(R^2\\) tricky use non-metric data, binomial, ordered categorical data, etc., deviations prediction might asymmetric (binomial data, response times, etc.). Instead, can use likelihood compute probability data point comes distribution defined model. , compute (total) joint probability multiplying probabilities , better still, computing sum log-transform (log likelihood)15. , can compare observed log likelihood highest theoretically possible log likelihood saturated model (\\(\\Theta_s\\)), many parameters data points, predicts point perfectly. original definition (total) deviance:\n\\[D = -2 \\cdot (log(p(y|\\Theta)) - log(p(y|\\Theta_s)))\\]Recall \\(log(\\frac{}{b}) = log() - log(b)\\), can rearrange see log-ratio likelihoods:\n\\[D = -2 \\cdot  log \\left(\\frac{p(y|\\Theta)}{p(y|\\Theta_s)}\\right)\\]\n\\(p(y|\\Theta)\\) increases, fraction inside get closer 1. \\(log()\\) bit flips non-linearly scales . minus sign flips back end smaller numbers meaning better fit.\n\\(2\\) facilitate significance testing nested models. Models \\(\\Theta1\\) \\(\\Theta2\\) nested, \\(\\Theta2\\) predictors \\(\\Theta1\\) plus k predictors. E.g., model \\(Divorce = Marriage~Rate\\) nested inside \\(Divorce = Marriage~Rate + Marriage~Age\\) later model 1 parameter. actual model \\(\\Theta\\) nested inside saturated model \\(\\Theta_s\\) \\(k = n - k_{model}\\) parameters, \\(n\\) sample size \\(k_{model}\\) number parameters model. turns case, can determine whether difference goodness--fit two models significant using \\(\\chi^2\\) distribution k degrees freedom. catch log-ratio half magnitude, need \\(2\\) match things up16.point, might remember definition deviance single model book :\n\\[D = -2 \\cdot log(p(y|\\Theta))\\]Unfortunately, term can used ways, refer log likelihood single model , technically correctly, log-ratio saw . reality, mostly see deviance defined book used compare nested models via \\(\\chi^2\\) distribution ’ve described , difference compare two nested models, just saturated one. nested models frequently compared, example, see anova() function R.Note deviance single model still expresses idea goodness--fit merely normalized deviance saturated model. Thus deviance book \\(D = -2 \\cdot log(p(y|\\Theta))\\) corresponds sum residuals (absolute values mean nothing can use compare two models data), whereas total deviance corresponds \\(R^2\\) (values directly interpretable).","code":""},{"path":"information-criteria.html","id":"general-idea-information-criteria-as-miles-per-gallon","chapter":"6 Information Criteria","heading":"6.2 General idea: information criteria as miles-per-gallon","text":"general formula information criteria discussed \n\\[-2\\cdot log \\left( \\frac{goodness~~fit}{model~complexity} \\right)\\]goodness--fit numerator likelihood, joint probability observing model given data point \\(\\prod_i p(\\Theta|y_i)\\). denominator expresses model complexity, .e., flexibility fitting sample , therefore, tendency overfit. Thus, fraction goodness--fit per unit model complexity. like miles-per-gallon car efficiency, better models efficient models, churning goodness per complexity.numerator same17 information criteria discussed differ compute model complexity.","code":""},{"path":"information-criteria.html","id":"akaike-information-criterion-aic","chapter":"6 Information Criteria","heading":"6.3 Akaike Information Criterion (AIC)","text":"formula commonly used18 \n\\[ AIC = -2\\cdot log(p(\\Theta|y)) + 2k \\]\n\\(k\\) number parameters model. mathematician used translate back--forth using logarithms, may fail spot ratio talking earlier. , need keep mind \\(log(\\frac{}{b}) = log() - log(b)\\) \\(= log(exp())\\). Let us re-arrange bit get log-ratio back:\n\\[ AIC = -2\\cdot log(p(\\Theta|y)) + 2k \\]\n\\[ AIC = -2 (log(p(\\Theta|y)) - k)\\]\n\\[ AIC = -2 (log(p(\\Theta|y)) - log(exp(k))\\]\n\\[ AIC = -2 \\cdot log \\left(\\frac{p(\\Theta|y)}{exp(k)} \\right)\\]\n, log-ratio ’ve promised! can see, AIC assumes model complexity grows exponentially number parameters.use AIC, current recommendation correct extra penalty size sample\n\\[AICc = AIC + \\frac{2k^2 + 2k}{n - k - 1}\\]\n\\(n\\) sample size. won’t able work added exponent denominator.","code":""},{"path":"information-criteria.html","id":"bayesian-information-criterion-bic","chapter":"6 Information Criteria","heading":"6.4 Bayesian information criterion (BIC)","text":".k.. Schwarz information criterion (SIC, SBC, SBIC)19. motivation AIC penalty (complexity term), addition number parameters, also reflects sample size \\(n\\).\\[BIC = -2\\cdot log(p(\\Theta|y)) + log(n) k \\]Let us re-arranging \n\\[BIC = -2\\cdot log(p(\\Theta|y)) + log(n) k \\]\n\\[BIC = -2 \\left( log(p(\\Theta|y)) + log(n) \\frac{k}{2} \\right) \\]\n\\[BIC = -2 \\left( log(p(\\Theta|y)) - log \\left(exp(log(n) \\cdot \\frac{k}{2} \\right) \\right) \\]complexity term, need keep mind \\(exp(\\cdot b) = exp()^b\\). Thus,\n\\[exp \\left(log(n) \\cdot \\frac{k}{2} \\right)= exp(log(n)) ^ \\frac{k}{2} = n^\\frac{k}{2}\\]\nPutting complexity term back, get\n\\[BIC = -2 \\left( log(p(\\Theta|y)) - log \\left(n^\\frac{k}{2} \\right) \\right)\\]\n\\[BIC = -2 \\cdot log \\left(\\frac{p(\\Theta|y)}{n^\\frac{k}{2}} \\right)\\]\nThus, end similar power law complexity term uses sample size instead Euler’s number base.","code":""},{"path":"information-criteria.html","id":"problem-of-aic-and-bic-one-size-may-not-fit-all","chapter":"6 Information Criteria","heading":"6.5 Problem of AIC and BIC: one size may not fit all","text":"AIC BIC assume model complexity flexibility, leads overfitting, reflected number parameters k. However, fairly indirect measure model flexibility, based models general tend overfit data general. probably want know specific model uses parameters fit specific sample much overfitting expect specific case. even parameter present model, may able fully use case regularization multilevel (adaptive regularization) models.Regularization, form strong priors, lasso/ridge regression, etc., restricts range values given parameter can take. Thus, model exploit much parameters less able use improve fit sample. Similarly, hierarchical multilevel modeling, may dozens hundreds parameters describe intercepts /slopes individual participants (random factors, general), trivially zero (similar group average) contribute little actual fit. cases, simple raw count, treats parameters equals, overestimate model complexity.desire go beyond one-size-fits-approach model- data-specific led development deviance information criterion (DIC) widely-applicable information criterion (WAIC). use entire posterior distribution -sample deviance base penalty variable posterior distribution . Higher variance, meaning model produces different fits ranging excellent terrible, hints model flexible good leads higher complexity estimate (penalty overfitting). Conversely, similar posterior deviance (low variance) means model restricted fine-tune sample complexity low.understand variance posterior distribution divergence related models’ flexibility , therefore, number effective parameters, just skip next section. , came musical instruments metaphor least people ’ve tested upon found useful.","code":""},{"path":"information-criteria.html","id":"musical-instruments-metaphor","chapter":"6 Information Criteria","heading":"6.6 Musical instruments metaphor","text":"Imagine trying play song just heard. instrument triangle. particularly flexible instruments pitch-wise, rendition song good (model underfits data). good news even worst really try, one notice worst performance sound much like best one. Simply hard make songs sound different using triangle. Thus, play song many times, trying different versions us judging close original, score give probably particularly high similar (low variance deviance fitting sample).give instrument can vary pitch least bit, like xylophone children. Now freedom version music sound much like original. , also gives opportunity make mess , rendition might sound nothing like music ’ve just heard. words, flexbile instrument increases difference best worst possible performance, variance performances (close original) also increases (higher variance deviance). flexible instrument make difference even bigger. Think violin trombone restricted scale, can play sound -can match music just heard exactly. Imagine music just heard odd --scale sounds. defect turntable, go constant speed, overall pitch wobbles overtime (noise)? experimental music piece deliberately designed sound odd (signal)? know sure, try play close original music heard possible, matching -scale sounds. , can play sound, range possible performance even larger one--one “please, mercy stop!” (even larger variance deviance).short, variance performance (posterior divergence) reflects flexible instrument . indicative effective number parameters? regularizing priors world music instrument metaphor. Imagine addition triangle, also give rubber bell20. Now, technically two instruments (number parameters 2) bell affect performance (put strong regularizing priors coefficients zero something -close zero). Thus, ability play song change variance performance stays . Two actual instruments, one “effective” one. , give piano allow use one octave white keys. Yes, piano regularization complex kids’ xylophone. potential number notes can play great (AIC BIC impressed slap heavy penalty ) actual “effective” range small. , regularize play scale-notes using violin (something learn ). cases, deliberately restrict . ? just play heard ? .e., fit well can? song heard short (sample small), regularization based knowledge real life helps ignore noise always present. E.g., know song kids’ xylophone, even heard notes outside single octave probably problem recording. , never heard piece violin know works composer always use scale-notes, use violin play -scale sounds case.Multilevel models also limit actual use parameters. Imagine heard recording symphonic orchestra. Lots violins figured actually play melody. can get away using one violin score (sample group average) assume violins play like (participants close group average). deviations group melody probably mistakes individual musicians, actual melody. goes hear choir. , many people sing (lots parameters!) , mostly, unison, need create individual score sheet (parameter) singer, just one per group singers.Wrapping metaphor, flexible instrument , variable performance can , easier mimic noise imperfections recording nothing piece . play next time, matching recording noise distortions perfectly, people know piece puzzled may even recognize (poor --sample predictions). Adopting melody limited instrument may make easier others recognize song! Thus, higher variance performance accuracy (higher variance deviance) indicates can overfit easily instrument (model) extra careful (impose higher penalty complexity).","code":""},{"path":"information-criteria.html","id":"deviance-information-criterion-dic-and-widely-applicable-information-criterion-waic","chapter":"6 Information Criteria","heading":"6.7 Deviance information criterion (DIC) and widely-applicable information criterion (WAIC)","text":"two similar, compute model complexity based posterior distribution log likelihood. key difference DIC sums log likelihood model (sample) first computes variance samples. WAIC computes variance log likelihood per point sums variances . musical instrument metaphor, DIC perform piece many times (generate many posterior samples), compute accuracy performance (deviance single sample), compute variable . WAIC, go note note (observation observation). note compute variance samples see consistent playing . , sum .\\[DIC = -2 \\cdot \\left( log(p(y|\\Theta)) - var(\\sum log(p(y|\\Theta_i))) \\right)\\]\n\\[WAIC = -2 \\cdot \\left( log(p(y|\\Theta)) - \\sum_i var(log(p(y_i|\\Theta))) \\right)\\]penalty replaces \\(k\\) AIC , therefore, go exponent inside ratio. , idea, increase variance deviance (either per sample DIC per point WAIC) leads exponentially increasing estimate complexity.WAIC stable mathematically mode widely applicable (’s statisticians tell us). Moreover, advantage explicitly recognizes data points sample equal. (outliers) much harder predict others. variance log likelihood points determines much model can overfit. inflexible model make poor consistent job (triangles don’t care pitch!), whereas complex model can anything spot-terrible (violins can anything). short, use WAIC recognize DIC see think somewhat less reliable WAIC, still better AIC BIC use regularizing priors /hierarchical models.","code":""},{"path":"information-criteria.html","id":"importance-sampling","chapter":"6 Information Criteria","heading":"6.8 Importance sampling","text":"Importance sampling mentioned chapter never explained, brief description. core idea pretend sample distribution need (access sampling directly inefficient) sampling another distribution (one access can sample efficiently) “translating” probabilities via importance ratios. mean?Imagine want know average total score given die throw ten times. procedure simple gets: toss die ten times, record number get throw, sum end. Repeat toss-ten-times--sum--many times want compute average. access die die kept lock International Bureau Weights Measures? Well, die can toss list importance ratios number. important ratios tell much likely number die (one ) compared die hand. Let’s say importance ratio 1 (, number 1 comes top) 3.0. means whenever die gives 1, assume die came 1 three throws. importance ratio 2 0.5, whenever see 2 die, record half throw (2 comes twice rarely real die die, two throws give 2 amount single throw). way can toss die every toss equates different number throws generated number die. , sample die record outcomes die. Funny thing don’t even need know fair die probability individual sides. long know importance ratios, keep tossing translating probabilities, get samples die interested .Note toss die ten times, translated number tosses die need add ten. Imagine , just chance, got 1 four times. Given importance ratio 3.0 alone translates twelve tosses. Solution? normalize result sum importance ratios get back ten tosses.obvious catch , know importance ratios? Well, situation specific. Sometimes, can compute know distributions, just one easier sample target one, , optimize use computing power21. Sometimes, case PSIS/LOO , can use approximation.","code":""},{"path":"information-criteria.html","id":"pareto-smoothed-importance-sampling-leave-one-out-cross-validation-psisloo","chapter":"6 Information Criteria","heading":"6.9 Pareto-smoothed importance sampling / leave-one-out cross-validation (PSIS/LOO)","text":"importance sampling ’ve described key PSIS/LOO. idea , want sample posterior model fitted without specific data point \\(y_i\\) (write \\(p(\\Theta_{-,s}|y_i)\\)). really want refit model. Instead, want use already , samples model fit data, including point \\(y_i\\). , wise minds figured use importance sampling trick, sampling \\(p(\\Theta_s|y_i)\\) translating \\(p(\\Theta_{-,s}|y_i)\\). thing need importance factors\n\\[r_i = \\frac{p(\\Theta_{-,s}|y_i)}{p(\\Theta_s|y_i)} \\propto \\frac{1}{p(y_i|\\Theta_s)}\\]importance ratio tells worse predicting point \\(y_i\\) -sample (smaller \\(p(y_i|\\Theta_s)\\) ), important consider model’s performance --sample (larger \\(\\frac{1}{p(y_i|\\Theta_s)}\\)). intuition behind . observation harder predict, included data model trained . , absence model use parameters fit data present, including fitting noise, spare parameters. , expect --sample deviance (\\(p(y_i|\\Theta_{-}\\)) always worse -sample deviance observation (\\(p(y_i|\\Theta\\)). much worse depends “typical” observation . typical “easy” model predict, absence model still see many similar “typical” observations well prepared predict . However, observations atypical, outlier, model won’t see many observations alike concentrate typical points.specifically, penalty particular point based importance ratios across samples reflects variable \\(p(y_i|\\Theta_s)\\) across samples. Recall definition book\n\\[lppd_{} = \\sum^N_{=1} log \\frac{\\sum^S_{s=1}r(\\theta_s)p(y_i|\\theta_s)}{\\sum^S_{s=1}r(\\theta_s)}\\]\nSubstituting \\(r_i = \\frac{1}{p(y_i|\\Theta_s)}\\) get\n\\[lppd_{} = \\sum^N_{=1} log \\frac{\\sum^S_{s=1}\\frac{1}{p(y_i|\\Theta_s)}p(y_i|\\theta_s)}{\\sum^S_{s=1}\\frac{1}{p(y_i|\\Theta_s)}}\\]\n\\[lppd_{} = \\sum^N_{=1} log \\frac{\\sum^S_{s=1}\\frac{p(y_i|\\theta_s)}{p(y_i|\\Theta_s)}}{\\sum^S_{s=1}\\frac{1}{p(y_i|\\Theta_s)}}\\]\n\\[lppd_{} = \\sum^N_{=1} log \\frac{\\sum^S_{s=1}1}{\\sum^S_{s=1}\\frac{1}{p(y_i|\\Theta_s)}}\\]\\[lppd_{} = \\sum^N_{=1} log \\frac{S}{\\sum^S_{s=1}\\frac{1}{p(y_i|\\Theta_s)}}\\]\n\\[lppd_{} = \\sum^N_{=1} log \\frac{1}{\\frac{1}{S}\\sum^S_{s=1}\\frac{1}{p(y_i|\\Theta_s)}}\\]looks like bizarre re-arrangement terms ends producing point-wise variance-based penalty similar WAIC. can build intuition playing easy compute numbers. Let us take vector values , .e., variance zero compute \\(lppd\\) \\(lppd_{}\\).Now let us keep mean slightly increase variance. can see tiny increase sample induces small decrease \\(lppd_{}\\), .e., assume since model variable point, probably much power leading -sample overfitting , therefore, poor --sample performance.understand penalty comes , helpful derive solution case just two values \\([p_1, p_2]\\). case can define difference mean, just : \\(p_1 = \\mu - \\epsilon\\) \\(p_2 = \\mu + \\epsilon\\). \\(lppd\\) trivially equal \\(\\mu\\):\n\\[lppd = \\frac{p_1 + p_2}{2}\\]\n\\[lppd =\\frac{(\\mu - \\epsilon) + (\\mu + \\epsilon)}{2}\\]\n\\[lppd =\\frac{2 \\cdot \\mu}{2}\\]\n\\[lppd = \\mu\\]\\(lppd_{}\\)?\n\\[lppd_{} = \\frac{2}{\\frac{1}{p_1} + \\frac{1}{p_2}}\\]\\[lppd_{} = \\frac{2}{\\frac{1}{\\mu - \\epsilon} + \\frac{1}{\\mu + \\epsilon}}\\]Bringing two fractions common denominator get\n\\[lppd_{} = \\frac{2}{\\frac{(\\mu + \\epsilon) + (\\mu - \\epsilon)}{(\\mu + \\epsilon)(\\mu - \\epsilon)}}\\]\nOpening brackets numerator\n\\[lppd_{} = \\frac{2}{\\frac{2 \\cdot \\mu }{(\\mu + \\epsilon)(\\mu - \\epsilon)}}\\]\nNow can get flip bottom fraction\n\\[lppd_{} = \\frac{2 \\cdot \\frac{(\\mu + \\epsilon)(\\mu - \\epsilon)}{2 \\cdot \\mu}}{\\frac{2 \\cdot \\mu }{(\\mu + \\epsilon)(\\mu - \\epsilon)} \\cdot \\frac{(\\mu + \\epsilon)(\\mu - \\epsilon)}{2 \\cdot \\mu}} \\]\\[lppd_{} = 2 \\cdot \\frac{(\\mu + \\epsilon)(\\mu - \\epsilon)}{2 \\cdot \\mu}\\]\ntwo goes away\n\\[lppd_{} = \\frac{(\\mu + \\epsilon)(\\mu - \\epsilon)}{\\mu}\\]\nOpening brackets numerator\n\\[lppd_{} = \\frac{\\mu^2 + \\mu\\epsilon - \\mu\\epsilon - \\epsilon^2}{\\mu}\\]\nSimplifying\n\\[lppd_{} = \\frac{\\mu^2 - \\epsilon^2}{\\mu}\\]\n\\[lppd_{} = \\mu - \\frac{\\epsilon^2}{\\mu}\\]Thus variance (\\(\\epsilon\\), deviation mean) increases, \\(lppd_{}\\) decreasesCompare \\(var log p(y_i|\\Theta_s)\\), WAIC penalty term, decrease due variance-based importance ratios. can see two close , therefore, produce similar estimates --sample performance. Pareto smoothed importance sampling advantage WAIC less keen reducing performance based samples, smoothed away. Still, methods identify problematic (high variance) data points pay closer attention .","code":"\np <- c(0.2, 0.2, 0.2) # mean = 0.2, variance = 0\nlogp <- log(p)\nlppd <- log(sum(p)/length(p))\nlppd_IS <- log(1 / ( (1 / length(p)) * sum( 1/ p)))## mean = 0.2, variance of log(p) = 0## lppd    = -1.609438## lppd_IS = -1.609438## lppd - lppd_IS =  0\np <- c(0.19, 0.2, 0.21)\nlogp <- log(p)\nlppd <- log(sum(p)/length(p))\nlppd_IS <- log(1 / ( (1 / length(p)) * sum( 1/ p)))## mean = 0.2, variance of log(p) = 0.001669798## lppd    = -1.609438## lppd_IS = -1.611107## lppd - lppd_IS =  0.001669449\ndelta <- seq(0.0, 0.19, length.out = 100)\n\ncreate_p <- function(dp) {0.2 + c(-dp, 0, dp)}\n\nlppd_df <- bind_rows(\n  tibble(Variance = purrr::map_dbl(delta, ~var2(log(create_p(.)))),\n         LPPD = purrr::map_dbl(delta, ~log(sum(create_p(.))/length(create_p(.)))),\n         Kind = \"lppd\"),\n  tibble(Variance = purrr::map_dbl(delta, ~var2(log(create_p(.)))),\n       LPPD = purrr::map_dbl(delta, ~log(1 / ( (1 / length(create_p(.))) * sum( 1/ create_p(.))))),\n       Kind = \"lppd_IS\")) %>%\n  mutate(SD = sqrt(Variance))\n\ntogether_plot <- \n  ggplot(lppd_df, aes(x=Variance, y=LPPD, color=Kind)) + \n  geom_line() + \n  xlab(\"var log p (WAIC penalty)\")\n\ndifference_plot <-\n  lppd_df %>%\n  pivot_wider(names_from = Kind, values_from = LPPD) %>%\n  mutate(`lppd_IS - lppd` = lppd_IS - lppd) %>%\n  ggplot(aes(x=Variance, y=`lppd_IS - lppd`)) + \n  geom_line() +\n  xlab(\"var log p (WAIC penalty)\")\n\ntogether_plot + difference_plot"},{"path":"information-criteria.html","id":"bayes-factor","chapter":"6 Information Criteria","heading":"6.10 Bayes Factor","text":"information criterion. However, popular way compare Bayesian models. Compared information criteria, logic reversed. case information criteria, asking model fits data best given penalty impose complexity. case Bayes Factor, already two models (different models different number parameters just different parameter values) interested well data matches models already .Let’s start Bayes theorem:\n\\[Pr(M|D)=\\frac {\\Pr(D|M)\\Pr(M)}{\\Pr(D)}\\]\n, D data M model (hypothesis). tricky part marginal probability (prior) data \\(Pr(D)\\). hardly ever know sure, making computing “correct” value \\(Pr(M|D)\\) problematic. using posterior sampling, side-step issue ignoring normalizing posterior sum posterior distribution. Alternatively, comparing two models, can compute ratio:\n\\[{\\frac {\\Pr(D|M_{1})}{\\Pr(D|M_{2})}}={\\frac {\\Pr(M_{1}|D)}{\\Pr(M_{2}|D)}} \\cdot {\\frac {\\Pr(M_{1})}{\\Pr(M_{2})}}\\]\n\\(\\frac{\\Pr(D|M_{1})}{\\Pr(D|M_{2})}\\) posterior odds, \\(\\frac {\\Pr(M_{1}|D)}{\\Pr(M_{2}|D)}\\) Bayes Factor, \\(\\frac {\\Pr(M_{2})}{\\Pr(M_{1})}\\) prior odds. common \\(Pr(D)\\) nicely cancels !assume hypotheses/models equally likely (flat priors), prior odds 1:1 posterior odds equal Bayes Factor , vice versa, Bayes Factor equal posterior odds. means can just pick likelihoods posterior sampled distribution compute ratio.big fan Bayes Factor conceptual reasons. Although can compare two models (long sample ), looks lot like Bayesian version p-value , therefore, lends naturally null-hypothesis testing. , far reading literature field concerned, people frequently use , cooler Bayesian way null-hypothesis testing. worries multiple comparisons (Bayesian, need error correction!) can prove null hypothesis (ratio, flip see much stronger H0 )! nothing wrong per se advantage Bayesian statistics information criteria need think terms null hypothesis testing nested models. Adopting Bayes Factor may prevent seeing allow continue analysis just differently colored wrapper. , nothing wrong exploratory analysis using null hypothesis testing can formulate better model. way approach modeling.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"bayesian-vs.-fequentist-statisics","chapter":"7 Bayesian vs. fequentist statisics","heading":"7 Bayesian vs. fequentist statisics","text":"suspect many student read “Statistical Rethinking” feeling something completely different learning “traditional” statistics classes. Bayesian approach “hands-” complicated, whereas “normal” statistics simpler easy work even “less powerful.”22 Thus, purpose note walk typical statistical analysis focus practical differences , importantly, similarity two approaches.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"choice-of-likelihood-both","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.1 Choice of likelihood (both)","text":"first look data decide distribution use model data / residuals normal, binomial, Poisson, beta, etc. first line models goes like \n\\[\n\\color{red}{y_i \\sim Normal(\\mu_i, \\sigma)} \\\\\n\\mu_i = \\alpha + \\beta_{x1} \\cdot X1 + \\beta_{x2} \\cdot X2 + \\beta_{x1\\cdot x2} \\cdot X1 \\cdot X2 \\dotso \\\\\n\\alpha \\sim Normal(0, 1) \\\\\n\\beta_{x1} \\sim Exponential(1) \\\\\n\\cdots \\\\\n\\sigma \\sim Exponential(1)\n\\]decision neither Bayesian, frequentist. decision model best describes data, independent inference method use. decision making even using “prepackaged” statistical tests like t-test ANOVA assume normally distributed residuals23.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"linear-model-both","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.2 Linear model (both)","text":"Next, decide deterministic part model expresses parameter distribution chose previous step computed various predictors. E.g., linear regression normally distributed residuals, decide predictors use compute mean. model line look something like \n\\[\ny_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\color{red}{\\mu_i = \\alpha + \\beta_{x1} \\cdot X1 + \\beta_{x2} \\cdot X2 + \\beta_{x1\\cdot x2} \\cdot X1 \\cdot X2 \\dotso} \\\\\n\\alpha \\sim Normal(0, 1) \\\\\n\\beta_{x1} \\sim Exponential(1) \\\\\n\\cdots\n\\]\n, neither Bayesian, frequentist decision, linear model decision. Chapters 4-6 8 concentrate make decision using directed-acyclic graphs (DAGs) introduce concepts multicollinearity, colliders bias can produce, backdoor paths identify , etc. explain can make educated decision predictors use based knowledge field problem. stage also decide whether normalize data, make interpreting model easier.always make decision. example, use (repeated measures) ANOVA, need decide factors use, whether include interactions, transform data make coefficients directly interpretable, use link function, etc.24","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"priors-optional-for-bayesian","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.3 Priors (optional for Bayesian)","text":"Priors Bayesian way regularize model, something need think Bayesian statistics25. model part look something like \n\\[\ny_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta_{x1} \\cdot X1 + \\beta_{x2} \\cdot X2 + \\beta_{x1\\cdot x2} \\cdot X1 \\cdot X2 \\dotso \\\\\n\\color{red}{\\alpha \\sim Normal(0, 1) \\\\\n\\beta_{x1} \\sim Exponential(1) \\\\\n\\cdots}\n\\]probably decision students worry feels subjective arbitrary decisions, choice likelihood predictors. Chapter 4 7 gave multiple examples nothing particularly arbitrary choices can come set justifiable priors based know topic based pre-processed data26.Still, think lot people “normal” statistics flat priors feels simpler also objective , therefore, trustworthy (“favor specific range values!”). case use flat priors (see side note ) making Bayesian frequentists models identical! , though, writing explicitly makes one realize range \\(-\\infty, +\\infty\\) remarkably large point obvious overkill\n\\[\ny_i \\sim Normal(\\mu_i, \\sigma) \\\\\n\\mu_i = \\alpha + \\beta_{x1} \\cdot X1 + \\beta_{x2} \\cdot X2 + \\beta_{x1\\cdot x2} \\cdot X1 \\cdot X2 \\dotso \\\\\n\\color{red}{\\alpha \\sim Uniform(-\\infty, +\\infty) \\\\\n\\beta_{x1} \\sim Uniform(-\\infty, +\\infty) \\\\\n\\cdots}\n\\]short, Bayesian inference gives option specify priors. need take option can use flat frequentist’s priors.Side note. reality, flat priors never good priors. sufficient data , cases, priors (flat ) much influence. However, data limited flat priors almost inevitably lead overfitting additional information counteract effect noise. overfitting may feel “objective” “data-driven” conservative underfitting data via strongly regularizing priors latter likely lead better --sample predictions , therefore, likely replicated.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"maximum-likelihood-maximum-a-posteriori-estimate-both","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.4 Maximum-likelihood / Maximum A Posteriori estimate (both)","text":"fitted model, get estimates parameter specified. opted flat priors, estimates minor differences due sampling Bayesian statistics. specify regularizing priors MAP estimates different MLE, although magnitude difference depend amount data: data , smaller influence priors, closer estimates (see also side note ). Importantly, types inferences produce (similar) estimates interpret values way.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"uncertainty-about-estimates-different-but-comparable","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.5 Uncertainty about estimates (different but comparable)","text":"point two approach fundamentally diverge. case frenquetists statistics obtain confidence intervals p-values based appropriate statistics degrees freedom, whereas case Bayesian inference obtain credible/compatibility intervals can use posterior distribution individual parameters compute probability strictly positive, negative, concentrated within certain region around zero, etc.measures conceptually different tend interpreted similarly mostly Bayesian perspective. think good idea compute report . close, make certain results. importantly, whenever diverge serves warning investigate case can cause difference.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"model-comparison-via-information-criteria-both","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.6 Model comparison via information criteria (both)","text":"approaches use information criteria compare models Akaike Bayesian/Schwarz information criteria developed specifically case flat priors frenquetist models. , Bayesian approach holds advantage full posterior allows elaborate information criteria DIC, WAIC, LOO. Still core idea interpretation comparison results .","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"generating-predictions-both","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.7 Generating predictions (both)","text":"generate predictions using model definition approach. Hence, going get similar predictions, least mean (depending priors, see MLE vs. MAP ). two approach differ characterize uncertainty, uncertainty predictions different , typically, comparable.","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"conclusions","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.8 Conclusions","text":"can see, practical point view, apart optional Bayesian priors different ways quantify uncertainty estimates, two approaches . require making decisions results interpreted way. lack difference becomes even apparent use software packages running statistical models. E.g., way specify model lme4 (frenquetist) brms (Bayesian) much point cases need change function name (lmer brm vice versa) leave rest .Thus, point choosing studying frenquetists Bayesian statistic. feel comfortable Bayesian, mostly makes easier interpret statistical significance. However, typical approach start frenquetist statistics (fast, good shooting hip), certain decisions (likelihood, model) re-impliement model Bayesian using informative priors see whether results match (reason). , report sets inferences. costs remarkably little time precisely little difference two approaches practical point view!","code":""},{"path":"bayesian-vs.-fequentist-statisics.html","id":"take-home-message","chapter":"7 Bayesian vs. fequentist statisics","heading":"7.9 Take home message","text":"studying something completely different! merely approaching unusual angle leads deeper understanding interesting insights long run.","code":""},{"path":"mixtures.html","id":"mixtures","chapter":"8 Mixtures","heading":"8 Mixtures","text":"","code":""},{"path":"mixtures.html","id":"beta-binomial","chapter":"8 Mixtures","heading":"8.1 Beta Binomial","text":"Beta binomial defined product binomial beta distributions.\n\\[BetaBinomial(k|N, p, \\theta) = Binomial(k|N,p) \\cdot Beta(p|\\beta_1, \\beta_2),\\]\n\\(k\\) number successes (e.g., “heads” coin toss), \\(N\\) total number trials/draws, \\(p\\) probability success), \\(\\beta_1\\) \\(\\beta_2\\) determine shape beta distribution. book uses reparametrized version beta distribution, sometimes called beta proportion:\n\\[BetaBinomial(k|N, p, \\theta) = Binomial(k|N,p) \\cdot Beta(p|p, \\theta),\\]\n\\(\\theta\\) precision parameter. \\(p\\) \\(\\theta\\) can computed \\(\\beta_1\\) \\(\\beta_2\\) \n\\[\np = \\frac{\\beta_1}{\\beta_1 + \\beta_2}\\\\\n\\theta = \\beta_1 + \\beta_2\n\\]\nlatter form makes intuitive look code dbetabinom(), see can use shape1 shape2 parameters instead probe theta.Recall (unnormalized) Bayes rule (\\(p\\) probability, \\(y\\) outcome, \\(...\\) parameters prior distribution):\n\\[\nPr(p | y) = Pr(y | p) \\cdot Pr(p | ...)\n\\]\nExamine formula can see can think beta binomial posterior distribution binomial likelihood beta distribution prior parameter \\(p'\\) binomial distribution:\\[BetaBinomial(N, p, \\theta | k) = Binomial(k|N,p) \\cdot Beta(p| p_{mode}, \\theta)\\]Thus, beta binomial combination binomial distributions weighted beta distribution mode \\(p_{mode}\\) width determined \\(\\theta\\). words, use binomial distribution alone, state can compute probability directly \\(p = \\text{linear model}\\). , state knowledge incomplete , best, can predict mode beta distribution probability comes let data determine variance/precision (\\(\\theta\\)) distribution. Thus, posterior reflect two uncertainties based two loss functions: one number observed events (many counts compatible given \\(p\\) different probabilities), binomial, plus another one \\(p\\) (many values \\(p\\) compatible given \\(p_{mode}\\) \\(\\theta\\)). allows model compute trade-considering values \\(p\\) less likely prior point view (away \\(p_{mode}\\)) result higher probability \\(k\\) given chosen \\(p\\). see trick later book, use incorporate uncertainty measured value (.e., best, can say actual observed value comes distribution mean standard deviation).practical terms, means parameter \\(\\theta\\) controls width distribution (see plots ). \\(\\theta\\) approaches positive infinity, prior uncertainty \\(p\\) reduced zero, means now consider one binomial distribution, \\(p' = p\\), equivalent simple binomial distribution. Thus, beta binomial narrow binomial distribution.","code":""},{"path":"mixtures.html","id":"negative-binomial-a.k.a.-gamma-poisson","chapter":"8 Mixtures","heading":"8.2 Negative binomial, a.k.a. Gamma Poisson","text":"idea : enough information compute rate events, instead, compute mean Gamma distribution rates come let data determine variance (scale). , practical terms means smallest scale uncertainty rate minimal distribution matches Poisson processes fixed rate. increase uncertainty (larger values scale parameter), mean broader distribution capable account extreme values.","code":""},{"path":"mixtures.html","id":"ordered-categorical","chapter":"8 Mixtures","heading":"8.3 Ordered categorical","text":"log odds logit link.\n\\[\nlog(\\frac{Pr(y_i \\leq k)}{1 - Pr(y_i \\leq k)}) = \\alpha_k \\\\\n\\frac{Pr(y_i \\leq k)}{1 - Pr(y_i \\leq k)} = e^{\\alpha_k} \\\\\nPr(y_i \\leq k) = e^{\\alpha_k} \\cdot ( 1 - Pr(y_i \\leq k)) \\\\\nPr(y_i \\leq k) \\cdot ( 1 + e^{\\alpha_k}) = e^{\\alpha_k} \\\\\nPr(y_i \\leq k) = \\frac{e^{\\alpha_k}}{1 + e^{\\alpha_k}}\n\\]","code":"\ndf_p <-\n  tibble(alpha = seq(-10, 10, length.out=100)) %>%\n  mutate(p = exp(alpha) / (1 + exp(alpha)))\n\nggplot(df_p, aes(x=alpha, y=p)) + \n  geom_line()\nequal_probability <- log(((1:6) / 7) /  (1 - (1:6) / 7))\n  \ndf_ord_cat <- \n  tibble(Response = 1:7, \n         p = rethinking::dordlogit(1:7, 0, equal_probability),\n         Label = \"Equal probability\") %>%\n  bind_rows(tibble(Response = 1:7, \n         p = rethinking::dordlogit(1:7, 0, equal_probability - 1),\n         Label = \"Beta = 1\")) %>%\n  bind_rows(tibble(Response = 1:7, \n         p = rethinking::dordlogit(1:7, 0, equal_probability + 2),\n         Label = \"Beta = -2\")) %>%\n  mutate(Label = factor(Label, levels = c(\"Beta = 1\", \"Equal probability\", \"Beta = -2\")))\n\ndf_cuts <-\n  tibble(Response = 1:6, \n         K = equal_probability,\n         Label = \"Equal probability\") %>%\n  bind_rows(tibble(Response = 1:6, \n         K = equal_probability - 1,\n         Label = \"Beta = 1\")) %>%\n  bind_rows(tibble(Response = 1:6, \n         K = equal_probability + 2,\n         Label = \"Beta = -2\")) %>%\n  mutate(Label = factor(Label, levels = c(\"Beta = 1\", \"Equal probability\", \"Beta = -2\")))\n  \n\ncuts_plot <-\n  ggplot(data=df_cuts,) + \n  geom_vline(aes(xintercept = K, color=Label), show.legend = FALSE) +\n  facet_grid(Label~.) +\n  scale_x_continuous(\"Odds ratio\")\n\nprob_plot <- \n  ggplot(data=df_ord_cat, aes(x=Response, y=p, color=Label)) + \n  geom_point() + \n  geom_line()\n\ncuts_plot + prob_plot"},{"path":"instrumental-variables.html","id":"instrumental-variables","chapter":"9 Instrumental Variables","heading":"9 Instrumental Variables","text":"","code":""},{"path":"instrumental-variables.html","id":"disclaimer","chapter":"9 Instrumental Variables","heading":"Disclaimer","text":"understanding instrumental variables unicorns. immensely powerful can magically turn observational study , effectively, randomize experiment enabling infer causality. rare, perhaps, even rarer unicorns , example find based (similar) data.also often described source “natural randomization”, yet, best examples found (effect military experience career/wages, effect studying charter school math skills) involved deliberate randomization form lottery can conveniently use.","code":""},{"path":"instrumental-variables.html","id":"can-we-estimate-an-effect-of-military-experience-on-wages","chapter":"9 Instrumental Variables","heading":"Can we estimate an effect of military experience on wages","text":"One examples, can find online, effect military experience wages. Conceptually, everything simple, just want know direct effect military experience wages.However, practice decision join military choices career can cause set unobserved factors. come family military background, affect decision join military kind career want pursue. patriotic want serve country, might opt military lower-earning career public sector. Conversely, interested becoming successful lawyer, going military might hindrance help. short, backdoor path unobserved variables easy way close .","code":""},{"path":"instrumental-variables.html","id":"draft-as-an-instrumental-variable","chapter":"9 Instrumental Variables","heading":"Draft as an instrumental variable","text":"able perform randomized experiment, things easy: Take similar people randomly send / send military, observe effect career wages. Now, , perhaps, can find situation actually done use advantage. makes whole situation sort hybrid: observe outcome randomization someone else purposesIn case, Vietnam war people drafted based assigned draft numbers. latter determined date birth relationship birth day within year draft number randomized lottery. addition, order within day randomized lottery initials. purpose create perfect randomization background (apart male eligible military service) effect whether drafted. , although lottery created address inequalities, created almost perfect randomized experiment us use. Almost, people early numbers avoided drafted various means, whereas people later draft numbers volunteered enlisted.DAG creates followingAs can see, still able close backdoor path military experience wages randomization perfect. see can still overcome problem using two-stage least squares (default approach find almost everywhere, referred “two-stage worst squares” McElreath) using covariance residuals (book).","code":""},{"path":"instrumental-variables.html","id":"two-stage-least-squares","chapter":"9 Instrumental Variables","heading":"Two-stage least squares","text":"main idea two split DAG two halves estimated effects one one, hence, “two-stage”. First, use draft number predict military experience.linear model \n\\[\\sim Normal(\\widehat{}, \\sigma_{})\\\\\n\\widehat{} = \\alpha_{} + \\beta_{DN} \\cdot DN\\]\\(\\alpha_M\\) arbitrary intercept term, \\(\\beta_{DN}\\) encode main effect draft number, \\(\\sigma\\) standard deviation residuals. can also write model different format, likely encounter tutorials lectures:\n\\[\\widehat{} = \\alpha_{} + \\beta_{DN} \\cdot DN + \\epsilon_{}\\]\n, difference predicted military experience (note hat \\(\\widehat{}\\)) actual one described \\(\\epsilon_{}\\): residuals come normal distribution centered 0 standard deviation \\(sigma_{}\\), implicit format. Note versions express variables encode dependence differ whether residuals (\\(epsilon\\)) standard deviation distribution come (\\(sigma\\)) implicit explicit.draft number independent unobserved variables due lottery, predicted military experience also independent . dependence gets transferred residuals instead, residuals variance signal explain (point become important later ).Now, second stage, use predicted military experience, independent unobserved variables , therefore, need close backdoor path!model , , simple note two-stage least squares using predicted military experience \\(\\widehat{}\\)!\n\\[CW \\sim Normal(\\widehat{CW}, \\sigma_{CW})\\\\\n\\widehat{CW} = \\alpha_{CW} + \\beta_{} \\cdot \\widehat{CW}\\\\\n\\\\\n\\widehat{CW} = \\alpha_{CW} + \\beta_{} \\cdot \\widehat{} + \\epsilon_{CW}\n\\], dependence unobserved variables career/wages gets offloaded residuals \\(\\epsilon_{CW}\\) get nice clean direct effect predicted military experience. Importantly, interpretation equate effect predicted military experience effect actual one. , course, one big elephant room need certain \\(\\widehat{}\\) indeed accurate. Unfortunately, , inferences incorrect back square one.","code":""},{"path":"instrumental-variables.html","id":"covarying-residuals","chapter":"9 Instrumental Variables","heading":"Covarying residuals","text":"approach presented book allows use actual military experience predictor still solve problem backdoor path. initial idea : Let us use instrumental variable compute “pure” military experience, uncontaminated unobserved variables.\\[\\widehat{} = \\alpha_{} + \\beta_{DN} \\cdot DN + \\epsilon_{}\\], variance due unobserved variables accumulated residuals (\\(\\epsilon_{}\\)), correlated . latter part trivially true residuals always correlated “noise”, precisely defined influence unobserved variables measure , therefore, explicitly account . measure include , observe correlation explicitly. Alas, measure , therefore, typical model fact residuals correlated unobserved variables immediate practical value (beyond checking exchangeability).Now, imagine compute just prediction first stage pure randomized military experience correlated unobserved variables. Obviously, , otherwise worry backdoor path, know linear model includes magic pure randomized military experience predictor? true direct effect career/wages independent unobserved variables , therefore, influence offloaded residuals (\\(\\epsilon_{\\widehat{CW}}\\), note hat differentiates residuals get use observed military experience), correlated unobserved variables. Just like residuals first stage model (\\(\\epsilon_{}\\))! sets residuals correlated unobserved variables (\\(\\epsilon_{} \\propto UV\\) \\(\\epsilon_{\\widehat{CW}} \\propto UV\\)), two sets residuals also correlated: \\(\\epsilon_{} \\propto \\epsilon_{\\widehat{CW}}\\).correlated residuals sound wonderfully entertaining care given \\(\\epsilon_{\\widehat{CW}}\\) hypothetical residuals given hypothetical randomized military experience access ? can make real using covariance \\(\\epsilon_{}\\) can compute! idea: let us use two simple linear models predict military experience draft number career/wages actual military experience allow residuals two models correlated. , given \\(\\epsilon_{} \\propto UV\\), make \\(\\epsilon_{} \\propto \\epsilon_{CW}\\), therefore, \\(\\epsilon_{CW} \\propto UV\\). words, \\(\\epsilon_{CW} \\approx \\epsilon_{\\widehat{CW}}\\). means since \\(\\epsilon_{CW}\\) accounts variance due unobserved variables, must accounted terms linear model effect military experience (\\(\\beta^{true}_{}\\)) expresses just direct path:\n\\[\\widehat{CW} = \\alpha_{CW} + \\beta^{true}_{} \\cdot + \\epsilon_{\\widehat{CW}}\\]elegant trick: know correlation residuals unobserved variables know must (similar) account enforcing correlation residuals deduced must exist. one hand, obvious elephant room whole thing works well assumption residuals correlated true. hand, fitting covariance matrix part model, can check whether assumption correlation supported data. , instrumental variable probably good hoped .","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"parameters-combining-information-from-an-individual-with-population","chapter":"10 Parameters: combining information from an individual with population","heading":"10 Parameters: combining information from an individual with population","text":"infer values parameters, must decide combine information available individual “random effect” entry (e.g., participant) information distribution values parameter sample general group within sample. former — data individual — describes individual , necessarily, noisy. latter — data entire sample — better signal--noise ratio, pools information across individuals, informative averages individuals.list various strategies used throughout book. main purpose show differ primarily relative contribution two sources individuals used compute averages. talk primarily intercepts, often varied parameter, logic applicable free parameter model.","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"everyone-is-the-same-single-parameter","chapter":"10 Parameters: combining information from an individual with population","heading":"10.1 Everyone is the same (single parameter)","text":"first strategy used employ just single intercept parameter model.\\[height_i \\sim \\alpha\\\\\n\\alpha \\sim Normal(178, 9.5)\\]extreme case ignore fact people (monkeys, countries, etc.) different , effectively, model everyone single typical average meta-individual. information variance within sample discarded.plot , measurement individual (distinguished position y-axis) plotted x-axis (black circles). using single intercept (vertical line) model, means get average height (blue open circles).Another way view data plotting raw data (y-axis) vs. used estimates (x-axis). vertical line denotes population mean, whereas diagonal line implies estimates equal data.","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"everyone-is-unique-independent-parameters","chapter":"10 Parameters: combining information from an individual with population","heading":"10.2 Everyone is unique (independent parameters)","text":"extreme assume everyone unique judged (estimated) using data. , information population discarded.\\[height_i \\sim \\alpha_i\\\\\n\\alpha_i \\sim Normal(178, 9.5)\\]approach taken paired t-test repeated measures ANOVA. Note likely overfit data, allow limited noisy data fully determine intercepts. Use weak flat priors (frequentist approach) likely make --sample performance even worse.plot , measurement individual (distinguished position y-axis) plotted x-axis (black circles). see black circles covered open blue circles — estimates used model.another representation data dots lying diagonal (estimate equal data). vertical blue line still denotes population mean information used estimates.","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"people-are-different-but-belong-to-a-population-pooled-parameters","chapter":"10 Parameters: combining information from an individual with population","heading":"10.3 People are different but belong to a population (pooled parameters)","text":"balanced approach combine data individual population. two level (multilevel) approach, individual parameter value comes population (group) level distribution.\\[height_i \\sim \\alpha_i\\\\\n\\alpha_i \\sim Normal(\\alpha^{pop}, \\sigma^{\\alpha})\\\\\n\\alpha^{pop} \\sim Normal(178, 9.5)\\\\\n\\sigma^{\\alpha} \\sim Exponential(1)\\]basic idea “regression mean”, unusual-looking individuals probably average appear. words, unusual noise particular measurement. next measurement noise different, probably extreme, individual appear normal. pull extreme observations toward mean one static prior. main difference use adaptive priors determine typical/unusual observation based observations , including extreme ones. “normal” priors, influence adaptive prior pronounced extreme, unreliable observations, observations small amount data. easier overfit , hence, benefit regularizing priors.","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"people-are-different-but-belong-to-a-group-within-a-population-multilevel-clusters-of-pooled-parameters","chapter":"10 Parameters: combining information from an individual with population","heading":"10.4 People are different but belong to a group within a population (multilevel clusters of pooled parameters)","text":"logical extension two-level approach extended levels: individual belongs group (cluster) , turn, belongs population.\\[height_i \\sim \\alpha_i\\\\\n\\alpha_i \\sim Normal(\\alpha^{group}_i, \\sigma^{group})\\\\\nalpha^{group} \\sim Normal(\\alpha^{pop}, \\sigma^{pop}) \\\\\n\\alpha^{pop} \\sim Normal(178, 9.5)\\\\\n\\sigma^{\\alpha}, \\sigma^{group} \\sim Exponential(1)\\]individual, allows pool information across relevant group. example, males tend taller females, make sense use female sub-population/group/cluster decide just typical given female . goes types clusters: e.g., students particular school probably similar knowledge particular subject (math) compared students schools. time, can still pull information across clusters, making inferences reliable well. , course, limited number levels can create: students within school, schools within district, districts within city, etc.plots , notice sex cluster distributions tighter single population distribution . Also notice individual observers pulled toward group mean , , pull proportional atypical value .","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"people-are-similar-to-some-but-different-to-others-gaussian-process","chapter":"10 Parameters: combining information from an individual with population","heading":"10.5 People are similar to some but different to others (Gaussian process)","text":"Multilevel approach clusters useful works well-defined discrete cluster: sex, school student belongs , occupation, etc. However, sometimes clear boundaries. Rather, can presume similarity property interested (height, case) proportional another well defined measure similarity / distance individuals. example, one , perhaps naively, assume individuals similar genes similar height. case, can compute distance genetic sequences use distance define population relative individual.\\[height_i \\sim \\alpha^{pop} + \\alpha_i\\\\\n\\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\dots \\\\ \\alpha_N \\end{pmatrix} \\sim MVNormal(\n\\begin{pmatrix}0, 0, \\dots, 0 \\end{pmatrix}, K)\\\\\nK_{ij} = = \\eta^2 exp(−\\rho^2D^2_{ij}) + \\delta_{ij}\\sigma^2 \\\\\n\\alpha^{pop} \\sim Normal(178, 9.5)\\]\\(D_{ij}\\) distance individuals (see book details formula Gaussian process general).Generally speaking, idea contribution observations proportional distance individual question. also means discrete cluster. Rather, observation gets distance-based population distribution judged . Note, however, parameter \\(\\rho\\) adjusts effect distance making less relevant. Thus, distance can ignored model (via smaller value \\(\\rho\\)) meaning observations/individuals contribute equally population distribution (relative individual), .e., given intercept correlated everybody else. Conversely, larger values \\(\\rho\\) mean nearest neighbors taken account.example , distance individuals determined distance vertical axis. first plot use \\(\\rho=1\\) samples observations (dot size reflect \\(K\\) used MVNormal). produces fairly broad population distribution shift first participant right.second plot uses distance measure \\(\\rho=10\\) meaning close neighbors affect estimate. , population distribution much tighter estimate first participant shifted left.","code":""},{"path":"parameters-combining-information-from-an-individual-with-population.html","id":"people-are-different-but-belong-to-a-population-in-which-parameters-are-correlated-correlated-pooled-parameters","chapter":"10 Parameters: combining information from an individual with population","heading":"10.6 People are different but belong to a population in which parameters are correlated (correlated pooled parameters)","text":"idea two () parameters correlated within population — e.g., taller people (intercept) grow drink milk (slope, effect milk) — can pool information across parameters use evaluate one.\\[height_i \\sim \\alpha^{pop} + \\alpha_i + \\beta_i \\cdot Milk\\\\\n\\begin{pmatrix} \\alpha_i \\\\ \\beta_i \\\\ \\dots \\\\ \\alpha_N \\end{pmatrix} \\sim MVNormal(\n\\begin{pmatrix}0, 0 \\end{pmatrix}, K) \\\\\n\\alpha^{pop} \\sim Normal(178, 9.5)\\], pulling forces bit complicated single parameter situation. Namely, parameters necessarily pulled towards center distribution (population averages) orthogonal isolines, adjusted relative . plot , black filled dot average respect slop higher average intercept. means , one one hand, intercept high average slope. One hand, slope average high slope. parameters pull time, us intercept becomes normal slope becomes higher (bit extreme). advantage can use parameters judge population.","code":""},{"path":"incorporating-measurement-error-a-rubber-band-metaphor.html","id":"incorporating-measurement-error-a-rubber-band-metaphor","chapter":"11 Incorporating measurement error: a rubber band metaphor","heading":"11 Incorporating measurement error: a rubber band metaphor","text":"One way think loss functions rubber bands27 (black vertical segments) connecting observations (white dots) corresponding mean (red dots) regression line. band tries pull mean , therefore, regression line towards observation final estimate simply minimal energy state: total sum pulling forces minimal, movement regression line decrease stretch sum dots produce way pulling dots, increasing overall force. “usual” way performed regression analysis assuming rubber bands connect every dot regression line.\nWithin metaphor, measurement error incorporated additional inferred value (blue dots, true value measurement) connected observed value via custom rubber band (colored) whose strength depends measurement error. .e., large measurement errors make weak stretchable bands, whereas high certainty leads small strong bands. Note connectivity pattern: observed value — (via custom rubber band) — (estimated) true value — (via common rubber band) — mean regression line. , dots smaller measurement error pull regression line much stronger towards observed value, whereas dots large error tolerate regression line even far away.Note cases, information measurement error incorporated directly. example, binomial likelihood number successes total number observations determine proportion success also confidence value.","code":""},{"path":"generalized-additive-models-as-continuous-random-effects.html","id":"generalized-additive-models-as-continuous-random-effects","chapter":"12 Generalized Additive Models as continuous random effects","heading":"12 Generalized Additive Models as continuous random effects","text":"purpose note give intuition generalized additive models (GAMs) , importantly, can handy causal inferences. Thus, leave mathematical details out28.","code":""},{"path":"generalized-additive-models-as-continuous-random-effects.html","id":"generalized-additive-models-an-über-brief-introduction","chapter":"12 Generalized Additive Models as continuous random effects","heading":"12.1 Generalized Additive Models: An Über-brief Introduction","text":"Imagine (completely made !) data set shown aboe describes probability consuming ice-cream function age. can see, twisty curve, simpler linear regression won’t . idea process(es) generated data, might least smooth see underlying relationship clearly. can number ways using running median, loess, etc., focus GAMs advantage (generalized) linear models, makes easy incorporate bigger generalized linear models.idea GAM approximate dependence sum basis functions. example , basis functions identical cover different parts space. However, can way around different basis functions covering identical (entire) space.example , basis functions cubic regression splines centered “knots”. tent coefficient determines height , also, transition knots. wigliness parameter determines different coefficients can , different , wigglier line, hence name. Accordingly, wigliness zero produces straight horizontal line (coefficients must ). Wigliness infinity means coefficients can take value fit data closely possible. can see, smoothing makes much easier see underlying relationship.\n","code":""},{"path":"generalized-additive-models-as-continuous-random-effects.html","id":"what-are-gams-good-for","chapter":"12 Generalized Additive Models as continuous random effects","heading":"12.2 What are GAMs good for?","text":"mentioned , GAMs just smoothers. approximate smooth relationship data without knowledge assumptions generative process behind . least two clear usage cases: exploratory data analysis generating predictions. former case, shape smoothed line might hint underlying mechanisms facilitate development causal model. latter case, long stick interpolation, can use smoothed line predict unobserved values future use.causal inference? Can GAMs useful case? Yes, can play useful obvious role. make “obvious” obvious, must take detour talk random factors.","code":""},{"path":"generalized-additive-models-as-continuous-random-effects.html","id":"covariates-random-factors","chapter":"12 Generalized Additive Models as continuous random effects","heading":"12.3 Covariates / random factors","text":"Imagine data ice-cream consumption , among things, measured respondents’ eye color. find people different eye color tend consume different amount ice-cream (average). ? made-example, know. genetic, group genes responsible eye color ice-cream craving. something else. Point , good () causal model generative process link two variables. really interested link also ignore . systematic relationship ignore leave unexplained variance , turn, make harder us see causal effect predictors interested . ? include eye color either covariate (fixed effect) random effect. frequentist models, difference whether intercept group estimated independently (former, flat priors, shrinkage) pooling information across groups (latter, produces shrinkage). Bayesian models, difference fixed (covariates) adaptative (random effect) priors. Still, idea stress idea particular variable correlated outcome causal generative model.","code":""},{"path":"generalized-additive-models-as-continuous-random-effects.html","id":"gams-as-continuous-random-factors","chapter":"12 Generalized Additive Models as continuous random effects","heading":"12.4 GAMs as continuous random factors","text":"Let us get back ice-cream consumption function age. easy situation eye color: idea generative process dependence strong ignored. Thus, must include age bigger model, even care age causes ice-cream consumption. can ? relationship linear even monotonic, use age linear covariate. , unlike categorical data eye color, data continuous well-defined levels estimate values (unless introduce artificial discrete levels). Plus, latter approach actually ignore dependence, ignore fact nearby levels similar distance ones. , ?GAMs rescue! pooling information nearby data samples, GAMs produce smooth estimate age can used model. smooth estimate , effectively, continuous random effect! Thus, use GAMs whenever continuous variable correlated outcome, idea underlying causal link relationship simple monotonic enough use conventional methods.","code":""},{"path":"flat-priors-the-strings-attached.html","id":"flat-priors-the-strings-attached","chapter":"13 Flat priors: the strings attached","heading":"13 Flat priors: the strings attached","text":"feeling one biggest issues lot people consider Bayesian statistics priors. something students worry feel insecure . result, rather stay frequentist world flat priors.flat priors lot going . convenient mathematicians make analytical derivations simpler. convenient users statistics worry even think priors. , think priors, flat priors look superior feel impartial. impose priori knowledge allow results determined data alone. Thus, whatever results get, can claim objective untainted person analysis.However, flat priors strings attached. may deal-breaker (although probably ) definitely something aware use .","code":""},{"path":"flat-priors-the-strings-attached.html","id":"flat-priors-are-silly","chapter":"13 Flat priors: the strings attached","heading":"13.1 Flat priors are silly","text":"Consider experimental data domain knowledgeable . biggest effect feel borderline realistic? .e., effect already ridiculously large anything larger must come malfunctioning equipment software, error analysis, etc. example, evoked potentials EEG measured microvolts, can safely assume difference , microvolts range, must artificial. Let’s say threshold “real effect” way-way overly optimistic 1 millivolt, 1000 μV. specialist already sounds ridiculous use flat priors explicitly state believe equally strongly difference evoked potential scale microvolts, millivolts, volts, even billions volts. priory belief human brain equally capable generating evoked current microvolts billions volts silly? sure sound silly . Forcing model make belief less silly.","code":""},{"path":"flat-priors-the-strings-attached.html","id":"flat-priors-make-you-pretend-that-you-are-naïve","chapter":"13 Flat priors: the strings attached","heading":"13.2 Flat priors make you pretend that you are naïve","text":"way look using flat priors explicitly claim naïve respect domain. act prior knowledge phenomenon, discarding experience acquired. really feel years studying subject relevance? really think good predictions least realistic range effect? probably . Even uncertain , range minus plus infinity awfully large can certainly better . yet, use flat priors implies completely clueless nugget wisdom aid analysis.","code":""},{"path":"flat-priors-the-strings-attached.html","id":"flat-priors-tend-to-overfit","chapter":"13 Flat priors: the strings attached","heading":"13.3 Flat priors tend to overfit","text":"Even best circumstances (), flat priors restrict model fitting sample close possible. means models almost certainly overfit data. might might big issue particular case, depend whether noise exaggerates belittles actual effect. suspect flat priors combined fortunate noise partially responsible plethora reported strong effects replicate. definitely something keep mind.","code":""},{"path":"flat-priors-the-strings-attached.html","id":"flat-priors-are-an-exception","chapter":"13 Flat priors: the strings attached","heading":"13.4 Flat priors are an exception","text":"Although people flat priors probably feel like norm, applicable specific cases (relatively) predictors plenty data. Remember advice many observations per variable? need afford flat priors lack regularization general. However, magic sweet spot fairly small flat priors become extremely dangerous soon step .observational study little data? See example “Ecological Detective” Hilborn Mangel give plenty situations unavoidable.\nFlat priors lead extreme overfitting point models just useless dangerously misleading. book mentioned shows usage proper priors can rescue analysis.lot data also lot predictors? probably end overfitting. field machine learning invests lot time energy regularization. Given sheer number predictors, set priors hand , instead, use forms method-specific batch regularization lasso ridge regression penalties coefficient weight, pruning trees, dropping neurons, etc.short, can afford flat priors regularization keep fairly specific kinds data sets. norm, exception.","code":""},{"path":"flat-priors-the-strings-attached.html","id":"the-irony-of-power-analysis","chapter":"13 Flat priors: the strings attached","heading":"13.5 The irony of power analysis","text":"Even fond using flat priors worried issues raised , still need think proper priors . Specifically, whenever need perform power analysis. , postulate silly things remaining “objective impartial” need use domain knowledge formulate sign magnitude effect order estimate sample size need. power analysis either extremely easy, know priors, extremely hard, know . Thus, even “flat priors” people avoid using proper ones suspect , cases, regular thinking priors domain makes much easier define power analysis.","code":""},{"path":"flat-priors-the-strings-attached.html","id":"conclusions-1","chapter":"13 Flat priors: the strings attached","heading":"13.6 Conclusions","text":"hope notes able show flat priors neither universal, best prior, norm. something can afford specific circumstances. course can use least make mental note think applicable particular case, advantages alternatives, costs using analysis.","code":""},{"path":"unbiased-mean-versus-biased-variance-in-plain-english.html","id":"unbiased-mean-versus-biased-variance-in-plain-english","chapter":"14 Unbiased mean versus biased variance in plain English","heading":"14 Unbiased mean versus biased variance in plain English","text":"One things learned statistics course mean unbiased estimator whereas variance biased estimator , therefore, requires correction29. attempt provide intuition case using formulas possible.start noting sample mean (mean data ) (almost) always different population “true” mean interested . trivial consequence sampling variance. pretty unlikely hit exactly “true” population mean limited sample. means sample mean wrong wrong balanced way. equally likely larger smaller “true” mean30. Therefore, draw infinite number samples size compute sample means random deviations left right true mean cancel average mean estimate correspond true mean. short, sample means wrong individually correct average. Thus, wrong systematic way , words, mean unbiased estimator.variance? Variance just average squared distance true population mean \\(\\mu\\): \\(\\frac{1}{N}\\sum\\limits^{N}_{=1}{(x_i-\\mu)^2}\\). Unfortunately, know true population mean , therefore, compute variance (.k.. average squared/ L2 distance) relative sample mean \\(\\bar{x}\\): \\(\\frac{1}{N}\\sum\\limits^{N}_{=1}{(x_i-\\bar{x})^2}\\) makes difference. Recall use squared distance loss function, sample mean point minimal average distance points sample31. put differently, sample mean point minimizes computed variance. pick point sample mean, average distance / variance necessarily larger. However, already established true population mean different sample mean compute sample variance relative true mean, larger (, always larger point sample mean). much larger depend wrong sample mean (something know) always larger. Thus, variance computed relative sample mean systematically smaller “correct” variance, .e. biased estimator. Hence \\(\\frac{1}{n-1}\\) instead \\(\\frac{1}{n}\\) attempts correct bias “average”. mean, even corrected variance sample wrong (equal true variance hidden distribution trying measure) , least, systematically wrong.","code":""},{"path":"probability-mass-versus-probability-density.html","id":"probability-mass-versus-probability-density","chapter":"15 Probability mass versus probability density","heading":"15 Probability mass versus probability density","text":"first encounter continuous distribution, one initially confusing things concept probability density looks like probability outcome () fact can positive value, just within 0 1. make understanding easier, let us start simple concept probability mass. , outcome gets probability must 0 1 probabilities outcomes must add 1 (makes values probability rather just plausibility). Imagine simplest case events equally likely. example, four cubes must guess height tower built. five possible towers (zero-height tower also tower, just particularly good one) without prior knowledge can assume tower height equally likely: 1/N, N=5, 1/5 = 0.2 (20%, like percentages ).another way thinking via cumulative mass function. tells cumulative (total) probability observing height tower equal smaller chosen value.\ncan see, cumulative probability observing tower zero height (lower) 0.2. consider height 1, bumps cumulative probability 0.4 (Pr(height=0) + Pr(height=1) = 0.2 + 0.2 = 0.4). Going 2 makes 0.6, 3 — 0.8 , finally, cumulative probability building tower height 4 lower 1 (100%!). latter includes possible tower heights, probability observe one 100%. Note height probability mass tells much cumulative probability increase. probability mass change cumulative probability become important later.Note cumulative probability can grow must 0 1, probabilities negative must sum 1 (otherwise, call plausibilities). plot now includes impossible heights -1 5. probabilities 0, cumulative probability grow.\nbuild “tower” fine sand? simplicity, let us assume height also within 0 4 (cubes height) range. now, way heights tower can . sand super fine, individual grains infinitesimally small, infinite numbers possible heights. Knowing equally likely sort helps compute probability individual height: 1/N, N=∞ mean 1/∞ ≈ 0. annoying thing infinities, make computing things really hard32. , compute probability individual event/value (height)? can still compute cumulative probability tower smaller equal particular height! cumulative function saw earlier, steps now fine talk sums integrals. now called cumulative density function (CDF).\nNote looks fairly similar previous plot cumulative probability discrete events. still grows 0 1 steps much finer. Yet can still tell probability observing tower height equal less . start zero, observe towers negative height, tower heights negative infinity zero total cumulative probability 033. pick height 4, cumulative probability observing tower equal--less 1 must smaller (defined , tower higher 4!). pick middle interval (height = 2), cumulative probability observing tower equal--smaller 0.5. using uniform distribution, half splitting height half also gives us 50% chance. can see, cumulative probability density behaves way cumulative probability mass example . fairly straightforward, long appreciate outcomes point interest, just single event.probability density function (PDF)? Recall probability mass function tells much cumulative probability changes “move right” include next outcome. thing continuous cases function describes rate change called derivative. formula continuous uniform distribution \n\\[CDF = \\frac{1}{4} \\cdot height\\]\n(note currently thinking range 0 4 make things simpler). can check indeed case plugging different heights. derivative respect height \n\\[\\frac{\\delta CDF}{\\delta height} = \\frac{1}{4}\\]Thus, can now plot CDF PDF.\n\ncan reverse logic say cumulative density function integral PDF, .e., area curve point. , total area PDF (blue line) end 1 (final value CDF). easy check rectangular like , just multiply width (height range 4) height (probability density value 0.25) get 1. Note restrict height=2 CDF values 0.5 area blue line (PDF) also 0.5 (compute!).example , probability density constant 0.25 makes look “normal”, least surprising. restrict tower-building, build anything taller 0.5 cubes (meters, units little relevance ). Now, CDF formula (check!):\n\\[CDF = 2 \\cdot height\\]PDF:\n\\[\\frac{\\delta CDF}{\\delta height} = 2\\]constant PDF value 2! Way one, come? Think slope think area rectangle: 0.5 units wide must 2 units tall make equal 1. point probability density probability care absolute values probability density, relative values. feel “adding ” values larger 1 give 1 end, yes, counter-intuitive. Integrals, things infinitely small large numbers, counter-intuitive. Thus, ignore units, ignore absolute values, just keep mind whenever higher, probable specific value . (, , way compute meaningful probability single value).","code":""},{"path":"effective-degrees-of-freedom-number-of-parameters.html","id":"effective-degrees-of-freedom-number-of-parameters","chapter":"16 Effective degrees of freedom / number of parameters","heading":"16 Effective degrees of freedom / number of parameters","text":"One thing learn start using linear mixed models (multulevel) models degrees freedom used model less number parameters, total number called number effective parameters / degrees freedom. parameter (whether formally “fixed” “random”) can use 1 degree freedom. use full one degree freedom, can take value. words, parameter flat prior takes 1 degree freedom. yet words, parameter takes one full degree freedom prior distribution infinite variance (different way say can take value prior flat). Conversely, prior distribution parameter zero variance, fixed (constant!) take zero degrees freedom. Finally, prior distribution parameter non-zero finite variance (whatever ), uses fraction degree freedom. much uses depends variance.fixed parameters frequentist statistics use full degree freedom (flat priors!) “fixed” parameters use less one Bayesian models non-flat priors (even weakly regularizing priors mean enjoying full freedom). Similarly, random effect repeated measures ANOVA uses one degree freedom flat priors (can take value, shrinkage) fraction linear mixed models, LMM random effects come Gaussian distribution finite variance (shrinkage!).","code":""},{"path":"multiple-regression---masked-relationship.html","id":"multiple-regression---masked-relationship","chapter":"17 Multiple regression - Masked relationship","heading":"17 Multiple regression - Masked relationship","text":"notes section 5.2 “Masked relationship” chapter 5 “Many Variables & Spurious Waffles”. section introduces mirror twin section 5.1 spurious associations. latter explores two predictors can strong relationship outcome variable fitted individually one retains relationship predictors used multiple regression. Section 5.2 concerned exact opposite. look association predictor variable (neocortex percent log body mass) richness milk individually, find nothing. However, use simultaneously via multiple regression, find strongly associated outcome variable.key understanding figure 5.9 (particularly, bottom counterfactual plots) explanation page 151 coefficients regression model shows “species high neocortex percent body mass higher milk energy” “species high body mass neocortex percent lower milk energy”. words, take several species similar body mass, expect higher percent neocortex richer milk. Conversely, handpick several species similar neocortex percentage, expect larger ones less energetic milk. two predictor variables correlated, relationship easy see fix one . Figure 5.9 tries show can better (artificially) grouping data looking relationship predictor variable milk energy individually.Let us start replicating top subplots figure 5.9. Note 1) used frequentist linear model fit flat priors via geom_smooth() , therefore, 2) stripes correspond standard error rather 89% credible interval. However, two close enough case illustration purposes., , see clear relationship evident consider neocortex percent ignoring body mass animal , vice versa, look log body mass ignoring cortex. Let us reduce state ignorance splitting individual species three groups based similarity neocortex percentage. Note put extra care grouping process apart ensuring groups roughly number species. Also note connecting lines specific meaning ’ve added make visual grouping dots color easier.group species either neocortex percent, can immediately see pattern. pattern left trivial merely shows grouping based low (red), medium (green), high (blue) percentage neocortex. However, look right plot, see species similar neocortex percentage cluster nicely together way can see nice negative correlation log body mass milk energy within group. absolute log body mass corresponds different relative log body mass group. .e., vertical slice body mass leads different values milk energy values come different neocortex percent groups. summarize, can see log body mass informative know neocortex percent animal.Let us try visualize multiple regression . Keep mind rough approximation use just three means (one neocortex percent group), whereas model computes appropriate mean every single specie. emulate model check whether “species particular neocortex percent body mass lower milk energy” centering group, .e., subtracting groups’ mean body mass specie group. words, align group average bogy mass species group center, larger species within group right smaller species within group left. way remove positive correlation neocortex percent body mass , therefore, can see effect body mass milk energy alone. , even approximate way dependence clear.plot expresses idea counterfactual right bottom plot figure 5.9. , question “effect log body mass species average neocortex percent”, single specific group animals. plot merely extended idea three groups. appreciate link, let us fit model plot counterfactuals group identified.can way around grouping species log body mass.","code":"\ndata(milk)\nmilk_df <- \n  milk %>%\n  # standardize\n  mutate(K = standardize(kcal.per.g),\n         N = standardize(neocortex.perc),\n         M = standardize(log(mass))) %>%\n  \n  # keep only complete cases\n  na.omit()\n\nNK_plot <- \n  ggplot(milk_df, aes(x = N, y=K)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  xlab(\"neocortex percent (std)\") + \n  ylab(\"kilocal per g (std)\") + \n  coord_equal()\n\nMK_plot <- \n  ggplot(milk_df, aes(x = M, y=K)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", formula=y~x) + \n  xlab(\"log body mass (std)\") + \n  ylab(\"kilocal per g (std)\") + \n  coord_equal()\n\nNK_plot | MK_plot\nmilk_df <- \n  milk_df %>%\n  # split data into three groups\n  mutate(`Mass group` = cut_number(M, 3), \n         `Neocortex group` = cut_number(N, 3))\n\nNK_plot <- \n  ggplot(milk_df, aes(x = N, y=K, color=`Neocortex group`)) + \n  geom_point() +\n  geom_line() +\n  xlab(\"neocortex percent (std)\") + \n  ylab(\"kilocal per g (std)\") +\n  theme(legend.position=\"bottom\") +\n  guides(col = guide_legend(nrow = 3, byrow = TRUE)) +\n  coord_equal()\n\nMK_plot <- \n  ggplot(milk_df, aes(x = M, y=K, color=`Neocortex group`)) + \n  geom_point() + \n  geom_line() +\n  xlab(\"log body mass (std)\") + \n  ylab(\"kilocal per g (std)\") + \n  theme(legend.position=\"bottom\") +\n  guides(col = guide_legend(nrow = 3, byrow = TRUE)) +\n  coord_equal()\n\nNK_plot | MK_plot\nmilk_df <- \n  milk_df %>%\n  group_by(`Neocortex group`) %>%\n  mutate(cM = M - mean(M))\n\nggplot(milk_df, aes(x = cM, y=K)) + \n  geom_smooth(method=\"lm\", formula=y~x, color=\"black\") +\n  geom_point(aes(color=`Neocortex group`)) + \n  xlab(\"log body mass - group mean log body mass (std)\") + \n  ylab(\"kilocal per g (std)\") + \n  theme(legend.position=\"right\") +\n  guides(col = guide_legend(nrow = 3, byrow = TRUE)) +\n  xlim(-2, 2) +\n  coord_equal()\nm5.7 <- quap(\n  alist(\n    K ~ dnorm( mu , sigma ) ,\n    mu <- a + bN*N + bM*M ,\n    a ~ dnorm( 0 , 0.2 ) ,\n    bN ~ dnorm( 0 , 0.5 ) ,\n    bM ~ dnorm( 0 , 0.5 ) ,\n    sigma ~ dexp( 1 )\n  ),\n  data=milk_df)\n\n\n# function to compute counterfactuals\ncompute_counterfactual <- function(quap_fit, neocortex, logmass){\n  mu <- link(quap_fit, data=data.frame(N=neocortex, M=logmass))\n  mu_PI <- apply(mu,2,PI)\n  tibble(M = logmass,\n         N = neocortex,\n         K = apply(mu,2,mean),\n         KLower = mu_PI[1, ],\n         KUpper = mu_PI[2, ])\n}\n\n\n\n# defining x-ticks as all milk values at regular intervals\nx_seq <- seq( from=min(milk_df$M)-0.15 , to=max(milk_df$M)+0.15 , length.out=30)\n\n# mean neocortex percent for each neocortex percent group\nneocortex_counterfactuals_df <-\n  # compute average neocortex for each group\n  milk_df %>%\n  group_by(`Neocortex group`) %>%\n  summarise(MeanPercent = mean(N), .groups=\"keep\") %>%\n  \n  # compute counterfactual predictions for each group\n  group_modify(~compute_counterfactual(m5.7, neocortex=.x$MeanPercent, logmass=x_seq)) %>%\n  ungroup()\n  \n# plotting data\nggplot(milk_df, aes(x = M, y=K, color=`Neocortex group`)) + \n  geom_ribbon(data=neocortex_counterfactuals_df, aes(ymin=KLower, ymax=KUpper, fill=`Neocortex group`), color=NA, alpha=0.25) +\n  geom_line(data=neocortex_counterfactuals_df) +\n  geom_point(shape=21, fill=\"white\", size=3) +\n  xlab(\"log body mass (std)\") + \n  ylab(\"kilocal per g (std)\") + \n  xlim(-2.5, 2.5) +\n  theme(legend.position=\"right\") +\n  guides(col = guide_legend(nrow = 3, byrow = TRUE)) +\n  coord_equal()\nlogmass_counterfactuals_df <-\n  milk_df %>%\n  group_by(`Mass group`) %>%\n  summarise(MeanMass = mean(M), .groups=\"keep\") %>%\n  \n  # compute counterfactual predictions for each group\n  group_modify(~compute_counterfactual(m5.7, neocortex=x_seq, logmass=.x$MeanMass)) %>%\n  ungroup()\n  \n# plotting data\nggplot(milk_df, aes(x = N, y=K, color=`Mass group`)) + \n  geom_ribbon(data=logmass_counterfactuals_df, aes(ymin=KLower, ymax=KUpper, fill=`Mass group`), color=NA, alpha=0.25) +\n  geom_line(data=logmass_counterfactuals_df) +\n  geom_point(shape=21, fill=\"white\", size=3) +\n  xlab(\"neocortex percent (std)\") + \n  ylab(\"kilocal per g (std)\") + \n  xlim(-2.5, 2.5) +\n  theme(legend.position=\"right\") +\n  guides(col = guide_legend(nrow = 3, byrow = TRUE)) +\n  coord_equal()"},{"path":"multiple-regression---masked-relationship.html","id":"a-warning-message","chapter":"17 Multiple regression - Masked relationship","heading":"17.1 A warning message","text":"two examples (spriouos associations masked relationship) mean putting variables model always good idea “magic”? Unfortunately, . examples handpicked McElreath show power multiple regression. can also handpick example mindlessly throwing variables model lead disaster, .k.. Causal Salad (watch three hour talk YouTube). take home message : Models golems, can help understand process investigating won’t understand ! job think causal relationship using statistical inferences merely aid, oracle.","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"ordered-categorical-data-i.e.-likert-scales","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18 Ordered Categorical Data, i.e., Likert-scales","text":"One popular type response psychology social sciences -called Likert-scale responses. example, may asked respond attractive find person photo 1 (unattractive) 7 (attractive). respond satisfied service 1 (unsatisfied) 4 (satisfied). rate confidence 5-point scale, etc. Likert-scale responses extremely common quite often analyzed via linear models (.e., t-test, repeated measures ANOVA, linear-mixed models) assuming response levels correspond directly real numbers. purpose notes document conceptual technical problems approach entails.","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"conceptualization-of-responses-internal-continuous-variable-discritized-into-external-responses-via-a-set-of-cut-points","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18.1 Conceptualization of responses: internal continuous variable discritized into external responses via a set of cut-points","text":"First, let us think behavioral responses correspond become important discuss conceptual problems common “direct” approach using linear models Likert-scale data.ask participant respond “scale 1 7, attractive find face photo?”, assume continuous internal variable (example, encoded via neural ensemble) represents attractiveness face (satisfaction service, confidence, etc.). strength representation varies continuous manner minimum (e.g., baseline firing rate, assume strength encoded spiking rate) maximum (maximum firing rate neural ensemble). impose seven-point scale participants, force discretize (bin) continuous variable, creating many--one mapping. words, participant decides values (intensities) within particular range get mapped \\(1\\), different adjacent range higher value corresponds \\(2\\), etc. can think values within range “rounded”34 towards mean defines responses. , equivalently, can think terms cut points define range individual values. discretization depicted figure . signal first cut point, participant’s response “1”. first second cut points, response “2” . right last sixth cut point, “7”. conceptualization means responses ordered categorical variable, underlying intensity response “1” necessarily smaller intensity response “2” smaller , , intensity response “3”, etc.per usual, even use stimulus ask question, participant’s internal continuous response varies trial trial due noise. model assuming given trial value drawn normal distribution centered “true” intensity level35. noisy intensity converted discrete responses, variability depend location (mean) width (standard deviation) distribution. broader distribution / closer cut point, activity “spill ” cut point adjacent regions variable discrete responses .Given conceptualization, goal recover cut points model shifts mean continuous internal variable (response experimental manipulation) using observed discrete responses.","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"conceptual-problem-with-linear-models-we-change-our-mind-about-what-responses-correspond-to.","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18.2 Conceptual problem with linear models: we change our mind about what responses correspond to.","text":"common approach fit Likert-scale data using linear model (t-test, repeated-measures ANOVA, linear-mixed models, etc.) assuming responses correspond directly real numbers. words, participants responded “unattractive”, “confident ”, “agree ” literally meant real number \\(1.0\\). used middle (let’s say third five-point scale) option “neither agree, disagree” literally meant \\(3.0\\).assumption appears simplify life dramatically expense changing narrative. Recall original (, , intuitive) conceptualization responses reflect many--one mapping underlying continuous variable discrete (ordered categorical) response. converting directly real numbers using outcome variable linear model assume one--one mapping continuous real-valued internal variable continuous(!) real-valued observed responses. means linear model point view, 7-point Likert scale real value valid possible response therefore participant responded 6.5, 3.14, 2.71828 , whatever reason (sheer luck?), observed handful (integer) values.Notice thought participants behave. think everyone36 object idea limited repertoire responses due endogenous processing rather exogenous limitations imposed experimental design. Yet, linear model “thinks” given outcome variable gave , careful, easy miss change narrative. , however, important means estimates produced model alternative one--one kind continuous responses, many--one discrete ones mind! alternative bad story per se, just different story confused original one.change narrative responses correspond also problem want use (fitted) linear model generate predictions simulate data. happily spit real valued responses like 6.5, 3.14, 2.7182837. two options. can bite bullet take face value, sticking “response real-valued variable” one--one mapping internal variable observed response. lets keep narrative means real ideal observers play different rules. responses different means conclusions based ideal observer behavior limited use. Alternatively, can round real-valued responses closest integer getting discrete categorical-like responses. Unfortunately, means changing narrative yet . case, fitted model assuming one--one mapping use predictions assuming many--one. good. really hard understand going , keep changing mind responses mean. linear model also generate --range responses, like -1 8. , little choice clip valid range, forcing many--one mapping least responses. , change narrative means model fitting model interpretation rely different conceptualizations response .may sound conceptual suspect people use linear models Likert-scale data directly realize model think , erroneously!, interpret one--one linear-model estimates many--one. difference may may crucial , unfortunately, one know important without comparing two kinds models directly. raises question: employ model something different need begin ? Remember, using appropriate model interpreting correctly job, mathematical model, job software package.","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"a-technical-problem-data-that-bunches-up-near-a-range-limit.","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18.3 A technical problem: Data that bunches up near a range limit.","text":"use linear model, assume residuals normally distributed. something may sure fit specific model, residuals data must normally distributed. However, cases may fairly certain case, variable limited range values mean (model prediction) close one limits. Whenever observations close hard limit, “bunch ” go lower higher . See figure illustration happens continuous variable \\(x\\) restricted 1 7 range38.\npresence limit deal breaker using linear models per se. physical measures negative39 long observations sufficiently far away zero, fine. negative height certainly can use linear models adult height , example, average female height USA 164±6.4 cm. words, mean 25 standards deviations away range limit zero latter can safely ignored.Unfortunately, Likert-scale data combines extremely limited range coarse step. Even 7-point Likert scale give much wiggle room routinely used 5-point scales even narrower. means unless mean smack middle (e.g., four 7-point scale), getting closer one limits residuals become either positively (approaching lower limit) negatively (upper one) skewed. words, residuals systematically normally distributed distribution depends mean. clearly violates assumption normality residuals conditional ..d. (Independent Identically Distributed). deal breaker parametric frequentist statistics (t-test, repeated-measures ANOVA, linear-mixed models), inferences built assumptions , therefore, become unreliable trusted.","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"another-technical-problem-can-we-assume-that-responses-correspond-to-real-numbers-that-we-picked","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18.4 Another technical problem: Can we assume that responses correspond to real numbers that we picked?","text":"skewed residuals described fundamental problem parametric frequentist methods critical use Bayesian non-parametric bootstrapping/permutation linear models. mean safe use ? Probably . use responses directly, assume direct correspondence response label (e.g., “agree”) real number \\(4.0\\). responses correspond real numbers picked, can perform usual arithmetic . E.g., can assume \\((4.0 + 4.0) / 2\\) equal \\((3.0 + 5.0) / 2\\) \\((2.0 + 6.0) / 2\\) \\((1.0 + 7.0)/ 2\\). However, case, responses correspond real numbers ’ve picked? basic arithmetic stops working way think! Take look figure “real value” responses integer picked .Unless know response levels correspond selected real number simple arithmetic holds, danger computing nonsense. problem obvious individual response levels labelled, e.g., \"Strongly disagree\", \"Disagree\", \"Neither disagree, agree\", \"Agree\", \"Strongly agree\". average \"Strongly disagree\" \"Strongly agree\"? average \"Disagree\" \"Agree\"? increase \"Strongly disagree\" \"Disagree\" identical \"Neither disagree, agree\" \"Agree\"? answer “knows?!” experience scales rarely truly linear people tend avoid extremes idea range internal variable levels corresponds particular response.noted , even scale levels explicitly named, common “convert” numbers ask computer compute average \"Disagree\" \"Agree\" (flatly refuse ) compute average \\(2\\) \\(4\\). error message! return \\(3\\)! Problem solved, right? really. Yes, computer complain idea \\(2\\) \\(4\\) stand . give real numbers, math. , pretend \"Disagree\" \"Agree\" correspond directly \\(2\\) \\(4\\) certainly look like normal math. imagine responses \"Disagree\" \"Strongly agree\", numbers \\(2\\) \\(5\\) computer return average value \\(3.5\\). even easier convince responses real numbers (see, decimal point !), just like linear models assume. Unfortunately, fooling computer (seriously care), fooling . math might check , responses correspond real numbers picked, might . cases, warning error message, just numbers interpret face value reach possibly erroneous conclusions. , problem wouldn’t know whether numbers looking valid nonsense dilemma (valid nonsense?) applicable inferences conclusions draw . short, direct correspondence response levels specific real numbers strong assumption validated, taken pure faith.","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"solution-an-ordered-logitprobit-model","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18.5 Solution: an ordered logit/probit model","text":"far summarized problems using linear models assuming responses correspond real numbers. can solve ? using ordered logistic/probit models. built using many--one mapping continuous variable limited range (simplicity ranges 0 1) discretized match behavioral responses using set cut points. principle, latter can fixed cases fitted part model. logit probit models assume sampling distribution underlying continuous variable standard normal distribution , therefore, continuous variable cut points live infinite real number line transformed 0..1 range via either logit probit link function. Strictly speaking, latter step necessary makes things easier math understanding outcome.mathematical point view, using logit probit makes easy compute area curve two cut points. Logit probit cumulative functions, standard normal distribution (centered \\(0\\) standard deviation \\(1\\)) compute area curve starting \\(-\\infty\\) point \\(k_i\\).Therefore, want compute area two cut points \\(k_{-1}\\) \\(k_i\\), can \\(logit(k_{})-logit(k_{-1})\\) (goes probit).logit probit non-linear transformations cut points evenly distributed 0..1 range evenly distributed real numbers line vice versa. Transforming real space 0..1 range also makes easier understand relative positions cut points changes continuous variable (translate discrete responses via cut points).","code":""},{"path":"ordered-categorical-data-i.e.-likert-scales.html","id":"using-ordered-logitprobit-models","chapter":"18 Ordered Categorical Data, i.e., Likert-scales","heading":"18.6 Using ordered logit/probit models","text":"several R packages implement regression models ordinal data including specialized packages ordinal, oglmx, well via ordered option brms package.coding point view, fitting ordered logit/probit model easy fitting regression model. However, presence link function complicates understanding parameters interact40. current approach try interpret parameters directly plot triptych.Compute posterior predictions compare distribution behavioral data understand well model fits data.\nFigure 18.1: Behavioral data (circles error bars depict group average bootstrapped 89% confidence intervals) versus model posterior predictions (lines ribbons depict mean 89% compatibility intervals).\nVisualize cut points 0..1 range understand mapping continuous intensity discrete responses well uncertainty position.\nFigure 18.2: Posterior distribution cut points transformed 0..1 range.\nVisualize compare changes continuous intensity 0..1 range adding cut points facilitate understanding.\nFigure 18.3: Posterior distribution change continuous intensity variable transformed 0..1 range. Text plot show mean 89% credible interval change.\n","code":""}]
